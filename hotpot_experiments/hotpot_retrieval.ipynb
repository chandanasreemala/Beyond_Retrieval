{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing HotpotQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'question', 'answer', 'type', 'level', 'supporting_facts',\n",
       "       'context', 'supporting_docs', 'supporting_sentences',\n",
       "       'supporting_docs_dedup', 'clean_supporting_sentences'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"old/outputs/processed_hotpot_qa_on_full_dataset.csv\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[array([\"Arthur\\'s Magazine (1844â€“1846) was an American literary periodical published in Philadelphia in the 19th century.\",\\n       \\' Edited by T.S. Arthur, it featured work by Edgar A. Poe, J.H. Ingraham, Sarah Josepha Hale, Thomas G. Spear, and others.\\',\\n       \\' In May 1846 it was merged into \"Godey\\\\\\'s Lady\\\\\\'s Book\".\\'],\\n      dtype=object), array([\"First for Women is a woman\\'s magazine published by Bauer Media Group in the USA.\",\\n       \\' The magazine was started in 1989.\\',\\n       \\' It is based in Englewood Cliffs, New Jersey.\\',\\n       \\' In 2011 the circulation of the magazine was 1,310,696 copies.\\'],\\n      dtype=object)]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0][\"supporting_docs_dedup\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\"arthur\\'s magazine (1844â€“1846) was an american literary periodical published in philadelphia in the 19th century.\", \"first for women is a woman\\'s magazine published by bauer media group in the usa.\"]'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0][\"clean_supporting_sentences\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'question', 'answer', 'passage', 'groundtruth_docs'], dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.rename(columns={\"clean_supporting_sentences\": \"groundtruth_docs\", \"supporting_docs_dedup\" : \"passage\"})\n",
    "df.drop(columns=['type', 'level', 'supporting_facts',\n",
    "       'context', 'supporting_docs', 'supporting_sentences'], inplace=True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>passage</th>\n",
       "      <th>groundtruth_docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5a7a06935542990198eaf050</td>\n",
       "      <td>Which magazine was started first Arthur's Maga...</td>\n",
       "      <td>Arthur's Magazine</td>\n",
       "      <td>[array([\"Arthur's Magazine (1844â€“1846) was an ...</td>\n",
       "      <td>[\"arthur's magazine (1844â€“1846) was an america...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5a879ab05542996e4f30887e</td>\n",
       "      <td>The Oberoi family is part of a hotel company t...</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>[array(['The Oberoi family is an Indian family...</td>\n",
       "      <td>['the oberoi family is an indian family that i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5a8d7341554299441c6b9fe5</td>\n",
       "      <td>Musician and satirist Allie Goertz wrote a son...</td>\n",
       "      <td>President Richard Nixon</td>\n",
       "      <td>[array(['Allison Beth \"Allie\" Goertz (born Mar...</td>\n",
       "      <td>['allison beth \"allie\" goertz (born march 2, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5a82171f5542990a1d231f4a</td>\n",
       "      <td>What nationality was James Henry Miller's wife?</td>\n",
       "      <td>American</td>\n",
       "      <td>[array(['Margaret \"Peggy\" Seeger (born June 17...</td>\n",
       "      <td>['margaret \"peggy\" seeger (born june 17, 1935)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5a84dd955542997b5ce3ff79</td>\n",
       "      <td>Cadmium Chloride is slightly soluble in this c...</td>\n",
       "      <td>alcohol</td>\n",
       "      <td>[array(['Cadmium chloride is a white crystalli...</td>\n",
       "      <td>['it is a hygroscopic solid that is highly sol...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id  \\\n",
       "0  5a7a06935542990198eaf050   \n",
       "1  5a879ab05542996e4f30887e   \n",
       "2  5a8d7341554299441c6b9fe5   \n",
       "3  5a82171f5542990a1d231f4a   \n",
       "4  5a84dd955542997b5ce3ff79   \n",
       "\n",
       "                                            question                   answer  \\\n",
       "0  Which magazine was started first Arthur's Maga...        Arthur's Magazine   \n",
       "1  The Oberoi family is part of a hotel company t...                    Delhi   \n",
       "2  Musician and satirist Allie Goertz wrote a son...  President Richard Nixon   \n",
       "3    What nationality was James Henry Miller's wife?                 American   \n",
       "4  Cadmium Chloride is slightly soluble in this c...                  alcohol   \n",
       "\n",
       "                                             passage  \\\n",
       "0  [array([\"Arthur's Magazine (1844â€“1846) was an ...   \n",
       "1  [array(['The Oberoi family is an Indian family...   \n",
       "2  [array(['Allison Beth \"Allie\" Goertz (born Mar...   \n",
       "3  [array(['Margaret \"Peggy\" Seeger (born June 17...   \n",
       "4  [array(['Cadmium chloride is a white crystalli...   \n",
       "\n",
       "                                    groundtruth_docs  \n",
       "0  [\"arthur's magazine (1844â€“1846) was an america...  \n",
       "1  ['the oberoi family is an indian family that i...  \n",
       "2  ['allison beth \"allie\" goertz (born march 2, 1...  \n",
       "3  ['margaret \"peggy\" seeger (born june 17, 1935)...  \n",
       "4  ['it is a hygroscopic solid that is highly sol...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "def to_python_list(s: str):\n",
    "    # Handle NaN/None\n",
    "    if pd.isna(s):\n",
    "        return []\n",
    "    s = str(s)\n",
    "\n",
    "    # 1) Convert NumPy-style repr to pure-Python list syntax\n",
    "    #    '[array([\"a\",\"b\"], dtype=object), array([\"c\"])]' -> '[[\"a\",\"b\"], [\"c\"]]'\n",
    "    s = s.replace(\"\\n\", \" \")\n",
    "    s = re.sub(r\"array\\(\", \"[\", s)\n",
    "    s = re.sub(r\",\\s*dtype=object\\)\", \"]\", s)\n",
    "\n",
    "    # 2) Safely parse into Python list\n",
    "    try:\n",
    "        obj = ast.literal_eval(s)\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "    # 3) Flatten any nesting\n",
    "    out = []\n",
    "    def _flatten(x):\n",
    "        if isinstance(x, (list, tuple)):\n",
    "            for y in x:\n",
    "                _flatten(y)\n",
    "        elif x is not None and str(x).strip().lower() != \"nan\":\n",
    "            out.append(str(x))\n",
    "    _flatten(obj)\n",
    "    return out\n",
    "\n",
    "def clean_items(items):\n",
    "    cleaned = []\n",
    "    for t in items:\n",
    "        # strip surrounding quotes/spaces, remove trailing punctuation spaces\n",
    "        tt = t.strip().strip('\"').strip(\"'\").strip()\n",
    "        # optional: remove trailing periods inside quotes duplicates\n",
    "        tt = re.sub(r'\\s+', ' ', tt)  # collapse extra spaces\n",
    "        # lowercase\n",
    "        tt = tt.lower()\n",
    "        cleaned.append(tt)\n",
    "    return cleaned\n",
    "\n",
    "# Apply to your DataFrame\n",
    "# df['passage'] is the column containing the raw strings like the example\n",
    "df[\"passage\"] = (\n",
    "    df[\"passage\"]\n",
    "    .apply(to_python_list)   # parse + flatten\n",
    "    .apply(clean_items)      # normalize each sentence and lowercase\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 90447 entries, 0 to 90446\n",
      "Data columns (total 6 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   Unnamed: 0        90447 non-null  int64 \n",
      " 1   id                90447 non-null  object\n",
      " 2   question          90447 non-null  object\n",
      " 3   answer            90447 non-null  object\n",
      " 4   passage           90447 non-null  object\n",
      " 5   groundtruth_docs  90447 non-null  object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 4.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# df_clean = pd.read_csv(\"hotpotqa_fulldataset_cleaned.csv\")\n",
    "df_clean = df.copy()\n",
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"arthur's magazine (1844â€“1846) was an american literary periodical published in philadelphia in the 19th century.\", 'edited by t.s. arthur, it featured work by edgar a. poe, j.h. ingraham, sarah josepha hale, thomas g. spear, and others.', \"in may 1846 it was merged into godey's lady's book.\", \"first for women is a woman's magazine published by bauer media group in the usa.\", 'the magazine was started in 1989.', 'it is based in englewood cliffs, new jersey.', 'in 2011 the circulation of the magazine was 1,310,696 copies.']\n",
      "[\"arthur's magazine (1844â€“1846) was an american literary periodical published in philadelphia in the 19th century.\", \"first for women is a woman's magazine published by bauer media group in the usa.\"]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import ast\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def parse_passage_cell(x, to_lower=True, strip_inner_quotes=True):\n",
    "    # Already a list â†’ clean each item\n",
    "    if isinstance(x, list):\n",
    "        items = x\n",
    "    else:\n",
    "        s = \"\" if pd.isna(x) else str(x).strip()\n",
    "        if not s:\n",
    "            return []\n",
    "        # If it's double-quoted JSON string, unwrap once\n",
    "        if (s.startswith('\"[') and s.endswith(']\"')) or (s.startswith(\"'[\") and s.endswith(\"]'\")):\n",
    "            try:\n",
    "                s = json.loads(s)\n",
    "            except Exception:\n",
    "                pass\n",
    "        # Try JSON first\n",
    "        items = None\n",
    "        if isinstance(s, str):\n",
    "            try:\n",
    "                obj = json.loads(s)\n",
    "                if isinstance(obj, list):\n",
    "                    items = obj\n",
    "            except Exception:\n",
    "                pass\n",
    "            # If not JSON, try Python literal (handles single quotes)\n",
    "            if items is None:\n",
    "                try:\n",
    "                    obj = ast.literal_eval(s)\n",
    "                    if isinstance(obj, list):\n",
    "                        items = obj\n",
    "                except Exception:\n",
    "                    pass\n",
    "        if items is None:\n",
    "            # Fallback: treat as one item\n",
    "            items = [s]\n",
    "\n",
    "    cleaned = []\n",
    "    for t in items:\n",
    "        t = \"\" if t is None else str(t)\n",
    "\n",
    "        # collapse whitespace\n",
    "        t = \" \".join(t.split())\n",
    "\n",
    "        # remove surrounding quotes\n",
    "        t = t.strip().strip('\"').strip(\"'\").strip()\n",
    "\n",
    "        # optional: remove inner double-quotes that wrap a phrase (your example)\n",
    "        # e.g., in may 1846 it was merged into \"godey's lady's book\". -> ... into godey's lady's book.\n",
    "        if strip_inner_quotes:\n",
    "            t = t.replace('\\\\\"', '\"')  # unescape\n",
    "            t = re.sub(r'\"([^\"]+)\"', r\"\\1\", t)\n",
    "\n",
    "        # optional: lowercase\n",
    "        if to_lower:\n",
    "            t = t.lower()\n",
    "\n",
    "        cleaned.append(t)\n",
    "    return cleaned\n",
    "\n",
    "# Apply to your DataFrame column \"passage\"\n",
    "# Example: df['passage'] contains stringified lists\n",
    "df_clean[\"passage\"] = df_clean[\"passage\"].apply(parse_passage_cell)\n",
    "df_clean[\"groundtruth_docs\"] = df_clean[\"groundtruth_docs\"].apply(parse_passage_cell)\n",
    "\n",
    "# Verify\n",
    "print(df_clean[\"passage\"].iloc[0])\n",
    "print(df_clean[\"groundtruth_docs\"].iloc[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.to_csv(\"hotpotqa_fulldataset_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Retriever models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csmala/miniconda3/envs/halu_rag/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Ground-truth sentences not found in corpus after normalization/dedup: 1/215661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Oct 16, 2025 11:38:45 AM org.apache.lucene.store.MemorySegmentIndexInputProvider <init>\n",
      "INFO: Using MemorySegmentIndexInput with Java 21; to disable start with -Dorg.apache.lucene.store.MMapDirectory.enableMemorySegments=false\n",
      "BM25 search: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90447/90447 [02:26<00:00, 619.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bm25 results saved to ./outputs/bm25_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "SPLADE encode: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42323/42323 [09:14<00:00, 76.39it/s]\n",
      "SPLADE search: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90447/90447 [2:58:26<00:00,  8.45it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splade results saved to ./outputs/splade_results.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from scipy.sparse import csr_matrix, save_npz, load_npz\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Utilities and helpers\n",
    "# -----------------------\n",
    "\n",
    "def ensure_dir(p):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def parse_list_like_strict(x):\n",
    "    \"\"\"\n",
    "    Return a real Python list[str] from:\n",
    "    - a Python list\n",
    "    - a JSON list string\n",
    "    - a Python-literal list string\n",
    "    - a double-encoded JSON list string\n",
    "    Never returns a nested string like '[\"...\"]' as a single item.\n",
    "    \"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return [str(s).strip() for s in x if isinstance(s, (str, int, float)) and str(s).strip()]\n",
    "\n",
    "    s = \"\" if pd.isna(x) else str(x).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "\n",
    "    if (s.startswith('\"[') and s.endswith(']\"')) or (s.startswith(\"'[\") and s.endswith(\"]'\")):\n",
    "        try:\n",
    "            s = json.loads(s)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if isinstance(s, str):\n",
    "        # Try JSON\n",
    "        try:\n",
    "            obj = json.loads(s)\n",
    "            if isinstance(obj, list):\n",
    "                return [str(z).strip() for z in obj if str(z).strip()]\n",
    "        except Exception:\n",
    "            pass\n",
    "        # Try Python literal\n",
    "        try:\n",
    "            obj = ast.literal_eval(s)\n",
    "            if isinstance(obj, list):\n",
    "                return [str(z).strip() for z in obj if str(z).strip()]\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return [s]\n",
    "\n",
    "def normalize_sentence(s: str) -> str:\n",
    "    s = s.replace(\"\\u00a0\", \" \")\n",
    "    s = \" \".join(s.split()).strip()\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'^[\"â€œâ€]+|[\"â€œâ€]+$', \"\", s)\n",
    "    return s\n",
    "\n",
    "def build_sentence_corpus_with_row_ids(df, passage_col=\"passage\"):\n",
    "    \"\"\"\n",
    "    Build a sentence-level corpus where each sentence is one document.\n",
    "    Doc IDs are 'rowIdx_sentIdx' (e.g., '1_0').\n",
    "    Global dedup is applied: identical normalized sentences map to a single doc_id chosen by first occurrence.\n",
    "    Returns:\n",
    "      - corpus_texts: list[str] aligned with corpus_doc_ids\n",
    "      - corpus_doc_ids: list[str] like 'row_sent'\n",
    "      - sentence_to_docid: dict[str normalized sentence -> doc_id]\n",
    "    \"\"\"\n",
    "    corpus_texts = []\n",
    "    corpus_doc_ids = []\n",
    "    sentence_to_docid = {}  # normalized sentence -> canonical doc_id\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        raw_list = parse_list_like_strict(df.loc[i, passage_col])\n",
    "        sents = [normalize_sentence(s) for s in raw_list if s and str(s).strip()]\n",
    "        for j, sent in enumerate(sents):\n",
    "            did = f\"{i}_{j}\"\n",
    "            if sent not in sentence_to_docid:\n",
    "                sentence_to_docid[sent] = did\n",
    "                corpus_texts.append(sent)\n",
    "                corpus_doc_ids.append(did)\n",
    "\n",
    "    return corpus_texts, corpus_doc_ids, sentence_to_docid\n",
    "\n",
    "def build_groundtruth_rel_sets(df, groundtruth_col, sentence_to_docid):\n",
    "    \"\"\"\n",
    "    For each query row, parse groundtruth_docs into a set of doc_ids using the canonical mapping.\n",
    "    Returns rel_sets: dict[qid -> set(doc_id)]\n",
    "    \"\"\"\n",
    "    rel_sets = {}\n",
    "    missing = 0\n",
    "    total = 0\n",
    "    for i in range(len(df)):\n",
    "        gt_list = parse_list_like_strict(df.loc[i, groundtruth_col])\n",
    "        gt_norm = [normalize_sentence(x) for x in gt_list if x and str(x).strip()]\n",
    "        rel = set()\n",
    "        for g in gt_norm:\n",
    "            total += 1\n",
    "            did = sentence_to_docid.get(g)\n",
    "            if did is not None:\n",
    "                rel.add(did)\n",
    "            else:\n",
    "                missing += 1\n",
    "        rel_sets[str(i)] = rel\n",
    "    if total > 0 and missing > 0:\n",
    "        print(f\"[Info] Ground-truth sentences not found in corpus after normalization/dedup: {missing}/{total}\")\n",
    "    return rel_sets\n",
    "\n",
    "def ap_at_k_multi(ret_ids, rel_ids_set, k):\n",
    "    score = 0.0\n",
    "    hit = 0\n",
    "    for rank, did in enumerate(ret_ids[:k], 1):\n",
    "        if did in rel_ids_set:\n",
    "            hit += 1\n",
    "            score += hit / rank\n",
    "    denom = min(len(rel_ids_set), k)\n",
    "    return score / denom if denom > 0 else 0.0\n",
    "\n",
    "def ndcg_at_k_multi(ret_ids, rel_ids_set, k):\n",
    "    dcg = 0.0\n",
    "    for rank, did in enumerate(ret_ids[:k], 1):\n",
    "        if did in rel_ids_set:\n",
    "            dcg += 1.0 / math.log2(rank + 1)\n",
    "    ideal = min(len(rel_ids_set), k)\n",
    "    idcg = sum(1.0 / math.log2(r + 1) for r in range(1, ideal + 1))\n",
    "    return (dcg / idcg) if idcg > 0 else 0.0\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# BM25 (Pyserini/Lucene)\n",
    "# -----------------------\n",
    "\n",
    "class BM25Retriever:\n",
    "    def __init__(self, index_dir=\"./indices/bm25\", threads=8, java_mem=\"8g\"):\n",
    "        self.index_dir = index_dir\n",
    "        self.threads = threads\n",
    "        self.java_mem = java_mem\n",
    "\n",
    "    def build_index(self, corpus_texts, corpus_doc_ids, work_dir=\"./work/bm25\"):\n",
    "        ensure_dir(self.index_dir); ensure_dir(work_dir)\n",
    "        corpus_dir = os.path.join(work_dir, \"json_corpus\"); ensure_dir(corpus_dir)\n",
    "        docs_path = os.path.join(corpus_dir, \"docs.jsonl\")\n",
    "        with open(docs_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for text, did in zip(corpus_texts, corpus_doc_ids):\n",
    "                f.write(json.dumps({\"id\": did, \"contents\": text}, ensure_ascii=False) + \"\\n\")\n",
    "        cmd = [\n",
    "            \"python\",\"-m\",\"pyserini.index.lucene\",\n",
    "            \"--collection\",\"JsonCollection\",\n",
    "            \"--input\", corpus_dir,\n",
    "            \"--index\", self.index_dir,\n",
    "            \"--generator\",\"DefaultLuceneDocumentGenerator\",\n",
    "            \"--threads\", str(self.threads),\n",
    "            \"--storePositions\",\"--storeDocvectors\",\"--storeRaw\"\n",
    "        ]\n",
    "        env = os.environ.copy()\n",
    "        env[\"JAVA_TOOL_OPTIONS\"] = f\"-Xms{self.java_mem} -Xmx{self.java_mem}\"\n",
    "        res = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, env=env)\n",
    "        if res.returncode != 0:\n",
    "            print(res.stdout); print(res.stderr)\n",
    "            raise RuntimeError(\"BM25 indexing failed (check Java 11+ and pyserini).\")\n",
    "\n",
    "    def retrieve(self, queries, topk=10):\n",
    "        from pyserini.search.lucene import LuceneSearcher\n",
    "        searcher = LuceneSearcher(self.index_dir)\n",
    "        results = {}\n",
    "        for qid, q in tqdm(queries.items(), desc=\"BM25 search\"):\n",
    "            hits = searcher.search(q, k=topk)\n",
    "            results[qid] = [(h.docid, float(h.score)) for h in hits]\n",
    "        return results\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# SPLADE (Transformers + CSR)\n",
    "# -----------------------\n",
    "\n",
    "class SPLADERetriever:\n",
    "    def __init__(self, index_dir=\"./indices/splade\", model_name=\"naver/splade-cocondenser-ensembledistil\",\n",
    "                 batch_size=8, max_length=256, min_weight=0.01):\n",
    "        self.index_dir = index_dir\n",
    "        self.model_name = model_name\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "        self.min_weight = min_weight\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.tok = None\n",
    "        self.model = None\n",
    "\n",
    "    def _load(self):\n",
    "        if self.tok is None:\n",
    "            self.tok = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        if self.model is None:\n",
    "            self.model = AutoModelForMaskedLM.from_pretrained(self.model_name).to(self.device).eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _encode_texts(self, texts):\n",
    "        self._load()\n",
    "        V = self.model.config.vocab_size\n",
    "        data, indices, indptr = [], [], [0]\n",
    "        for i in tqdm(range(0, len(texts), self.batch_size), desc=\"SPLADE encode\"):\n",
    "            batch = texts[i:i+self.batch_size]\n",
    "            toks = self.tok(batch, return_tensors=\"pt\", padding=True, truncation=True,\n",
    "                            max_length=self.max_length).to(self.device)\n",
    "            logits = self.model(**toks).logits  # [B,L,V]\n",
    "            activ = torch.log1p(torch.relu(logits))\n",
    "            weights = activ.max(dim=1).values.cpu().numpy()  # [B,V]\n",
    "            for row in weights:\n",
    "                nz = np.where(row >= self.min_weight)[0]\n",
    "                indices.extend(nz.tolist())\n",
    "                data.extend(row[nz].astype(np.float32).tolist())\n",
    "                indptr.append(len(indices))\n",
    "        return csr_matrix((np.array(data, np.float32),\n",
    "                           np.array(indices, np.int32),\n",
    "                           np.array(indptr, np.int32)),\n",
    "                          shape=(len(texts), V), dtype=np.float32)\n",
    "\n",
    "    def build_index(self, corpus_texts, corpus_doc_ids):\n",
    "        ensure_dir(self.index_dir)\n",
    "        texts = [\"\" if pd.isna(t) else str(t) for t in corpus_texts]\n",
    "        mat = self._encode_texts(texts)\n",
    "        save_npz(os.path.join(self.index_dir, \"docs.npz\"), mat)\n",
    "        with open(os.path.join(self.index_dir, \"doc_ids.json\"), \"w\") as f:\n",
    "            json.dump(list(corpus_doc_ids), f)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _encode_query(self, q):\n",
    "        self._load()\n",
    "        toks = self.tok([q], return_tensors=\"pt\", padding=True, truncation=True,\n",
    "                        max_length=self.max_length).to(self.device)\n",
    "        logits = self.model(**toks).logits\n",
    "        activ = torch.log1p(torch.relu(logits))\n",
    "        w = activ.max(dim=1).values.squeeze(0).cpu().numpy()\n",
    "        nz = np.where(w >= self.min_weight)[0]\n",
    "        return csr_matrix((w[nz].astype(np.float32), nz.astype(np.int32), np.array([0,len(nz)], np.int32)),\n",
    "                          shape=(1, self.model.config.vocab_size), dtype=np.float32)\n",
    "\n",
    "    def retrieve(self, queries, topk=10):\n",
    "        docs = load_npz(os.path.join(self.index_dir, \"docs.npz\"))\n",
    "        with open(os.path.join(self.index_dir, \"doc_ids.json\"), \"r\") as f:\n",
    "            doc_ids = json.load(f)\n",
    "        out = {}\n",
    "        for qid, q in tqdm(queries.items(), desc=\"SPLADE search\"):\n",
    "            qv = self._encode_query(q)  # [1,V]\n",
    "            scores = (docs @ qv.T).toarray().ravel()\n",
    "            if topk >= len(doc_ids):\n",
    "                idx = np.argsort(-scores)\n",
    "            else:\n",
    "                idx = np.argpartition(scores, -topk)[-topk:]\n",
    "                idx = idx[np.argsort(-scores[idx])]\n",
    "            out[qid] = [(doc_ids[i], float(scores[i])) for i in idx[:topk]]\n",
    "        return out\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Pipeline\n",
    "# -----------------------\n",
    "\n",
    "def run_pipeline(df,\n",
    "                 output_dir=\"./outputs\",\n",
    "                 bm25_index_dir=\"./indices/bm25\",\n",
    "                 splade_index_dir=\"./indices/splade\",\n",
    "                 work_dir=\"./work\",\n",
    "                 topk=10,\n",
    "                 groundtruth_col=\"groundtruth_docs\"):\n",
    "    # Required columns\n",
    "    for col in [\"question\", \"passage\", groundtruth_col]:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Missing required column '{col}'.\")\n",
    "    if \"answer\" not in df.columns:\n",
    "        df[\"answer\"] = \"\"\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # Build sentence-level corpus with per-row doc_ids like \"i_j\", dedup globally\n",
    "    corpus_texts, corpus_doc_ids, sentence_to_docid = build_sentence_corpus_with_row_ids(df, \"passage\")\n",
    "    corpus_lookup = {did: txt for did, txt in zip(corpus_doc_ids, corpus_texts)}\n",
    "\n",
    "    # Build relevance sets from groundtruth_docs (NOT from passage)\n",
    "    rel_sets = build_groundtruth_rel_sets(df, groundtruth_col, sentence_to_docid)\n",
    "\n",
    "    # Queries\n",
    "    queries = {str(i): str(df.loc[i, \"question\"]) for i in range(len(df))}\n",
    "\n",
    "    ensure_dir(output_dir); ensure_dir(os.path.dirname(bm25_index_dir)); ensure_dir(os.path.dirname(splade_index_dir)); ensure_dir(work_dir)\n",
    "\n",
    "    # BM25\n",
    "    bm25 = BM25Retriever(index_dir=bm25_index_dir)\n",
    "    bm25.build_index(corpus_texts, corpus_doc_ids, work_dir=os.path.join(work_dir, \"bm25\"))\n",
    "    bm25_res = bm25.retrieve(queries, topk=topk)\n",
    "    save_results(\"bm25\", df, \"passage\", groundtruth_col, bm25_res, rel_sets, corpus_lookup, os.path.join(output_dir, \"bm25_results.csv\"))\n",
    "\n",
    "    # SPLADE\n",
    "    splade = SPLADERetriever(index_dir=splade_index_dir)\n",
    "    splade.build_index(corpus_texts, corpus_doc_ids)\n",
    "    splade_res = splade.retrieve(queries, topk=topk)\n",
    "    save_results(\"splade\", df, \"passage\", groundtruth_col, splade_res, rel_sets, corpus_lookup, os.path.join(output_dir, \"splade_results.csv\"))\n",
    "\n",
    "def save_results(name, df, passage_col, groundtruth_col, ret, rel_sets, corpus_lookup, out_csv):\n",
    "    rows = []\n",
    "    ks = [3, 5, 10]\n",
    "    for i in range(len(df)):\n",
    "        qid = str(i)\n",
    "        retrieved = ret.get(qid, [])\n",
    "        ret_ids = [d for d, _ in retrieved]\n",
    "        rel_set = rel_sets.get(qid, set())\n",
    "\n",
    "        metrics = {}\n",
    "        for k in ks:\n",
    "            metrics[f\"MAP@{k}\"] = ap_at_k_multi(ret_ids, rel_set, k)\n",
    "            metrics[f\"NDCG@{k}\"] = ndcg_at_k_multi(ret_ids, rel_set, k)\n",
    "\n",
    "        # NEW: store the complete document text, plus a short preview_snippet for convenience\n",
    "        packed = []\n",
    "        for d, s in retrieved:\n",
    "            full = corpus_lookup.get(d, \"\")\n",
    "            # Keep the document exactly as in the corpus (already normalized at sentence level)\n",
    "            preview = full[:200].replace(\"\\n\", \" \")\n",
    "            packed.append({\n",
    "                \"doc_id\": d,\n",
    "                \"score\": round(float(s), 4),\n",
    "                \"full_text\": full,\n",
    "                \"preview_snippet\": preview\n",
    "            })\n",
    "\n",
    "        # Preserve normalized passage and GT (as you had)\n",
    "        passage_list = [normalize_sentence(x) for x in parse_list_like_strict(df.loc[i, passage_col])]\n",
    "        gt_list = [normalize_sentence(x) for x in parse_list_like_strict(df.loc[i, groundtruth_col])]\n",
    "\n",
    "        rows.append({\n",
    "            \"question\": str(df.loc[i, \"question\"]),\n",
    "            \"answer\": \"\" if \"answer\" not in df.columns or pd.isna(df.loc[i, \"answer\"]) else str(df.loc[i, \"answer\"]),\n",
    "            \"passage\": json.dumps(passage_list, ensure_ascii=False),\n",
    "            \"groundtruth_docs\": json.dumps(gt_list, ensure_ascii=False),\n",
    "            f\"{name}_ret_docs\": json.dumps(packed, ensure_ascii=False),\n",
    "            **metrics\n",
    "        })\n",
    "\n",
    "    pd.DataFrame(rows).to_csv(out_csv, index=False)\n",
    "    print(f\"{name} results saved to {out_csv}\")\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Example usage\n",
    "# -----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # df must have: question, answer, passage (list), groundtruth_docs (list)\n",
    "    # For example:\n",
    "    # df = pd.read_parquet(\"your_dataset.parquet\")\n",
    "    # Or:\n",
    "    df = pd.read_csv(\"hotpotqa_fulldataset_cleaned.csv\")\n",
    "    # Optional: subset for quick test\n",
    "    # df = df.head(300)\n",
    "\n",
    "\n",
    "    run_pipeline(\n",
    "        df,\n",
    "        topk=10,\n",
    "        groundtruth_col=\"groundtruth_docs\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 Results:\n",
      "  MAP@3: 0.4529, NDCG@3: 0.5334\n",
      "  MAP@5: 0.4685, NDCG@5: 0.5571\n",
      "  MAP@10: 0.4829, NDCG@10: 0.5819\n",
      "\n",
      "SPLADE Results:\n",
      "  MAP@3: 0.4913, NDCG@3: 0.5690\n",
      "  MAP@5: 0.5070, NDCG@5: 0.5922\n",
      "  MAP@10: 0.5211, NDCG@10: 0.6156\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "for name, path in [(\"BM25\",\"./outputs/bm25_results.csv\"), (\"SPLADE\",\"./outputs/splade_results.csv\")]:\n",
    "    df = pd.read_csv(path)\n",
    "    print(name, \"Results:\")\n",
    "    for k in (3,5,10):\n",
    "        print(f\"  MAP@{k}: {pd.to_numeric(df[f'MAP@{k}'], errors='coerce').mean():.4f}, \"\n",
    "              f\"NDCG@{k}: {pd.to_numeric(df[f'NDCG@{k}'], errors='coerce').mean():.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Retriever models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBuilds a sentence-level corpus from passage (each sentence = one doc, deduped across the dataset).\\nSupports two dense models:\\nsentence-transformers/all-mpnet-base-v2\\nfacebook/contriever\\nUses FAISS for indexing/search.\\nRetrieves top-k per question, saves results to CSV, and computes MAP/NDCG@3/5/10 against groundtruth_docs (multiple relevant sentences per question).\\nReuses the same helpers and evaluation style you already have.\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Uses sentence-level documents from the passage column (each item is a doc), with global dedup.\n",
    "Evaluates MAP/NDCG strictly against groundtruth_docs (not passage).\n",
    "Uses the same normalization on corpus, queries, and groundtruth to avoid string-mismatch pitfalls.\n",
    "Uses FAISS IndexFlatIP with L2-normalized embeddings (cosine-like) for indexing/search..\n",
    "Uses two proper dense encoders:\n",
    "sentence-transformers/all-mpnet-base-v2 via AutoModel + mean pooling + L2 norm\n",
    "facebook/contriever via AutoModel + mean pooling + L2 norm\n",
    "Retrieves top-k per question, and computes MAP/NDCG@3/5/10 against groundtruth_docs (multiple relevant docs per question).\n",
    "Stores retrieved docs, normalized passage, normalized groundtruth_docs, and metrics in CSV.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csmala/miniconda3/envs/halu_rag/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Ground-truth sentences not found in corpus after normalization/dedup: 1/215661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encode sentence-transformers/multi-qa-mpnet-base-dot-v1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5291/5291 [06:30<00:00, 13.56it/s]\n",
      "Encode sentence-transformers/multi-qa-mpnet-base-dot-v1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1414/1414 [01:37<00:00, 14.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense results saved to ./outputs_dense/hotpotqa_multiqa.csv\n"
     ]
    }
   ],
   "source": [
    "#New\n",
    "\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import faiss\n",
    "\n",
    "import os\n",
    "os.makedirs(\"./outputs_dense\", exist_ok=True)\n",
    "\n",
    "# -----------------------\n",
    "# Normalization & parsing\n",
    "# -----------------------\n",
    "\n",
    "def ensure_dir(p):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def parse_list_like_strict(x):\n",
    "    \"\"\"\n",
    "    Robust parsing of lists that may be:\n",
    "    - Python list\n",
    "    - JSON list string\n",
    "    - Python literal list string\n",
    "    - Double-encoded JSON list string\n",
    "    Always returns a flat list[str] without nested stringified lists.\n",
    "    \"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return [str(s).strip() for s in x if isinstance(s, (str, int, float)) and str(s).strip()]\n",
    "\n",
    "    s = \"\" if pd.isna(x) else str(x).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "\n",
    "    # Unwrap double-encoded JSON arrays like \"\\\"[...]\"\"\n",
    "    if (s.startswith('\"[') and s.endswith(']\"')) or (s.startswith(\"'[\") and s.endswith(\"]'\")):\n",
    "        try:\n",
    "            s = json.loads(s)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if isinstance(s, str):\n",
    "        # Try JSON\n",
    "        try:\n",
    "            obj = json.loads(s)\n",
    "            if isinstance(obj, list):\n",
    "                return [str(z).strip() for z in obj if str(z).strip()]\n",
    "        except Exception:\n",
    "            pass\n",
    "        # Try Python literal\n",
    "        try:\n",
    "            obj = ast.literal_eval(s)\n",
    "            if isinstance(obj, list):\n",
    "                return [str(z).strip() for z in obj if str(z).strip()]\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return [s]\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    if s is None or (isinstance(s, float) and np.isnan(s)):\n",
    "        return \"\"\n",
    "    s = str(s).replace(\"\\u00a0\", \" \")\n",
    "    s = \" \".join(s.split()).strip()\n",
    "    s = s.lower()\n",
    "    # strip wrapping quotes\n",
    "    s = re.sub(r'^[\"â€œâ€]+|[\"â€œâ€]+$', \"\", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Corpus & relevance mapping (sentence-level)\n",
    "# -----------------------\n",
    "\n",
    "def build_sentence_corpus_and_mappings(df, passage_col=\"passage\"):\n",
    "    \"\"\"\n",
    "    Build a global deduplicated corpus of sentences (each sentence is a doc).\n",
    "    - corpus_texts: list[str] of unique normalized sentences\n",
    "    - corpus_doc_ids: list[str] of doc IDs \"rowIdx_sentIdx\" for FIRST occurrence of that sentence\n",
    "    - sentence_to_docid: dict normalized sentence -> canonical doc_id\n",
    "    - row_sentence_docsets: dict qid -> set(doc_ids) for that row's sentences (useful for checks)\n",
    "    \"\"\"\n",
    "    corpus_texts = []\n",
    "    corpus_doc_ids = []\n",
    "    sentence_to_docid = {}\n",
    "    row_sentence_docsets = {}\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        sents_raw = parse_list_like_strict(df.loc[i, passage_col])\n",
    "        sents = [normalize_text(s) for s in sents_raw if s and str(s).strip()]\n",
    "        docset = set()\n",
    "        for j, sent in enumerate(sents):\n",
    "            did = f\"{i}_{j}\"\n",
    "            if sent not in sentence_to_docid:\n",
    "                sentence_to_docid[sent] = did\n",
    "                corpus_texts.append(sent)\n",
    "                corpus_doc_ids.append(did)\n",
    "            else:\n",
    "                did = sentence_to_docid[sent]\n",
    "            docset.add(did)\n",
    "        row_sentence_docsets[str(i)] = docset\n",
    "\n",
    "    return corpus_texts, corpus_doc_ids, sentence_to_docid, row_sentence_docsets\n",
    "\n",
    "def build_groundtruth_rel_sets(df, groundtruth_col, sentence_to_docid):\n",
    "    \"\"\"\n",
    "    For each row, map groundtruth_docs sentences -> canonical doc_ids using sentence_to_docid.\n",
    "    Returns rel_sets: dict[qid -> set(doc_id)]\n",
    "    \"\"\"\n",
    "    rel_sets = {}\n",
    "    missing = 0\n",
    "    total = 0\n",
    "    for i in range(len(df)):\n",
    "        gt_list = parse_list_like_strict(df.loc[i, groundtruth_col])\n",
    "        gt_norm = [normalize_text(x) for x in gt_list if x and str(x).strip()]\n",
    "        rel = set()\n",
    "        for g in gt_norm:\n",
    "            total += 1\n",
    "            did = sentence_to_docid.get(g)\n",
    "            if did is not None:\n",
    "                rel.add(did)\n",
    "            else:\n",
    "                missing += 1\n",
    "        rel_sets[str(i)] = rel\n",
    "    if total > 0 and missing > 0:\n",
    "        print(f\"[Info] Ground-truth sentences not found in corpus after normalization/dedup: {missing}/{total}\")\n",
    "    return rel_sets\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Metrics\n",
    "# -----------------------\n",
    "\n",
    "def ap_at_k_multi(ret_ids, rel_ids_set, k):\n",
    "    score = 0.0\n",
    "    hit = 0\n",
    "    for rank, did in enumerate(ret_ids[:k], 1):\n",
    "        if did in rel_ids_set:\n",
    "            hit += 1\n",
    "            score += hit / rank\n",
    "    denom = min(len(rel_ids_set), k)\n",
    "    return score / denom if denom > 0 else 0.0\n",
    "\n",
    "def ndcg_at_k_multi(ret_ids, rel_ids_set, k):\n",
    "    dcg = 0.0\n",
    "    for rank, did in enumerate(ret_ids[:k], 1):\n",
    "        if did in rel_ids_set:\n",
    "            dcg += 1.0 / math.log2(rank + 1)\n",
    "    ideal = min(len(rel_ids_set), k)\n",
    "    idcg = sum(1.0 / math.log2(r + 1) for r in range(1, ideal + 1))\n",
    "    return (dcg / idcg) if idcg > 0 else 0.0\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Dense encoders (AutoModel + mean pooling + L2 norm)\n",
    "# -----------------------\n",
    "\n",
    "def mean_pool(last_hidden_state, attention_mask):\n",
    "    # last_hidden_state: [B, L, H], attention_mask: [B, L]\n",
    "    mask = attention_mask.unsqueeze(-1).type_as(last_hidden_state)  # [B,L,1]\n",
    "    summed = (last_hidden_state * mask).sum(dim=1)                  # [B,H]\n",
    "    denom = mask.sum(dim=1).clamp(min=1e-9)                         # [B,1]\n",
    "    return summed / denom\n",
    "\n",
    "class DenseEncoder:\n",
    "    \"\"\"\n",
    "    Proper dense encoding with AutoModel:\n",
    "      - sentence-transformers/all-mpnet-base-v2 (AutoModel + mean pooling)\n",
    "      - facebook/contriever (AutoModel + mean pooling)\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str, device: str = None, max_length: int = 256, batch_size: int = 64):\n",
    "        self.model_name = model_name\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tok = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(self.device).eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode(self, texts):\n",
    "        vecs = []\n",
    "        for i in tqdm(range(0, len(texts), self.batch_size), desc=f\"Encode {self.model_name}\"):\n",
    "            batch = texts[i:i+self.batch_size]\n",
    "            toks = self.tok(\n",
    "                batch,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(self.device)\n",
    "            out = self.model(**toks)\n",
    "            emb = mean_pool(out.last_hidden_state, toks[\"attention_mask\"])\n",
    "            emb = torch.nn.functional.normalize(emb, p=2, dim=1)\n",
    "            vecs.append(emb.cpu().numpy().astype(np.float32))\n",
    "        return np.vstack(vecs) if len(vecs) else np.zeros((0, self.model.config.hidden_size), dtype=np.float32)\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# FAISS helpers\n",
    "# -----------------------\n",
    "\n",
    "def build_faiss_index(embs: np.ndarray, use_gpu=False):\n",
    "    d = embs.shape[1]\n",
    "    index = faiss.IndexFlatIP(d)  # cosine-like when vectors are L2-normalized\n",
    "    if use_gpu and faiss.get_num_gpus() > 0:\n",
    "        res = faiss.StandardGpuResources()\n",
    "        index = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "    index.add(embs)\n",
    "    return index\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Dense Pipeline (HotpotQA) with groundtruth_docs evaluation\n",
    "# -----------------------\n",
    "\n",
    "def run_dense_pipeline_hotpotqa(\n",
    "    df: pd.DataFrame,\n",
    "    model_name: str,\n",
    "    output_csv: str,\n",
    "    faiss_dir: str,\n",
    "    topk: int = 10,\n",
    "    max_length: int = 256,\n",
    "    batch_size: int = 64,\n",
    "    groundtruth_col: str = \"groundtruth_docs\",\n",
    "    passage_col: str = \"passage\",\n",
    "):\n",
    "    # Validate columns\n",
    "    for col in [\"question\", passage_col, groundtruth_col]:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Missing required column: {col}\")\n",
    "    if \"answer\" not in df.columns:\n",
    "        df[\"answer\"] = \"\"\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # Build sentence-level corpus and canonical mapping\n",
    "    corpus_texts, corpus_doc_ids, sentence_to_docid, _ = build_sentence_corpus_and_mappings(df, passage_col=passage_col)\n",
    "    corpus_lookup = {did: txt for did, txt in zip(corpus_doc_ids, corpus_texts)}\n",
    "\n",
    "    # Build relevance sets from groundtruth_docs only\n",
    "    rel_sets = build_groundtruth_rel_sets(df, groundtruth_col, sentence_to_docid)\n",
    "\n",
    "    # Prepare normalized queries\n",
    "    queries = [normalize_text(df.loc[i, \"question\"]) for i in range(len(df))]\n",
    "\n",
    "    # Encode corpus\n",
    "    ensure_dir(faiss_dir)\n",
    "    encoder = DenseEncoder(model_name=model_name, max_length=max_length, batch_size=batch_size)\n",
    "    corpus_vecs = encoder.encode(corpus_texts)  # [N, D]\n",
    "\n",
    "    # Build FAISS\n",
    "    index = build_faiss_index(corpus_vecs, use_gpu=False)\n",
    "\n",
    "    # Encode queries and search\n",
    "    query_vecs = encoder.encode(queries)  # [Q, D]\n",
    "    scores, idxs = index.search(query_vecs, topk)  # [Q, topk]\n",
    "    doc_ids_array = np.array(corpus_doc_ids)\n",
    "\n",
    "    # Collect rows and compute metrics\n",
    "    rows = []\n",
    "    ks = [3, 5, 10]\n",
    "    for i in range(len(df)):\n",
    "        qid = str(i)\n",
    "        retrieved = []\n",
    "        for r in range(topk):\n",
    "            j = int(idxs[i, r])\n",
    "            if j < 0 or j >= len(doc_ids_array):\n",
    "                continue\n",
    "            did = doc_ids_array[j]\n",
    "            sc = float(scores[i, r])\n",
    "            retrieved.append((did, sc))\n",
    "        ret_ids = [d for d, _ in retrieved]\n",
    "        rel_set = rel_sets.get(qid, set())\n",
    "        metrics = {f\"MAP@{k}\": ap_at_k_multi(ret_ids, rel_set, k) for k in ks}\n",
    "        metrics.update({f\"NDCG@{k}\": ndcg_at_k_multi(ret_ids, rel_set, k) for k in ks})\n",
    "\n",
    "        # Store complete doc text + short preview (keeps CSV readable while enabling exact text eval later)\n",
    "        pack = []\n",
    "        for d, s in retrieved:\n",
    "            full = corpus_lookup.get(d, \"\")\n",
    "            preview = full[:200].replace(\"\\n\", \" \")\n",
    "            pack.append({\n",
    "                \"doc_id\": d,\n",
    "                \"score\": round(float(s), 6),\n",
    "                \"full_text\": full,\n",
    "                \"preview_snippet\": preview\n",
    "            })\n",
    "\n",
    "        passage_list = [normalize_text(x) for x in parse_list_like_strict(df.loc[i, passage_col])]\n",
    "        gt_list = [normalize_text(x) for x in parse_list_like_strict(df.loc[i, groundtruth_col])]\n",
    "\n",
    "        rows.append({\n",
    "            \"question\": normalize_text(df.loc[i, \"question\"]),\n",
    "            \"answer\": normalize_text(df.loc[i, \"answer\"]),\n",
    "            \"passage\": json.dumps(passage_list, ensure_ascii=False),\n",
    "            \"groundtruth_docs\": json.dumps(gt_list, ensure_ascii=False),\n",
    "            f\"{model_name.split('/')[-1]}_ret_docs\": json.dumps(pack, ensure_ascii=False),\n",
    "            **metrics\n",
    "        })\n",
    "\n",
    "    pd.DataFrame(rows).to_csv(output_csv, index=False)\n",
    "    print(f\"Dense results saved to {output_csv}\")\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Run both models if called directly\n",
    "# -----------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example:\n",
    "    df = pd.read_csv(\"hotpotqa_fulldataset_cleaned.csv\")\n",
    "    # Ensure df has columns: question, answer, passage (list or stringified list), groundtruth_docs (list or stringified list)\n",
    "\n",
    "    # Uncomment and set your path:\n",
    "    # df = pd.read_csv(\"hotpotqa_fulldataset_cleaned.csv\")\n",
    "\n",
    "    # Example runs:\n",
    "    run_dense_pipeline_hotpotqa(\n",
    "        df,\n",
    "        model_name=\"sentence-transformers/multi-qa-mpnet-base-dot-v1\",\n",
    "        output_csv=\"./outputs_dense/hotpotqa_multiqa.csv\",\n",
    "        faiss_dir=\"./indices_dense/faiss_multiqa\",\n",
    "        topk=10,\n",
    "        max_length=256,\n",
    "        batch_size=64,\n",
    "        groundtruth_col=\"groundtruth_docs\",\n",
    "        passage_col=\"passage\",\n",
    "    )\n",
    "\n",
    "    # run_dense_pipeline_hotpotqa(\n",
    "    #     df,\n",
    "    #     model_name=\"facebook/contriever\",\n",
    "    #     output_csv=\"./outputs_dense/hotpotqa_contriever.csv\",\n",
    "    #     faiss_dir=\"./indices_dense/faiss_contriever\",\n",
    "    #     topk=10,\n",
    "    #     max_length=256,\n",
    "    #     batch_size=64,\n",
    "    #     groundtruth_col=\"groundtruth_docs\",\n",
    "    #     passage_col=\"passage\",\n",
    "    # )\n",
    "    # pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPNet Results:\n",
      "  MAP@3: 0.3314, NDCG@3: 0.4034\n",
      "  MAP@5: 0.3461, NDCG@5: 0.4272\n",
      "  MAP@10: 0.3606, NDCG@10: 0.4539\n",
      "\n",
      "Contriever Results:\n",
      "  MAP@3: 0.3212, NDCG@3: 0.3966\n",
      "  MAP@5: 0.3396, NDCG@5: 0.4256\n",
      "  MAP@10: 0.3555, NDCG@10: 0.4545\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# results summary\n",
    "import pandas as pd\n",
    "for name, path in [(\"MPNet\",\"./outputs_dense/hotpotqa_mpnet.csv\"), (\"Contriever\",\"./outputs_dense/hotpotqa_contriever.csv\")]:\n",
    "    df = pd.read_csv(path)\n",
    "    print(name, \"Results:\")\n",
    "    for k in (3,5,10):\n",
    "        print(f\"  MAP@{k}: {pd.to_numeric(df[f'MAP@{k}'], errors='coerce').mean():.4f}, \"\n",
    "              f\"NDCG@{k}: {pd.to_numeric(df[f'NDCG@{k}'], errors='coerce').mean():.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "halu_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
