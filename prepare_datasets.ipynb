{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Halubench datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: output_files/halubench_hybrid_contriever_combined_results.csv\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Load inputs\n",
    "# df_halu_hybrid = pd.read_csv('input_files/halubench_hybrid_linear_with_rerank_minilm_alpha_0.3_beta_0.85.csv')\n",
    "# df_halu_cont = pd.read_csv('input_files/contriever_results.csv')\n",
    "# df = pd.read_parquet(\"hf://datasets/PatronusAI/HaluBench/data/test-00000-of-00001.parquet\")\n",
    "\n",
    "# # Keep only the needed column from contriever results\n",
    "# df_halu_cont = df_halu_cont[['contriever_ret_docs']]\n",
    "\n",
    "# # Keep only the needed column from the parquet df\n",
    "# df_source = df[['source_ds']]\n",
    "\n",
    "# # Concatenate columns side-by-side with df_halu_hybrid\n",
    "# # This assumes the rows align in order across the three inputs.\n",
    "# # If they don't align, merge on a key instead of concat.\n",
    "# df_out = pd.concat([df_halu_hybrid.reset_index(drop=True),\n",
    "#                     df_halu_cont.reset_index(drop=True),\n",
    "#                     df_source.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# # Reorder/select final columns\n",
    "# final_cols = ['question', 'answer', 'groundtruth_with_ids',\n",
    "#               'hybrid_reranked_docs', 'contriever_ret_docs', 'source_ds']\n",
    "# df_out = df_out[final_cols]\n",
    "\n",
    "# # Save to CSV\n",
    "# output_path = 'output_files/halubench_hybrid_contriever_combined_results.csv'\n",
    "# df_out.to_csv(output_path, index=False)\n",
    "# print(f\"Saved: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preparing test set for generation all in one code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csmala/miniconda3/envs/halu_rag/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: output_files/halubench/halubench_combined_results_stratified_120.csv\n",
      "Final shape: (120, 6)\n",
      "Per-category counts:\n",
      " source_ds\n",
      "covidqa         20\n",
      "drop            20\n",
      "financebench    20\n",
      "halueval        20\n",
      "pubmedqa        20\n",
      "ragtruth        20\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2722407/165475214.py:53: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: g.sample(n=per_class, random_state=seed).index)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Rebuild the combined dataset cleanly with aligned indices\n",
    "df_halu_hybrid = pd.read_csv('input_files/halubench_hybrid_linear_with_rerank_minilm_alpha_0.3_beta_0.85.csv')\n",
    "df_halu_cont   = pd.read_csv('input_files/halubench_contriever_results.csv')\n",
    "df_parquet     = pd.read_parquet(\"hf://datasets/PatronusAI/HaluBench/data/test-00000-of-00001.parquet\")\n",
    "\n",
    "# Keep only needed columns from the two side frames\n",
    "df_halu_cont = df_halu_cont[['contriever_ret_docs']].reset_index(drop=True)\n",
    "df_source    = df_parquet[['source_ds']].reset_index(drop=True)\n",
    "\n",
    "# Ensure the main df has a clean index\n",
    "df_halu_hybrid = df_halu_hybrid.reset_index(drop=True)\n",
    "\n",
    "# Sanity check same length; if not, fail fast to prevent misalignment\n",
    "if not (len(df_halu_hybrid) == len(df_halu_cont) == len(df_source)):\n",
    "    raise ValueError(f\"Row count mismatch: hybrid={len(df_halu_hybrid)}, contriever={len(df_halu_cont)}, source={len(df_source)}. \"\n",
    "                     \"Align your inputs (same order/length) before concatenation.\")\n",
    "\n",
    "# Concatenate side-by-side with aligned indices\n",
    "combined = pd.concat([df_halu_hybrid, df_halu_cont, df_source], axis=1)\n",
    "\n",
    "# Keep only the final columns you want in output\n",
    "final_cols = ['question', 'answer', 'groundtruth_with_ids',\n",
    "              'hybrid_reranked_docs', 'contriever_ret_docs', 'source_ds']\n",
    "missing = [c for c in final_cols if c not in combined.columns]\n",
    "if missing:\n",
    "    raise KeyError(f\"Missing expected columns: {missing}\")\n",
    "\n",
    "combined = combined[final_cols].copy()\n",
    "\n",
    "# 2) Normalize source_ds to lowercase/trim\n",
    "combined['source_ds'] = combined['source_ds'].astype(str).str.strip().str.lower()\n",
    "\n",
    "# 3) Filter to the expected categories\n",
    "expected_cats = ['halueval', 'drop', 'pubmedqa', 'financebench', 'covidqa', 'ragtruth']\n",
    "filtered = combined[combined['source_ds'].isin(expected_cats)].copy()\n",
    "\n",
    "# 4) Stratified sampling by getting indices first, then slicing\n",
    "seed = 42\n",
    "per_class = 20\n",
    "\n",
    "# Verify counts\n",
    "counts = filtered['source_ds'].value_counts()\n",
    "too_small = [c for c in expected_cats if counts.get(c, 0) < per_class]\n",
    "if too_small:\n",
    "    raise ValueError(f\"Not enough rows for categories {too_small}. Required {per_class} each. Actual counts: {counts.to_dict()}\")\n",
    "\n",
    "# Collect sampled indices per category (no mutation of the frame)\n",
    "sampled_index = (\n",
    "    filtered\n",
    "    .groupby('source_ds', group_keys=False)\n",
    "    .apply(lambda g: g.sample(n=per_class, random_state=seed).index)\n",
    ")\n",
    "\n",
    "# Flatten the index collection (handles both Series/Index return shapes)\n",
    "if hasattr(sampled_index, 'explode'):\n",
    "    sampled_index = sampled_index.explode().astype(int).tolist()\n",
    "else:\n",
    "    # Fallback if explode isn't available\n",
    "    sampled_index = [int(i) for sub in sampled_index for i in list(sub)]\n",
    "\n",
    "# Slice the original filtered dataframe with .loc to preserve all columns exactly\n",
    "sampled = filtered.loc[sampled_index].copy()\n",
    "\n",
    "# Optional: sort within category or keep random order. Here we keep as-sampled order.\n",
    "# If you prefer a stable order, you can do: sampled = sampled.sort_values(['source_ds']).reset_index(drop=True)\n",
    "\n",
    "# 5) Save to CSV with all selected columns intact\n",
    "output_path = 'output_files/halubench/halubench_combined_results_stratified_120.csv'\n",
    "sampled.to_csv(output_path, index=False)\n",
    "print(f\"Saved: {output_path}\")\n",
    "\n",
    "# Quick sanity checks\n",
    "print(\"Final shape:\", sampled.shape)\n",
    "print(\"Per-category counts:\\n\", sampled['source_ds'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Top-5 full_text extractor ===\n",
      "Please provide a path.\n",
      "Please provide a path.\n",
      "Please provide a path.\n",
      "Please provide a path.\n",
      "Please provide a path.\n"
     ]
    }
   ],
   "source": [
    "# #!/usr/bin/env python3\n",
    "# import ast\n",
    "# import json\n",
    "# import os\n",
    "# import sys\n",
    "# import pandas as pd\n",
    "\n",
    "# def prompt_path(prompt_text, must_exist=False, default=None):\n",
    "#     while True:\n",
    "#         raw = input(f\"{prompt_text}{' [' + default + ']' if default else ''}: \").strip()\n",
    "#         path = raw or (default or \"\")\n",
    "#         if not path:\n",
    "#             print(\"Please provide a path.\")\n",
    "#             continue\n",
    "#         if must_exist and not os.path.exists(path):\n",
    "#             print(f\"Path not found: {path}\")\n",
    "#             continue\n",
    "#         return path\n",
    "\n",
    "# def maybe_parse_listlike_column(series):\n",
    "#     \"\"\"\n",
    "#     Best-effort convert stringified list-of-dicts to actual Python list objects.\n",
    "#     Leaves non-string values unchanged.\n",
    "#     \"\"\"\n",
    "#     def parse_cell(x):\n",
    "#         if isinstance(x, str):\n",
    "#             xs = x.strip()\n",
    "#             # Heuristics: only try to parse if it looks like a Python/JSON list\n",
    "#             if (xs.startswith(\"[\") and xs.endswith(\"]\")) or (xs.startswith(\"{\") and xs.endswith(\"}\")):\n",
    "#                 try:\n",
    "#                     # literal_eval handles Python-like repr (single quotes) safely\n",
    "#                     return ast.literal_eval(xs)\n",
    "#                 except Exception:\n",
    "#                     # Fallback: try json if it looks like JSON\n",
    "#                     try:\n",
    "#                         return json.loads(xs)\n",
    "#                     except Exception:\n",
    "#                         return x\n",
    "#         return x\n",
    "#     return series.apply(parse_cell)\n",
    "\n",
    "# def top5_full_text(items):\n",
    "#     \"\"\"\n",
    "#     Extract up to the first 5 'full_text' strings from a list of dicts/objects.\n",
    "#     Returns a list of strings.\n",
    "#     \"\"\"\n",
    "#     if not isinstance(items, (list, tuple)):\n",
    "#         return []\n",
    "#     out = []\n",
    "#     for d in items[:5]:\n",
    "#         if isinstance(d, dict):\n",
    "#             val = d.get(\"full_text\", None)\n",
    "#             if val is not None:\n",
    "#                 out.append(val)\n",
    "#         else:\n",
    "#             # Try attribute access if not a dict\n",
    "#             try:\n",
    "#                 val = getattr(d, \"full_text\", None)\n",
    "#                 if val is not None:\n",
    "#                     out.append(val)\n",
    "#             except Exception:\n",
    "#                 pass\n",
    "#     return out\n",
    "\n",
    "# def main():\n",
    "#     print(\"=== Top-5 full_text extractor ===\")\n",
    "#     in_csv = prompt_path(\"output_files/halubench_combined_results_stratified_120.csv\", must_exist=True)\n",
    "\n",
    "#     # Load CSV\n",
    "#     try:\n",
    "#         df = pd.read_csv(in_csv)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Failed to read CSV: {e}\")\n",
    "#         sys.exit(1)\n",
    "\n",
    "#     # Verify required columns\n",
    "#     required_cols = [\"hybrid_reranked_docs\", \"contriever_ret_docs\"]\n",
    "#     missing = [c for c in required_cols if c not in df.columns]\n",
    "#     if missing:\n",
    "#         print(f\"Missing expected columns: {missing}\")\n",
    "#         print(\"Columns present:\", list(df.columns))\n",
    "#         sys.exit(1)\n",
    "\n",
    "#     # Attempt to parse list-like cells if they are stringified\n",
    "#     for col in required_cols:\n",
    "#         df[col] = maybe_parse_listlike_column(df[col])\n",
    "\n",
    "#     # Create new columns\n",
    "#     df[\"hybrid_full_text\"] = df[\"hybrid_reranked_docs\"].apply(top5_full_text)\n",
    "#     df[\"contriever_full_text\"] = df[\"contriever_ret_docs\"].apply(top5_full_text)\n",
    "\n",
    "#     # Ask how to save list columns in CSV\n",
    "#     print(\"\\nHow should list columns be saved?\")\n",
    "#     print(\"1) As JSON strings (recommended; preserves structure)\")\n",
    "#     print(\"2) As Python repr strings\")\n",
    "#     choice = input(\"Enter 1 or 2 [1]: \").strip() or \"1\"\n",
    "\n",
    "#     df_to_save = df.copy()\n",
    "#     list_cols = [\"hybrid_full_text\", \"contriever_full_text\"]\n",
    "#     if choice == \"1\":\n",
    "#         for col in list_cols:\n",
    "#             df_to_save[col] = df_to_save[col].apply(json.dumps)\n",
    "#     else:\n",
    "#         # Leave as Python repr, which pandas will write as strings\n",
    "#         df_to_save[list_cols] = df_to_save[list_cols].astype(str)\n",
    "\n",
    "#     default_out = os.path.splitext(in_csv)[0] + \"_with_texts.csv\"\n",
    "#     out_csv = prompt_path(\"Enter path to output CSV file\", default=default_out)\n",
    "\n",
    "#     try:\n",
    "#         df_to_save.to_csv(out_csv, index=False)\n",
    "#         print(f\"\\nSaved modified DataFrame to: {out_csv}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Failed to write CSV: {e}\")\n",
    "#         sys.exit(1)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items in first row hybrid_full_text: 5\n",
      "Number of items in first row contriever_full_text: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['question', 'answer', 'groundtruth_with_ids', 'hybrid_reranked_docs',\n",
       "       'contriever_ret_docs', 'source_ds', 'hybrid_full_text',\n",
       "       'contriever_full_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "df = pd.read_csv('output_files/halubench/halubench_combined_results_stratified_120.csv')\n",
    "\n",
    "\n",
    "# Function to extract top 5 full_text from string representation of list\n",
    "def extract_top5_full_text(docs_string):\n",
    "    \"\"\"\n",
    "    Extract the 'full_text' field from the top 5 documents\n",
    "    Handles string representation of lists\n",
    "    \"\"\"\n",
    "    if pd.isna(docs_string) or docs_string == '':\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        # Convert string to actual list of dictionaries\n",
    "        docs_list = ast.literal_eval(docs_string)\n",
    "        \n",
    "        if not isinstance(docs_list, list):\n",
    "            return []\n",
    "        \n",
    "        # Take only the first 5 documents\n",
    "        top5_docs = docs_list[:5]\n",
    "        \n",
    "        # Extract full_text from each document\n",
    "        full_texts = [doc.get('full_text', '') for doc in top5_docs if isinstance(doc, dict)]\n",
    "        \n",
    "        return full_texts\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "# Apply the function to create new columns\n",
    "df['hybrid_full_text'] = df['hybrid_reranked_docs'].apply(extract_top5_full_text)\n",
    "df['contriever_full_text'] = df['contriever_ret_docs'].apply(extract_top5_full_text)\n",
    "\n",
    "# Verify the results\n",
    "print(f\"Number of items in first row hybrid_full_text: {len(df['hybrid_full_text'].iloc[0])}\")\n",
    "print(f\"Number of items in first row contriever_full_text: {len(df['contriever_full_text'].iloc[0])}\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('output_files/halubench/halubench_combined_results_stratified_120.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HotpotQA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['question', 'answer', 'mpnet_ret_docs', 'splade_ret_docs',\n",
       "       'hybrid_ret_docs', 'hybrid_reranked_docs', 'passages_with_ids',\n",
       "       'groundtruth_with_ids', 'FUSED_MAP', 'FUSED_NDCG', 'FUSED_MAP@3',\n",
       "       'FUSED_NDCG@3', 'FUSED_MAP@5', 'FUSED_NDCG@5', 'FUSED_MAP@10',\n",
       "       'FUSED_NDCG@10', 'RERANK_MAP', 'RERANK_NDCG', 'RERANK_MAP@3',\n",
       "       'RERANK_NDCG@3', 'RERANK_MAP@5', 'RERANK_NDCG@5', 'RERANK_MAP@10',\n",
       "       'RERANK_NDCG@10'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_hybrid = pd.read_csv('input_files/hotpotqa_hybrid_linear_with_rerank_minilm_alpha_0.3_beta_0.85.csv')  \n",
    "df_hybrid.columns    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['question', 'answer', 'passage', 'groundtruth_docs',\n",
       "       'contriever_ret_docs', 'MAP@3', 'MAP@5', 'MAP@10', 'NDCG@3', 'NDCG@5',\n",
       "       'NDCG@10'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cont = pd.read_csv('input_files/hotpotqa_contriever.csv')  \n",
    "df_cont.columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: output_files/hotpot/hotpotqa_sampled_120_with_full_texts.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import ast\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "INPUT_HYBRID = \"input_files/hotpotqa_hybrid_linear_with_rerank_minilm_alpha_0.3_beta_0.85.csv\"\n",
    "INPUT_CONT   = \"input_files/hotpotqa_contriever.csv\"\n",
    "OUTPUT_CSV   = \"output_files/hotpot/hotpotqa_sampled_120_with_full_texts.csv\"  # change as needed\n",
    "SAMPLE_SIZE  = 120\n",
    "RANDOM_SEED  = 42\n",
    "\n",
    "REQ_COLS_HYBRID = [\"question\", \"answer\", \"groundtruth_with_ids\", \"hybrid_reranked_docs\"]\n",
    "REQ_COLS_FINAL  = [\"question\", \"answer\", \"groundtruth_with_ids\",\n",
    "                   \"hybrid_reranked_docs\", \"contriever_ret_docs\",\n",
    "                   \"hybrid_full_text\", \"contriever_full_text\"]\n",
    "\n",
    "def maybe_parse_listlike(series):\n",
    "    \"\"\"\n",
    "    Convert stringified list-of-dicts to real Python lists if needed.\n",
    "    Leaves non-string values unchanged.\n",
    "    \"\"\"\n",
    "    def parse_cell(x):\n",
    "        if isinstance(x, str):\n",
    "            xs = x.strip()\n",
    "            if (xs.startswith(\"[\") and xs.endswith(\"]\")) or (xs.startswith(\"{\") and xs.endswith(\"}\")):\n",
    "                # Try literal_eval first (handles single quotes), then JSON fallback\n",
    "                try:\n",
    "                    return ast.literal_eval(xs)\n",
    "                except Exception:\n",
    "                    try:\n",
    "                        return json.loads(xs)\n",
    "                    except Exception:\n",
    "                        return x\n",
    "        return x\n",
    "    return series.apply(parse_cell)\n",
    "\n",
    "def top5_full_text(items):\n",
    "    \"\"\"\n",
    "    Extract up to 5 'full_text' strings from a list of dicts/objects.\n",
    "    \"\"\"\n",
    "    if not isinstance(items, (list, tuple)):\n",
    "        return []\n",
    "    out = []\n",
    "    for d in items[:5]:\n",
    "        if isinstance(d, dict):\n",
    "            val = d.get(\"full_text\", None)\n",
    "            if val is not None:\n",
    "                out.append(val)\n",
    "        else:\n",
    "            # Try attribute access if objects\n",
    "            try:\n",
    "                val = getattr(d, \"full_text\", None)\n",
    "                if val is not None:\n",
    "                    out.append(val)\n",
    "            except Exception:\n",
    "                pass\n",
    "    return out\n",
    "\n",
    "def main():\n",
    "    # Load dataframes\n",
    "    try:\n",
    "        df_hybrid = pd.read_csv(INPUT_HYBRID)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read hybrid CSV: {INPUT_HYBRID}\\nError: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    try:\n",
    "        df_cont = pd.read_csv(INPUT_CONT)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read contriever CSV: {INPUT_CONT}\\nError: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Validate required columns in df_hybrid\n",
    "    missing_h = [c for c in REQ_COLS_HYBRID if c not in df_hybrid.columns]\n",
    "    if missing_h:\n",
    "        print(f\"df_hybrid missing required columns: {missing_h}\")\n",
    "        print(\"df_hybrid columns:\", list(df_hybrid.columns))\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Keep only 'contriever_ret_docs' in df_cont\n",
    "    if \"contriever_ret_docs\" not in df_cont.columns:\n",
    "        print(\"df_cont missing 'contriever_ret_docs' column.\")\n",
    "        print(\"df_cont columns:\", list(df_cont.columns))\n",
    "        sys.exit(1)\n",
    "    df_cont = df_cont[[\"contriever_ret_docs\"]]\n",
    "\n",
    "    # Align lengths (assumes same row order alignment by index)\n",
    "    if len(df_cont) != len(df_hybrid):\n",
    "        print(f\"Warning: Row count mismatch. df_hybrid={len(df_hybrid)}, df_cont={len(df_cont)}\")\n",
    "        # If you need to align by an id, merge on that key instead of concatenating by index.\n",
    "        # For now, we will align by index and truncate to the min length.\n",
    "        min_len = min(len(df_hybrid), len(df_cont))\n",
    "        df_hybrid = df_hybrid.iloc[:min_len].reset_index(drop=True)\n",
    "        df_cont   = df_cont.iloc[:min_len].reset_index(drop=True)\n",
    "\n",
    "    # Concatenate contriever_ret_docs into df_hybrid\n",
    "    df = pd.concat([df_hybrid.reset_index(drop=True), df_cont.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    # Keep only the specified columns before sampling\n",
    "    keep_cols = [\"question\", \"answer\", \"groundtruth_with_ids\", \"hybrid_reranked_docs\", \"contriever_ret_docs\"]\n",
    "    missing_keep = [c for c in keep_cols if c not in df.columns]\n",
    "    if missing_keep:\n",
    "        print(f\"Missing columns before sampling: {missing_keep}\")\n",
    "        print(\"Available columns:\", list(df.columns))\n",
    "        sys.exit(1)\n",
    "    df = df[keep_cols]\n",
    "\n",
    "    # Randomly sample 1200 rows with seed 1200\n",
    "    if len(df) < SAMPLE_SIZE:\n",
    "        print(f\"Warning: Requested sample size {SAMPLE_SIZE} > available rows {len(df)}. Taking all rows.\")\n",
    "        df_sampled = df.copy()\n",
    "    else:\n",
    "        df_sampled = df.sample(n=SAMPLE_SIZE, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "\n",
    "    # Parse list-like columns if they are stringified\n",
    "    for col in [\"hybrid_reranked_docs\", \"contriever_ret_docs\"]:\n",
    "        df_sampled[col] = maybe_parse_listlike(df_sampled[col])\n",
    "\n",
    "    # Create the two new columns from top-5 full_text\n",
    "    df_sampled[\"hybrid_full_text\"] = df_sampled[\"hybrid_reranked_docs\"].apply(top5_full_text)\n",
    "    df_sampled[\"contriever_full_text\"] = df_sampled[\"contriever_ret_docs\"].apply(top5_full_text)\n",
    "\n",
    "    # Ensure final column order\n",
    "    df_out = df_sampled[REQ_COLS_FINAL].copy()\n",
    "\n",
    "    # Save: store list columns as JSON strings to preserve structure in CSV\n",
    "    df_save = df_out.copy()\n",
    "    for col in [\"hybrid_reranked_docs\", \"contriever_ret_docs\", \"hybrid_full_text\", \"contriever_full_text\"]:\n",
    "        df_save[col] = df_save[col].apply(lambda x: json.dumps(x) if isinstance(x, (list, dict)) else x)\n",
    "\n",
    "    os.makedirs(os.path.dirname(OUTPUT_CSV), exist_ok=True)\n",
    "    try:\n",
    "        df_save.to_csv(OUTPUT_CSV, index=False)\n",
    "        print(f\"Saved: {OUTPUT_CSV}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to write CSV: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final generation output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['question', 'answer', 'groundtruth_with_ids', 'hybrid_reranked_docs',\n",
       "       'contriever_ret_docs', 'hybrid_full_text', 'contriever_full_text',\n",
       "       'answer_gemma2b_hybrid'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('output_files/hotpot/hotpotqa_answers_hybrid_qwen3_4b_1200rows.csv')\n",
    "# df = pd.read_csv('output_files/hotpot/hotpot_1200_answers_hybrid_llama3.1_8b.csv')\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200, 8)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove duplicate rows in the dtaaset\n",
    "df = df.drop_duplicates()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['question', 'answer', 'answer_gemma2b_hybrid'], dtype='object')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new = df.drop(columns=[ 'groundtruth_with_ids', 'hybrid_reranked_docs',\n",
    "       'contriever_ret_docs', 'hybrid_full_text', 'contriever_full_text']) \n",
    "df_new.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['question', 'answer', 'answer_qwen4b_hybrid'], dtype='object')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rename this column name to answer_gemma2b_contriever\n",
    "df_new = df_new.rename(columns={\"answer_gemma2b_hybrid\": \"answer_qwen4b_hybrid\"})\n",
    "df_new.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.to_csv(\"hotpotqa_answers_hybrid_qwen3_4b_1200rows_final_cut.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GT\n",
    "[{'doc_id': '64_0', 'full_text': 'iqaluit airport (iata: yfb, icao: cyfb) serves iqaluit, nunavut, canada and is located adjacent to the town.'}, {'doc_id': '64_5', 'full_text': 'canadian north inc. is an airline headquartered in calgary, alberta, canada.'}]\n",
    "\n",
    "MPNet\n",
    "[{'doc_id': '39763_1', 'score': 0.744383, 'full_text': 'it is located in iqaluit, nunavut.'}, \n",
    "{'doc_id': '64_0', 'score': 0.743217, 'full_text': 'iqaluit airport (iata: yfb, icao: cyfb) serves iqaluit, nunavut, canada and is located adjacent to the town.'}, \n",
    "{'doc_id': '64_8', 'score': 0.680723, 'full_text': 'its main base is edmonton airport.']\n",
    "\n",
    "Splade \n",
    "[{'doc_id': '64_0', 'score': 22.4232, 'full_text': 'iqaluit airport (iata: yfb, icao: cyfb) serves iqaluit, nunavut, canada and is located adjacent to the town.' }, \n",
    "{'doc_id': '39763_2', 'score': 19.4198, 'full_text': 'iqaluit (inuktitut ), meaning place of fish, is the capital of the canadian territory of nunavut; its largest community, and its only city.'}, \n",
    "{'doc_id': '64_5', 'score': 17.4174, 'full_text': 'canadian north inc. is an airline headquartered in calgary, alberta, canada.'}\n",
    "\n",
    "Hybrid\n",
    "[{'doc_id': '64_0', 'score': 0.9969582344194297, 'rank': 1, 'full_text': 'iqaluit airport (iata: yfb, icao: cyfb) serves iqaluit, nunavut, canada and is located adjacent to the town.'}, \n",
    "{'doc_id': '64_5', 'score': 0.5659488834121346, 'rank': 2, 'full_text': 'canadian north inc. is an airline headquartered in calgary, alberta, canada.'}\n",
    "{'doc_id': '39763_2', 'score': 0.3417903555643511, 'rank': 3, 'full_text': 'iqaluit (inuktitut), meaning place of fish, is the capital of the canadian territory of nunavut; its largest community, and its only city.'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "halu_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
