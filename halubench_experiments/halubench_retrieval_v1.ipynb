{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Retrieval on Halubench (BM25 and SPLADE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csmala/miniconda3/envs/halu_rag/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Oct 16, 2025 5:57:33 PM org.apache.lucene.store.MemorySegmentIndexInputProvider <init>\n",
      "INFO: Using MemorySegmentIndexInput with Java 21; to disable start with -Dorg.apache.lucene.store.MMapDirectory.enableMemorySegments=false\n",
      "BM25 search: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14900/14900 [00:12<00:00, 1226.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bm25 results saved to ./outputs/bm25_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "SPLADE encode: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1553/1553 [00:38<00:00, 40.21it/s]\n",
      "SPLADE search: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14900/14900 [03:20<00:00, 74.45it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splade results saved to ./outputs/splade_results.csv\n"
     ]
    }
   ],
   "source": [
    "import os, json, math, subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from scipy.sparse import csr_matrix, save_npz, load_npz\n",
    "\n",
    "# -----------------------\n",
    "# Small helpers\n",
    "# -----------------------\n",
    "\n",
    "def ensure_dir(p):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def dedupe_passages(df, text_col):\n",
    "    \"\"\"Return deduped dataframe of passages + mapping from original row -> dedup id (str).\"\"\"\n",
    "    x = df.copy()\n",
    "    x[text_col] = x[text_col].fillna(\"\").astype(str)\n",
    "    dedup = x[[text_col]].drop_duplicates().reset_index(drop=True)\n",
    "    text_to_id = {dedup.loc[i, text_col]: str(i) for i in range(len(dedup))}\n",
    "    rel_map = {}\n",
    "    for i in range(len(df)):\n",
    "        t = \"\" if pd.isna(df.loc[i, text_col]) else str(df.loc[i, text_col])\n",
    "        rel_map[str(i)] = text_to_id.get(t, text_to_id.get(\"\", \"0\"))\n",
    "    return dedup, rel_map\n",
    "\n",
    "def ap_at_k(ret_ids, rel_id, k):\n",
    "    for rank, did in enumerate(ret_ids[:k], 1):\n",
    "        if did == rel_id:\n",
    "            return 1.0 / rank\n",
    "    return 0.0\n",
    "\n",
    "def ndcg_at_k(ret_ids, rel_id, k):\n",
    "    for rank, did in enumerate(ret_ids[:k], 1):\n",
    "        if did == rel_id:\n",
    "            return 1.0 / math.log2(rank + 1)\n",
    "    return 0.0\n",
    "\n",
    "# -----------------------\n",
    "# BM25 (Pyserini/Lucene)\n",
    "# -----------------------\n",
    "\n",
    "class BM25Retriever:\n",
    "    def __init__(self, index_dir=\"./indices/bm25\", threads=8, java_mem=\"8g\"):\n",
    "        self.index_dir = index_dir\n",
    "        self.threads = threads\n",
    "        self.java_mem = java_mem\n",
    "\n",
    "    def build_index(self, dedup_df, text_col, work_dir=\"./work/bm25\"):\n",
    "        ensure_dir(self.index_dir); ensure_dir(work_dir)\n",
    "        corpus_dir = os.path.join(work_dir, \"json_corpus\"); ensure_dir(corpus_dir)\n",
    "        docs_path = os.path.join(corpus_dir, \"docs.jsonl\")\n",
    "        with open(docs_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for i in range(len(dedup_df)):\n",
    "                text = \"\" if pd.isna(dedup_df.loc[i, text_col]) else str(dedup_df.loc[i, text_col])\n",
    "                f.write(json.dumps({\"id\": str(i), \"contents\": text}, ensure_ascii=False) + \"\\n\")\n",
    "        cmd = [\n",
    "            \"python\",\"-m\",\"pyserini.index.lucene\",\n",
    "            \"--collection\",\"JsonCollection\",\n",
    "            \"--input\", corpus_dir,\n",
    "            \"--index\", self.index_dir,\n",
    "            \"--generator\",\"DefaultLuceneDocumentGenerator\",\n",
    "            \"--threads\", str(self.threads),\n",
    "            \"--storePositions\",\"--storeDocvectors\",\"--storeRaw\"\n",
    "        ]\n",
    "        env = os.environ.copy()\n",
    "        env[\"JAVA_TOOL_OPTIONS\"] = f\"-Xms{self.java_mem} -Xmx{self.java_mem}\"\n",
    "        res = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, env=env)\n",
    "        if res.returncode != 0:\n",
    "            print(res.stdout); print(res.stderr)\n",
    "            raise RuntimeError(\"BM25 indexing failed (check Java 11+ and pyserini).\")\n",
    "\n",
    "    def retrieve(self, queries, topk=10):\n",
    "        from pyserini.search.lucene import LuceneSearcher\n",
    "        searcher = LuceneSearcher(self.index_dir)\n",
    "        results = {}\n",
    "        for qid, q in tqdm(queries.items(), desc=\"BM25 search\"):\n",
    "            hits = searcher.search(q, k=topk)\n",
    "            results[qid] = [(h.docid, float(h.score)) for h in hits]\n",
    "        return results\n",
    "\n",
    "# -----------------------\n",
    "# SPLADE (Transformers + CSR)\n",
    "# -----------------------\n",
    "\n",
    "class SPLADERetriever:\n",
    "    def __init__(self, index_dir=\"./indices/splade\", model_name=\"naver/splade-cocondenser-ensembledistil\",\n",
    "                 batch_size=8, max_length=256, min_weight=0.01):\n",
    "        self.index_dir = index_dir\n",
    "        self.model_name = model_name\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "        self.min_weight = min_weight\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.tok = None\n",
    "        self.model = None\n",
    "\n",
    "    def _load(self):\n",
    "        if self.tok is None:\n",
    "            self.tok = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        if self.model is None:\n",
    "            self.model = AutoModelForMaskedLM.from_pretrained(self.model_name).to(self.device).eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _encode_texts(self, texts):\n",
    "        self._load()\n",
    "        V = self.model.config.vocab_size\n",
    "        data, indices, indptr = [], [], [0]\n",
    "        for i in tqdm(range(0, len(texts), self.batch_size), desc=\"SPLADE encode\"):\n",
    "            batch = texts[i:i+self.batch_size]\n",
    "            toks = self.tok(batch, return_tensors=\"pt\", padding=True, truncation=True,\n",
    "                            max_length=self.max_length).to(self.device)\n",
    "            logits = self.model(**toks).logits  # [B,L,V]\n",
    "            activ = torch.log1p(torch.relu(logits))\n",
    "            weights = activ.max(dim=1).values.cpu().numpy()  # [B,V]\n",
    "            for row in weights:\n",
    "                nz = np.where(row >= self.min_weight)[0]\n",
    "                indices.extend(nz.tolist())\n",
    "                data.extend(row[nz].astype(np.float32).tolist())\n",
    "                indptr.append(len(indices))\n",
    "        return csr_matrix((np.array(data, np.float32),\n",
    "                           np.array(indices, np.int32),\n",
    "                           np.array(indptr, np.int32)),\n",
    "                          shape=(len(texts), V), dtype=np.float32)\n",
    "\n",
    "    def build_index(self, dedup_df, text_col):\n",
    "        ensure_dir(self.index_dir)\n",
    "        texts = [\"\" if pd.isna(dedup_df.loc[i, text_col]) else str(dedup_df.loc[i, text_col])\n",
    "                 for i in range(len(dedup_df))]\n",
    "        mat = self._encode_texts(texts)\n",
    "        save_npz(os.path.join(self.index_dir, \"docs.npz\"), mat)\n",
    "        with open(os.path.join(self.index_dir, \"doc_ids.json\"), \"w\") as f:\n",
    "            json.dump([str(i) for i in range(len(texts))], f)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _encode_query(self, q):\n",
    "        self._load()\n",
    "        toks = self.tok([q], return_tensors=\"pt\", padding=True, truncation=True,\n",
    "                        max_length=self.max_length).to(self.device)\n",
    "        logits = self.model(**toks).logits\n",
    "        activ = torch.log1p(torch.relu(logits))\n",
    "        w = activ.max(dim=1).values.squeeze(0).cpu().numpy()\n",
    "        nz = np.where(w >= self.min_weight)[0]\n",
    "        return csr_matrix((w[nz].astype(np.float32), nz.astype(np.int32), np.array([0,len(nz)], np.int32)),\n",
    "                          shape=(1, self.model.config.vocab_size), dtype=np.float32)\n",
    "\n",
    "    def retrieve(self, queries, topk=10):\n",
    "        docs = load_npz(os.path.join(self.index_dir, \"docs.npz\"))\n",
    "        with open(os.path.join(self.index_dir, \"doc_ids.json\"), \"r\") as f:\n",
    "            doc_ids = json.load(f)\n",
    "        out = {}\n",
    "        for qid, q in tqdm(queries.items(), desc=\"SPLADE search\"):\n",
    "            qv = self._encode_query(q)  # [1,V]\n",
    "            scores = (docs @ qv.T).toarray().ravel()\n",
    "            if topk >= len(doc_ids):\n",
    "                idx = np.argsort(-scores)\n",
    "            else:\n",
    "                idx = np.argpartition(scores, -topk)[-topk:]\n",
    "                idx = idx[np.argsort(-scores[idx])]\n",
    "            out[qid] = [(doc_ids[i], float(scores[i])) for i in idx[:topk]]\n",
    "        return out\n",
    "\n",
    "# -----------------------\n",
    "# Pipeline\n",
    "# -----------------------\n",
    "\n",
    "def run_pipeline(df,\n",
    "                 output_dir=\"./outputs\",\n",
    "                 bm25_index_dir=\"./indices/bm25\",\n",
    "                 splade_index_dir=\"./indices/splade\",\n",
    "                 work_dir=\"./work\"):\n",
    "    # Basic column handling\n",
    "    if \"question\" not in df.columns:\n",
    "        raise ValueError(\"Missing 'question' column.\")\n",
    "    text_col = \"passage\" if \"passage\" in df.columns else (\"context\" if \"context\" in df.columns else None)\n",
    "    if text_col is None:\n",
    "        raise ValueError(\"Missing 'passage' or 'context' column.\")\n",
    "    if \"answer\" not in df.columns:\n",
    "        df[\"answer\"] = \"\"\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # Deduplicate passages\n",
    "    dedup_df, rel_map = dedupe_passages(df, text_col)\n",
    "    corpus_lookup = {str(i): (\"\" if pd.isna(dedup_df.loc[i, text_col]) else str(dedup_df.loc[i, text_col]))\n",
    "                     for i in range(len(dedup_df))}\n",
    "    queries = {str(i): str(df.loc[i, \"question\"]) for i in range(len(df))}\n",
    "\n",
    "    ensure_dir(output_dir); ensure_dir(os.path.dirname(bm25_index_dir)); ensure_dir(os.path.dirname(splade_index_dir)); ensure_dir(work_dir)\n",
    "\n",
    "    # BM25\n",
    "    bm25 = BM25Retriever(index_dir=bm25_index_dir)\n",
    "    bm25.build_index(dedup_df, text_col, work_dir=os.path.join(work_dir, \"bm25\"))\n",
    "    bm25_res = bm25.retrieve(queries, topk=10)\n",
    "    save_results(\"bm25\", df, text_col, bm25_res, rel_map, corpus_lookup, os.path.join(output_dir, \"bm25_results.csv\"))\n",
    "\n",
    "    # SPLADE\n",
    "    splade = SPLADERetriever(index_dir=splade_index_dir)\n",
    "    splade.build_index(dedup_df, text_col)\n",
    "    splade_res = splade.retrieve(queries, topk=10)\n",
    "    save_results(\"splade\", df, text_col, splade_res, rel_map, corpus_lookup, os.path.join(output_dir, \"splade_results.csv\"))\n",
    "\n",
    "# def save_results(name, df, text_col, ret, rel_map, corpus_lookup, out_csv):\n",
    "#     rows = []\n",
    "#     ks = [3,5,10]\n",
    "#     for i in range(len(df)):\n",
    "#         qid = str(i)\n",
    "#         retrieved = ret.get(qid, [])\n",
    "#         ret_ids = [d for d,_ in retrieved]\n",
    "#         rel = rel_map[qid]\n",
    "#         metrics = {}\n",
    "#         for k in ks:\n",
    "#             metrics[f\"MAP@{k}\"] = ap_at_k(ret_ids, rel, k)\n",
    "#             metrics[f\"NDCG@{k}\"] = ndcg_at_k(ret_ids, rel, k)\n",
    "#         pack = [{\"doc_id\": d, \"score\": round(float(s),4), \"snippet\": corpus_lookup.get(d,\"\")[:200].replace(\"\\n\",\" \")}\n",
    "#                 for d,s in retrieved]\n",
    "#         rows.append({\n",
    "#             \"question\": str(df.loc[i, \"question\"]),\n",
    "#             \"answer\": \"\" if \"answer\" not in df.columns or pd.isna(df.loc[i, \"answer\"]) else str(df.loc[i, \"answer\"]),\n",
    "#             text_col: \"\" if pd.isna(df.loc[i, text_col]) else str(df.loc[i, text_col]),\n",
    "#             f\"{name}_ret_docs\": json.dumps(pack, ensure_ascii=False),\n",
    "#             **metrics\n",
    "#         })\n",
    "#     pd.DataFrame(rows).to_csv(out_csv, index=False)\n",
    "#     print(f\"{name} results saved to {out_csv}\")\n",
    "\n",
    "def save_results(name, df, text_col, ret, rel_map, corpus_lookup, out_csv):\n",
    "    rows = []\n",
    "    ks = [3, 5, 10]\n",
    "    for i in range(len(df)):\n",
    "        qid = str(i)\n",
    "        retrieved = ret.get(qid, [])\n",
    "        ret_ids = [d for d, _ in retrieved]\n",
    "        rel = rel_map[qid]\n",
    "\n",
    "        metrics = {}\n",
    "        for k in ks:\n",
    "            metrics[f\"MAP@{k}\"] = ap_at_k(ret_ids, rel, k)\n",
    "            metrics[f\"NDCG@{k}\"] = ndcg_at_k(ret_ids, rel, k)\n",
    "\n",
    "        # Store complete doc text + short preview for readability\n",
    "        packed = []\n",
    "        for d, s in retrieved:\n",
    "            full = corpus_lookup.get(d, \"\")\n",
    "            preview = full[:200].replace(\"\\n\", \" \")\n",
    "            packed.append({\n",
    "                \"doc_id\": d,\n",
    "                \"score\": round(float(s), 4),\n",
    "                # keep snippet for backward compatibility (optional)\n",
    "                \"snippet\": preview,\n",
    "                # new field with the full text (no truncation)\n",
    "                \"full_text\": full,\n",
    "                # explicit preview field (optional)\n",
    "                \"preview_snippet\": preview\n",
    "            })\n",
    "\n",
    "        rows.append({\n",
    "            \"question\": \"\" if pd.isna(df.loc[i, \"question\"]) else str(df.loc[i, \"question\"]),\n",
    "            \"answer\": \"\" if \"answer\" not in df.columns or pd.isna(df.loc[i, \"answer\"]) else str(df.loc[i, \"answer\"]),\n",
    "            text_col: \"\" if pd.isna(df.loc[i, text_col]) else str(df.loc[i, text_col]),\n",
    "            f\"{name}_ret_docs\": json.dumps(packed, ensure_ascii=False),\n",
    "            **metrics\n",
    "        })\n",
    "\n",
    "    pd.DataFrame(rows).to_csv(out_csv, index=False)\n",
    "    print(f\"{name} results saved to {out_csv}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    df = pd.read_parquet(\"hf://datasets/PatronusAI/HaluBench/data/test-00000-of-00001.parquet\")\n",
    "    run_pipeline(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 Results:\n",
      "  MAP@3: 0.8234, NDCG@3: 0.8319\n",
      "  MAP@5: 0.8283, NDCG@5: 0.8407\n",
      "  MAP@10: 0.8319, NDCG@10: 0.8495\n",
      "\n",
      "SPLADE Results:\n",
      "  MAP@3: 0.8305, NDCG@3: 0.8379\n",
      "  MAP@5: 0.8346, NDCG@5: 0.8452\n",
      "  MAP@10: 0.8381, NDCG@10: 0.8537\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, path in [(\"BM25\",\"./outputs/bm25_results.csv\"), (\"SPLADE\",\"./outputs/splade_results.csv\")]:\n",
    "    df = pd.read_csv(path)\n",
    "    print(name, \"Results:\")\n",
    "    for k in (3,5,10):\n",
    "        print(f\"  MAP@{k}: {pd.to_numeric(df[f'MAP@{k}'], errors='coerce').mean():.4f}, \"\n",
    "              f\"NDCG@{k}: {pd.to_numeric(df[f'NDCG@{k}'], errors='coerce').mean():.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Only BM25- Halubench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csmala/miniconda3/envs/halu_rag/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Oct 13, 2025 8:52:00 AM org.apache.lucene.store.MemorySegmentIndexInputProvider <init>\n",
      "INFO: Using MemorySegmentIndexInput with Java 21; to disable start with -Dorg.apache.lucene.store.MMapDirectory.enableMemorySegments=false\n",
      "BM25 search: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14900/14900 [00:10<00:00, 1417.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Results saved to ./bm25_results.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import subprocess\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pyserini.search.lucene import LuceneSearcher   # <-- updated import\n",
    "\n",
    "\n",
    "def ensure_dir(path: str):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def dedupe_passages(df: pd.DataFrame, text_col: str) -> Tuple[pd.DataFrame, Dict[str, str]]:\n",
    "    dedup = df[[text_col]].fillna(\"\").drop_duplicates().reset_index(drop=True)\n",
    "    text_to_docid = {dedup.loc[i, text_col]: str(i) for i in range(len(dedup))}\n",
    "    relevant = {}\n",
    "    for i, row in df.iterrows():\n",
    "        passage = \"\" if pd.isna(row[text_col]) else str(row[text_col])\n",
    "        relevant[str(i)] = text_to_docid[passage]\n",
    "    return dedup, relevant\n",
    "\n",
    "def ap_at_k(retrieved_ids: List[str], relevant_id: str, k: int) -> float:\n",
    "    for rank, doc_id in enumerate(retrieved_ids[:k], start=1):\n",
    "        if doc_id == relevant_id:\n",
    "            return 1.0 / rank\n",
    "    return 0.0\n",
    "\n",
    "def ndcg_at_k(retrieved_ids: List[str], relevant_id: str, k: int) -> float:\n",
    "    for rank, doc_id in enumerate(retrieved_ids[:k], start=1):\n",
    "        if doc_id == relevant_id:\n",
    "            return 1.0 / math.log2(rank + 1)\n",
    "    return 0.0\n",
    "\n",
    "def run_bm25_pipeline(\n",
    "    df: pd.DataFrame,\n",
    "    text_col: str = \"passage\",\n",
    "    question_col: str = \"question\",\n",
    "    answer_col: str = \"answer\",\n",
    "    index_dir: str = \"./bm25_index\",\n",
    "    output_csv: str = \"./bm25_results.csv\",\n",
    "    tmp_dir: str = \"./bm25_tmp\",\n",
    "    topk: int = 10,\n",
    "):\n",
    "    if question_col not in df.columns:\n",
    "        raise ValueError(f\"Missing '{question_col}' column\")\n",
    "    if text_col not in df.columns:\n",
    "        raise ValueError(f\"Missing '{text_col}' column\")\n",
    "    if answer_col not in df.columns:\n",
    "        df[answer_col] = \"\"\n",
    "\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    corpus_df, relevant = dedupe_passages(df, text_col)\n",
    "    doc_lookup = {str(i): corpus_df.loc[i, text_col] for i in range(len(corpus_df))}\n",
    "\n",
    "    ensure_dir(tmp_dir)\n",
    "    corpus_path = os.path.join(tmp_dir, \"docs.jsonl\")\n",
    "    with open(corpus_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, row in corpus_df.iterrows():\n",
    "            f.write(json.dumps({\"id\": str(i), \"contents\": row[text_col]}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    ensure_dir(index_dir)\n",
    "    cmd = [\n",
    "        \"python\", \"-m\", \"pyserini.index.lucene\",\n",
    "        \"--collection\", \"JsonCollection\",\n",
    "        \"--input\", tmp_dir,\n",
    "        \"--index\", index_dir,\n",
    "        \"--generator\", \"DefaultLuceneDocumentGenerator\",\n",
    "        \"--threads\", \"4\",\n",
    "        \"--storePositions\", \"--storeDocvectors\", \"--storeRaw\"\n",
    "    ]\n",
    "    res = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    if res.returncode != 0:\n",
    "        print(res.stdout)\n",
    "        print(res.stderr)\n",
    "        raise RuntimeError(\"Indexing failed (check Java + Pyserini installation).\")\n",
    "\n",
    "    searcher = LuceneSearcher(index_dir)   # <-- updated class\n",
    "    searcher.set_bm25()\n",
    "\n",
    "    queries = {str(i): str(df.loc[i, question_col]) for i in range(len(df))}\n",
    "    all_hits = {}\n",
    "    for qid, query in tqdm(queries.items(), desc=\"BM25 search\"):\n",
    "        hits = searcher.search(query, k=topk)\n",
    "        all_hits[qid] = [(hit.docid, float(hit.score)) for hit in hits]\n",
    "\n",
    "    rows = []\n",
    "    for i, row in df.iterrows():\n",
    "        qid = str(i)\n",
    "        gold_doc = relevant[qid]\n",
    "        ret = all_hits.get(qid, [])\n",
    "        ret_ids = [doc_id for doc_id, _ in ret]\n",
    "\n",
    "        metrics = {}\n",
    "        for k in (3, 5, 10):\n",
    "            metrics[f\"MAP@{k}\"] = ap_at_k(ret_ids, gold_doc, k)\n",
    "            metrics[f\"NDCG@{k}\"] = ndcg_at_k(ret_ids, gold_doc, k)\n",
    "\n",
    "        rows.append({\n",
    "            question_col: str(row[question_col]),\n",
    "            answer_col: \"\" if pd.isna(row[answer_col]) else str(row[answer_col]),\n",
    "            text_col: \"\" if pd.isna(row[text_col]) else str(row[text_col]),\n",
    "            \"bm25_retrieved\": json.dumps(\n",
    "                [\n",
    "                    {\n",
    "                        \"doc_id\": doc_id,\n",
    "                        \"score\": round(score, 4),\n",
    "                        \"passage\": doc_lookup.get(doc_id, \"\")[:200].replace(\"\\n\", \" \")\n",
    "                    }\n",
    "                    for doc_id, score in ret\n",
    "                ],\n",
    "                ensure_ascii=False\n",
    "            ),\n",
    "            **metrics\n",
    "        })\n",
    "\n",
    "    pd.DataFrame(rows).to_csv(output_csv, index=False)\n",
    "    print(f\"Done. Results saved to {output_csv}\")\n",
    "\n",
    "def print_bm25_overall(csv_path: str, ks=(3, 5, 10)):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    print(\"BM25 Results:\")\n",
    "    for k in ks:\n",
    "        map_mean = pd.to_numeric(df[f\"MAP@{k}\"], errors=\"coerce\").mean()\n",
    "        ndcg_mean = pd.to_numeric(df[f\"NDCG@{k}\"], errors=\"coerce\").mean()\n",
    "        print(f\"  MAP@{k}: {map_mean:.4f}, NDCG@{k}: {ndcg_mean:.4f}\")\n",
    "    print()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example:\n",
    "\n",
    "    from datasets import load_dataset\n",
    "    ds = load_dataset(\"PatronusAI/HaluBench\")\n",
    "    df = ds[\"test\"].to_pandas()\n",
    "    run_bm25_pipeline(df, text_col=\"passage\")\n",
    "    print_bm25_overall(\"./bm25_results.csv\")\n",
    "  \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 Results:\n",
      "  MAP@3: 0.8234, NDCG@3: 0.8319\n",
      "  MAP@5: 0.8283, NDCG@5: 0.8407\n",
      "  MAP@10: 0.8319, NDCG@10: 0.8495\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_bm25_overall(csv_path: str, ks=(3, 5, 10)):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    print(\"BM25 Results:\")\n",
    "    for k in ks:\n",
    "        map_mean = pd.to_numeric(df[f\"MAP@{k}\"], errors=\"coerce\").mean()\n",
    "        ndcg_mean = pd.to_numeric(df[f\"NDCG@{k}\"], errors=\"coerce\").mean()\n",
    "        print(f\"  MAP@{k}: {map_mean:.4f}, NDCG@{k}: {ndcg_mean:.4f}\")\n",
    "    print()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print_bm25_overall(\"./bm25_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Retrieval on HaluBench (ST and Contriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Treat each passage as a single document (no split). Deduplicate passages globally before indexing.\n",
    "Use two dense models:\n",
    "all-mpnet-base-v2 (sentence-level encoder)\n",
    "facebook/contriever (unsupervised dense retriever)\n",
    "Use FAISS to build vector indices.\n",
    "Evaluate by comparing retrieved docs to the original passage of each question (after mapping to dedup doc IDs).\n",
    "Save results to CSV (one per model) with the same pattern and report MAP/NDCG@3/5/10.\n",
    "Design choices and reasoning:\n",
    "\n",
    "Embedding models:\n",
    "MPNet (sentence-transformers/all-mpnet-base-v2) is designed for sentence-level semantic similarity. We will load via AutoModel and use standard mean pooling over the last hidden states with attention mask, then L2-normalize.\n",
    "Contriever (facebook/contriever) is explicitly built for dense retrieval. Weâ€™ll also use mean pooling + L2 normalization for compatibility and cosine-like search.\n",
    "We are not using AutoModelForMaskedLM here because MLM heads are for token prediction, not sentence embeddings.\n",
    "Text normalization:\n",
    "We normalize passages and queries consistently: strip, collapse whitespace, lowercase. This helps dedup and matching.\n",
    "Relevance is determined by exact match to the dedupbed passage string, which avoids fuzzy alignment errors.\n",
    "FAISS index:\n",
    "Use IndexFlatIP with L2-normalized embeddings, equivalent to cosine similarity search. Itâ€™s simplest and accurate for these encoders.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Building FAISS index for mpnet ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 195/195 [00:36<00:00,  5.39it/s]\n",
      "Encoding: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 233/233 [00:14<00:00, 15.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpnet: results saved to ./outputs_dense/mpnet_results.csv\n",
      "\n",
      "=== Building FAISS index for contriever ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 195/195 [00:32<00:00,  6.08it/s]\n",
      "Encoding: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 233/233 [00:14<00:00, 16.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contriever: results saved to ./outputs_dense/contriever_results.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import faiss\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Config\n",
    "# -----------------------\n",
    "\n",
    "MPNET_NAME = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "CONTRIEVER_NAME = \"facebook/contriever\"\n",
    "TOPK = 10\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Utils\n",
    "# -----------------------\n",
    "\n",
    "def ensure_dir(p):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    if s is None or (isinstance(s, float) and np.isnan(s)):\n",
    "        return \"\"\n",
    "    s = str(s)\n",
    "    s = s.replace(\"\\u00a0\", \" \")\n",
    "    s = \" \".join(s.split())  # collapse whitespace\n",
    "    return s.strip().lower()\n",
    "\n",
    "def build_passage_dedup(df: pd.DataFrame, passage_col: str = \"passage\"):\n",
    "    \"\"\"\n",
    "    Build a deduplicated corpus of passages (each passage is one document).\n",
    "    Returns:\n",
    "      - corpus: list[str] (unique normalized passages)\n",
    "      - docid_lookup: dict[int->str] mapping doc_id to passage text\n",
    "      - rel_map: dict[row_idx_str -> doc_id_str] mapping each question to its relevant doc id\n",
    "    \"\"\"\n",
    "    # Normalize passage strings\n",
    "    passages = [normalize_text(p) for p in df[passage_col].tolist()]\n",
    "    # Deduplicate with order preserved\n",
    "    seen = {}\n",
    "    corpus = []\n",
    "    for p in passages:\n",
    "        if p not in seen:\n",
    "            seen[p] = len(corpus)\n",
    "            corpus.append(p)\n",
    "    # Build relevance map: each row maps to the doc_id of its (normalized) passage\n",
    "    rel_map = {}\n",
    "    for i, p in enumerate(passages):\n",
    "        rel_map[str(i)] = str(seen[p])\n",
    "    docid_lookup = {str(i): corpus[i] for i in range(len(corpus))}\n",
    "    return corpus, docid_lookup, rel_map\n",
    "\n",
    "def mean_pool(last_hidden_state, attention_mask):\n",
    "    # last_hidden_state: [B, L, H], attention_mask: [B, L]\n",
    "    mask = attention_mask.unsqueeze(-1).type_as(last_hidden_state)  # [B,L,1]\n",
    "    sum_embeddings = (last_hidden_state * mask).sum(dim=1)          # [B,H]\n",
    "    lengths = mask.sum(dim=1).clamp(min=1e-9)                       # [B,1]\n",
    "    return sum_embeddings / lengths\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_texts(texts, tokenizer, model, device=\"cuda\" if torch.cuda.is_available() else \"cpu\", batch_size=64, max_length=256):\n",
    "    model.eval().to(device)\n",
    "    all_vecs = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        toks = tokenizer(\n",
    "            batch,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        toks = {k: v.to(device) for k, v in toks.items()}\n",
    "        outputs = model(**toks)\n",
    "        # Use last_hidden_state and mean pooling with attention mask\n",
    "        emb = mean_pool(outputs.last_hidden_state, toks[\"attention_mask\"])\n",
    "        # L2 normalize\n",
    "        emb = torch.nn.functional.normalize(emb, p=2, dim=1)\n",
    "        all_vecs.append(emb.cpu())\n",
    "    return torch.cat(all_vecs, dim=0).numpy().astype(\"float32\")  # [N, D]\n",
    "\n",
    "def build_faiss_index(embs: np.ndarray, use_gpu=False):\n",
    "    \"\"\"\n",
    "    Build a FAISS IndexFlatIP and add embeddings.\n",
    "    \"\"\"\n",
    "    d = embs.shape[1]\n",
    "    index = faiss.IndexFlatIP(d)\n",
    "    # If GPU desired and available:\n",
    "    if use_gpu and faiss.get_num_gpus() > 0:\n",
    "        res = faiss.StandardGpuResources()\n",
    "        index = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "    index.add(embs)  # add vectors\n",
    "    return index\n",
    "\n",
    "def search_faiss(index, query_vecs: np.ndarray, topk: int = 10):\n",
    "    # FAISS expects float32\n",
    "    q = query_vecs.astype(\"float32\")\n",
    "    scores, idxs = index.search(q, topk)  # [Q, topk]\n",
    "    return scores, idxs\n",
    "\n",
    "def ap_at_k(ret_ids, rel_id, k):\n",
    "    for rank, did in enumerate(ret_ids[:k], 1):\n",
    "        if did == rel_id:\n",
    "            return 1.0 / rank\n",
    "    return 0.0\n",
    "\n",
    "def ndcg_at_k(ret_ids, rel_id, k):\n",
    "    for rank, did in enumerate(ret_ids[:k], 1):\n",
    "        if did == rel_id:\n",
    "            return 1.0 / math.log2(rank + 1)\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Model wrappers\n",
    "# -----------------------\n",
    "\n",
    "class DenseRetriever:\n",
    "    def __init__(self, model_name, batch_size=64, max_length=256, device=None):\n",
    "        self.model_name = model_name\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tok = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    def encode_corpus(self, corpus_texts):\n",
    "        return encode_texts(corpus_texts, self.tok, self.model, self.device, self.batch_size, self.max_length)\n",
    "\n",
    "    def encode_queries(self, queries):\n",
    "        return encode_texts(queries, self.tok, self.model, self.device, self.batch_size, self.max_length)\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Pipeline\n",
    "# -----------------------\n",
    "\n",
    "def run_pipeline_halubench(\n",
    "    parquet_path=\"hf://datasets/PatronusAI/HaluBench/data/test-00000-of-00001.parquet\",\n",
    "    output_dir=\"./outputs_dense\",\n",
    "    topk=TOPK,\n",
    "    batch_size=64,\n",
    "    use_gpu_faiss=False\n",
    "):\n",
    "    ensure_dir(output_dir)\n",
    "\n",
    "    # 1) Load data\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "    # Expect columns: id, question, answer, passage\n",
    "    for col in [\"id\", \"question\", \"answer\", \"passage\"]:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Missing required column '{col}' in dataframe\")\n",
    "\n",
    "    # 2) Normalize and deduplicate passages (each passage is one document)\n",
    "    corpus, docid_lookup, rel_map = build_passage_dedup(df, passage_col=\"passage\")\n",
    "\n",
    "    # 3) Prepare queries (normalize questions for consistency)\n",
    "    queries = {str(i): normalize_text(df.loc[i, \"question\"]) for i in range(len(df))}\n",
    "    query_list = [queries[str(i)] for i in range(len(df))]\n",
    "\n",
    "    # 4) Create two dense retrievers: MPNet and Contriever\n",
    "    retrievers = {\n",
    "        \"mpnet\": DenseRetriever(MPNET_NAME, batch_size=batch_size),\n",
    "        \"contriever\": DenseRetriever(CONTRIEVER_NAME, batch_size=batch_size),\n",
    "    }\n",
    "\n",
    "    for name, retr in retrievers.items():\n",
    "        print(f\"\\n=== Building FAISS index for {name} ===\")\n",
    "        # 5) Encode corpus and build FAISS\n",
    "        corpus_embs = retr.encode_corpus(corpus)  # [N, D], L2-normalized\n",
    "        index = build_faiss_index(corpus_embs, use_gpu=use_gpu_faiss)\n",
    "\n",
    "        # 6) Encode queries and retrieve\n",
    "        query_embs = retr.encode_queries(query_list)  # [Q, D], L2-normalized\n",
    "        scores, idxs = search_faiss(index, query_embs, topk=topk)\n",
    "\n",
    "        # 7) Build retrieval results in the same structure as before\n",
    "        # Map FAISS ids to our doc_id strings\n",
    "        out_res = {}\n",
    "        for i in range(len(df)):\n",
    "            ret = []\n",
    "            for r in range(topk):\n",
    "                did_int = int(idxs[i, r])\n",
    "                if did_int < 0 or did_int >= len(corpus):\n",
    "                    continue\n",
    "                did = str(did_int)\n",
    "                sc = float(scores[i, r])\n",
    "                ret.append((did, sc))\n",
    "            out_res[str(i)] = ret\n",
    "\n",
    "        # 8) Save results and compute MAP/NDCG against the single relevant doc per question\n",
    "        save_results_dense(\n",
    "            name=name,\n",
    "            df=df,\n",
    "            ret=out_res,\n",
    "            rel_map=rel_map,           # single relevant doc id per row (string)\n",
    "            corpus_lookup=docid_lookup,\n",
    "            out_csv=os.path.join(output_dir, f\"{name}_results.csv\")\n",
    "        )\n",
    "\n",
    "        print(f\"{name}: results saved to {os.path.join(output_dir, f'{name}_results.csv')}\")\n",
    "\n",
    "\n",
    "def save_results_dense(name, df, ret, rel_map, corpus_lookup, out_csv):\n",
    "    rows = []\n",
    "    ks = [3, 5, 10]\n",
    "    for i in range(len(df)):\n",
    "        qid = str(i)\n",
    "        retrieved = ret.get(qid, [])\n",
    "        ret_ids = [d for d, _ in retrieved]\n",
    "        rel = rel_map[qid]  # single relevant doc id for this question\n",
    "\n",
    "        metrics = {}\n",
    "        for k in ks:\n",
    "            metrics[f\"MAP@{k}\"] = ap_at_k(ret_ids, rel, k)\n",
    "            metrics[f\"NDCG@{k}\"] = ndcg_at_k(ret_ids, rel, k)\n",
    "\n",
    "        # Store full passage text + short preview for readability\n",
    "        pack = []\n",
    "        for d, s in retrieved:\n",
    "            full = corpus_lookup.get(d, \"\")\n",
    "            preview = full[:300].replace(\"\\n\", \" \")\n",
    "            pack.append({\n",
    "                \"doc_id\": d,\n",
    "                \"score\": round(float(s), 6),\n",
    "                # full content for robust evaluation\n",
    "                \"full_text\": full,\n",
    "                # backward-compatible alias (optional)\n",
    "                \"snippet\": preview,\n",
    "                # # explicit preview field (optional)\n",
    "                # \"passage\": preview\n",
    "            })\n",
    "\n",
    "        rows.append({\n",
    "            \"id\": df.loc[i, \"id\"],\n",
    "            \"question\": normalize_text(df.loc[i, \"question\"]),\n",
    "            \"answer\": normalize_text(df.loc[i, \"answer\"]),\n",
    "            # Store the original (normalized) passage used as the single ground truth\n",
    "            \"passage\": normalize_text(df.loc[i, \"passage\"]),\n",
    "            f\"{name}_ret_docs\": json.dumps(pack, ensure_ascii=False),\n",
    "            **metrics\n",
    "        })\n",
    "\n",
    "    pd.DataFrame(rows).to_csv(out_csv, index=False)\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Run\n",
    "# -----------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # This loads HaluBench and runs both dense retrievers with FAISS\n",
    "    run_pipeline_halubench(\n",
    "        parquet_path=\"hf://datasets/PatronusAI/HaluBench/data/test-00000-of-00001.parquet\",\n",
    "        output_dir=\"./outputs_dense\",\n",
    "        topk=10,\n",
    "        batch_size=64,\n",
    "        use_gpu_faiss=False  # set True if you have faiss-gpu installed and a GPU available\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mpnet Results:\n",
      "  MAP@3: 0.7672, NDCG@3: 0.7779\n",
      "  MAP@5: 0.7740, NDCG@5: 0.7903\n",
      "  MAP@10: 0.7793, NDCG@10: 0.8028\n",
      "\n",
      "Contriever Results:\n",
      "  MAP@3: 0.7477, NDCG@3: 0.7570\n",
      "  MAP@5: 0.7524, NDCG@5: 0.7656\n",
      "  MAP@10: 0.7562, NDCG@10: 0.7746\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, os\n",
    "for name in [\"mpnet\", \"contriever\"]:\n",
    "    df = pd.read_csv(os.path.join(\"outputs_dense\", f\"{name}_results.csv\"))\n",
    "    print(name.capitalize(), \"Results:\")\n",
    "    for k in (3,5,10):\n",
    "        print(f\"  MAP@{k}: {pd.to_numeric(df[f'MAP@{k}'], errors='coerce').mean():.4f}, \"\n",
    "              f\"NDCG@{k}: {pd.to_numeric(df[f'NDCG@{k}'], errors='coerce').mean():.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "halu_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
