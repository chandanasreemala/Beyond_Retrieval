{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['question', 'answer', 'passage', 'splade_ret_docs', 'MAP@3', 'NDCG@3',\n",
       "       'MAP@5', 'NDCG@5', 'MAP@10', 'NDCG@10'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('./prior_results/halubench_splade.csv') \n",
    "\n",
    "df.columns\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"doc_id\": \"391\", \"score\": 20.2985, \"snippet\": \"September 25, 2011 at Paul Brown Stadium, Cincinnati, Ohio (Blacked Out) Playing versus the San Francisco 49ers for the first time since 2003, the Bengals lost 13-8 with the smallest crowd for a Benga\"}, {\"doc_id\": \"432\", \"score\": 19.8393, \"snippet\": \"After winning on the road, the Bengals returned home for Game 2 against the Steelers.  The Bengals scored first in the first quarter when Randy Bullock kicked a 35-yard field goal to make it 3-0.  The\"}, {\"doc_id\": \"201\", \"score\": 19.5582, \"snippet\": \" After winning at home, the Bengals traveled down south to take on the Jaguars.  The Jags scored first in the first quarter when Josh Lambo kicked a 32-yard field goal to make it 3-0.  They would make\"}, {\"doc_id\": \"199\", \"score\": 19.3586, \"snippet\": \" The Steelers started their 2014 season at home against the Browns.  In the first quarter, the Steelers would score first when Shaun Suisham kicked a 36-yard field goal for a 3-0 lead.  However, the B\"}, {\"doc_id\": \"13\", \"score\": 19.3057, \"snippet\": \"San Francisco kept Detroit in the game with missed opportunities, then made just enough plays for a rare two-game winning streak and its first road win. RB Frank Gore set a franchise record with 148 y\"}, {\"doc_id\": \"474\", \"score\": 19.1567, \"snippet\": \" For their last home game of the season, the San Francisco 49ers took on the visiting team the Lions. The Lions took an early lead midway through the 1st quarter with a 27-yard Jason Hanson field goal\"}, {\"doc_id\": \"402\", \"score\": 19.1182, \"snippet\": \"The Steelers went back home for a Thursday Night duel against the Titans. In the first quarter, the Steelers scored first when Ben Roethlisberger found Antonio Brown on a 41-yard pass to make it 7-0. \"}, {\"doc_id\": \"386\", \"score\": 18.9689, \"snippet\": \" For their home opener, the Lions hosted the Kansas City Chiefs. The Lions scored first when Calvin Johnson caught a 15-yard pass for a touchdown. Kansas City then scored their only points of the game\"}, {\"doc_id\": \"476\", \"score\": 18.8317, \"snippet\": \"The 49ers dominated the first half and led 28-7 at halftime.  However, their second half offense stalled while the defense fell apart as the 49ers squandered a 21-point 3rd quarter lead and a 14-point\"}, {\"doc_id\": \"180\", \"score\": 18.6487, \"snippet\": \" The Bengals\\' home opener was against the Chargers.  In the first quarter, Andy Dalton found A. J. Green on a 16-yard pass for a 7-0 lead.  The Chargers would later on score when Josh Lambo kicked a 4\"}]'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Halubench Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<unknown>:45: SyntaxWarning: invalid decimal literal\n",
      "<unknown>:50: SyntaxWarning: invalid decimal literal\n",
      "<unknown>:96: SyntaxWarning: invalid decimal literal\n",
      "<unknown>:49: SyntaxWarning: invalid decimal literal\n",
      "<unknown>:53: SyntaxWarning: invalid decimal literal\n",
      "<unknown>:37: SyntaxWarning: invalid decimal literal\n",
      "<unknown>:49: SyntaxWarning: invalid decimal literal\n",
      "<unknown>:53: SyntaxWarning: invalid decimal literal\n",
      "<unknown>:45: SyntaxWarning: invalid decimal literal\n",
      "<unknown>:50: SyntaxWarning: invalid decimal literal\n",
      "<unknown>:45: SyntaxWarning: invalid decimal literal\n",
      "<unknown>:50: SyntaxWarning: invalid decimal literal\n",
      "<unknown>:105: SyntaxWarning: invalid decimal literal\n",
      "<unknown>:1: SyntaxWarning: invalid decimal literal\n",
      "<unknown>:1: SyntaxWarning: invalid decimal literal\n",
      "<unknown>:1: SyntaxWarning: invalid decimal literal\n",
      "<unknown>:1: SyntaxWarning: invalid decimal literal\n",
      "<unknown>:1: SyntaxWarning: invalid decimal literal\n",
      "<unknown>:1: SyntaxWarning: invalid decimal literal\n",
      "<unknown>:1: SyntaxWarning: invalid decimal literal\n",
      "<unknown>:1: SyntaxWarning: invalid decimal literal\n",
      "<unknown>:1: SyntaxWarning: invalid decimal literal\n",
      "<unknown>:1: SyntaxWarning: invalid decimal literal\n",
      "<unknown>:1: SyntaxWarning: invalid decimal literal\n",
      "<unknown>:1: SyntaxWarning: invalid decimal literal\n",
      "<unknown>:1: SyntaxWarning: invalid decimal literal\n",
      "<unknown>:1: SyntaxWarning: invalid decimal literal\n",
      "<unknown>:1: SyntaxWarning: invalid decimal literal\n",
      "<unknown>:1: SyntaxWarning: invalid decimal literal\n",
      "<unknown>:1: SyntaxWarning: invalid decimal literal\n",
      "<unknown>:1: SyntaxWarning: invalid decimal literal\n",
      "<unknown>:1: SyntaxWarning: invalid decimal literal\n",
      "<unknown>:1: SyntaxWarning: invalid decimal literal\n",
      "<unknown>:1: SyntaxWarning: invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from typing import List, Dict, Any, Optional\n",
    "import math\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "import string\n",
    "\n",
    "# ---------- Safe checks ----------\n",
    "\n",
    "def is_nan_like(x: Any) -> bool:\n",
    "    if x is None:\n",
    "        return True\n",
    "    if isinstance(x, (list, tuple, np.ndarray, dict)):\n",
    "        return False\n",
    "    try:\n",
    "        if pd.isna(x):\n",
    "            return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    if isinstance(x, str) and x.strip() == \"\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def to_python_list(x: Any) -> Any:\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return x.tolist()\n",
    "    return x\n",
    "\n",
    "# ---------- Text normalization ----------\n",
    "\n",
    "NOWIKI_PATTERN = re.compile(r\"<nowiki>.*?</nowiki>\", flags=re.IGNORECASE)\n",
    "HTML_TAG_PATTERN = re.compile(r\"<[^>]+>\")  # crude strip of any HTML tags\n",
    "WHITESPACE_PATTERN = re.compile(r\"\\s+\")\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        s = str(s)\n",
    "    s = s.lower()\n",
    "    # remove <nowiki>...</nowiki>\n",
    "    s = NOWIKI_PATTERN.sub(\" \", s)\n",
    "    # remove html tags if any\n",
    "    s = HTML_TAG_PATTERN.sub(\" \", s)\n",
    "    # remove punctuation\n",
    "    s = s.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    # collapse whitespace\n",
    "    s = WHITESPACE_PATTERN.sub(\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# ---------- Parsing helpers ----------\n",
    "\n",
    "def parse_docs(cell: Any, id_key: str = \"doc_id\") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Expect retrieved docs as:\n",
    "      - JSON/Python list of dicts with {doc_id, score?, snippet?}\n",
    "      - list of doc_id strings/ints\n",
    "    Returns a list of dicts with doc_id and snippet (normalized_snippet added).\n",
    "    \"\"\"\n",
    "    if is_nan_like(cell):\n",
    "        return []\n",
    "\n",
    "    cell = to_python_list(cell)\n",
    "\n",
    "    if isinstance(cell, list):\n",
    "        out = []\n",
    "        for d in cell:\n",
    "            if isinstance(d, dict) and id_key in d:\n",
    "                doc = {**d, id_key: str(d[id_key])}\n",
    "                snip = doc.get(\"snippet\")\n",
    "                if isinstance(snip, str):\n",
    "                    doc[\"_norm_snippet\"] = normalize_text(snip)\n",
    "                else:\n",
    "                    doc[\"_norm_snippet\"] = \"\"\n",
    "                out.append(doc)\n",
    "            elif isinstance(d, (str, int)):\n",
    "                out.append({id_key: str(d), \"_norm_snippet\": \"\"})\n",
    "        return out\n",
    "\n",
    "    if isinstance(cell, dict):\n",
    "        if id_key in cell:\n",
    "            doc = {**cell, id_key: str(cell[id_key])}\n",
    "            snip = doc.get(\"snippet\")\n",
    "            doc[\"_norm_snippet\"] = normalize_text(snip) if isinstance(snip, str) else \"\"\n",
    "            return [doc]\n",
    "        return []\n",
    "\n",
    "    if isinstance(cell, str):\n",
    "        s = cell.strip()\n",
    "        if not s:\n",
    "            return []\n",
    "        try:\n",
    "            data = json.loads(s)\n",
    "            return parse_docs(data, id_key=id_key)\n",
    "        except json.JSONDecodeError:\n",
    "            try:\n",
    "                data = ast.literal_eval(s)\n",
    "                return parse_docs(data, id_key=id_key)\n",
    "            except Exception:\n",
    "                return []\n",
    "\n",
    "    return []\n",
    "\n",
    "def parse_passage_as_text_list(cell: Any) -> List[str]:\n",
    "    \"\"\"\n",
    "    Passage holds ground-truth text(s). Accept:\n",
    "      - single string (one passage)\n",
    "      - list of strings\n",
    "      - JSON or Python-literal string for either of the above\n",
    "    Return list of normalized strings.\n",
    "    \"\"\"\n",
    "    if is_nan_like(cell):\n",
    "        return []\n",
    "\n",
    "    cell = to_python_list(cell)\n",
    "\n",
    "    texts = []\n",
    "    if isinstance(cell, list):\n",
    "        for x in cell:\n",
    "            if isinstance(x, str):\n",
    "                texts.append(normalize_text(x))\n",
    "            elif isinstance(x, (int, float)):\n",
    "                texts.append(normalize_text(str(x)))\n",
    "            elif isinstance(x, dict):\n",
    "                # if dict, try common keys\n",
    "                for k in (\"text\", \"passage\", \"snippet\"):\n",
    "                    if k in x and isinstance(x[k], str):\n",
    "                        texts.append(normalize_text(x[k]))\n",
    "                        break\n",
    "    elif isinstance(cell, dict):\n",
    "        for k in (\"text\", \"passage\", \"snippet\"):\n",
    "            if k in cell and isinstance(cell[k], str):\n",
    "                texts.append(normalize_text(cell[k]))\n",
    "                break\n",
    "    elif isinstance(cell, (str, int, float)):\n",
    "        s = str(cell).strip()\n",
    "        if not s:\n",
    "            return []\n",
    "        # Try JSON then literal_eval; if fails, treat as plain string\n",
    "        try:\n",
    "            data = json.loads(s)\n",
    "            return parse_passage_as_text_list(data)\n",
    "        except json.JSONDecodeError:\n",
    "            try:\n",
    "                data = ast.literal_eval(s)\n",
    "                return parse_passage_as_text_list(data)\n",
    "            except Exception:\n",
    "                texts.append(normalize_text(s))\n",
    "    return texts\n",
    "\n",
    "# ---------- Simple text-based relevance matching ----------\n",
    "\n",
    "def is_match(gt_text: str, cand_text: str) -> bool:\n",
    "    \"\"\"\n",
    "    Heuristic match:\n",
    "      - substring either way after normalization\n",
    "    \"\"\"\n",
    "    if not gt_text or not cand_text:\n",
    "        return False\n",
    "    return (gt_text in cand_text) or (cand_text in gt_text)\n",
    "\n",
    "def build_binary_relevance_from_text(\n",
    "    retrieved: List[Dict[str, Any]],\n",
    "    gt_texts: List[str]\n",
    ") -> List[int]:\n",
    "    \"\"\"\n",
    "    For each retrieved item (in order), produce 1 if any GT text matches the snippet text, else 0.\n",
    "    \"\"\"\n",
    "    rel = []\n",
    "    for d in retrieved:\n",
    "        cand = d.get(\"_norm_snippet\", \"\")\n",
    "        hit = any(is_match(gt, cand) for gt in gt_texts)\n",
    "        rel.append(1 if hit else 0)\n",
    "    return rel\n",
    "\n",
    "# ---------- RRF fusion ----------\n",
    "\n",
    "def rrf_fuse_lists(\n",
    "    lists: List[List[Dict[str, Any]]],\n",
    "    k_rrf: int = 60,\n",
    "    id_key: str = \"doc_id\"\n",
    ") -> List[Dict[str, Any]]:\n",
    "    scores = {}\n",
    "    rep = {}\n",
    "    for lst in lists:\n",
    "        # If not guaranteed ranked, enable sorting:\n",
    "        # lst = sorted(lst, key=lambda d: d.get(\"score\", float(\"-inf\")), reverse=True)\n",
    "        for rank, d in enumerate(lst, start=1):\n",
    "            did = d.get(id_key)\n",
    "            if did is None:\n",
    "                continue\n",
    "            did = str(did)\n",
    "            scores[did] = scores.get(did, 0.0) + 1.0 / (k_rrf + rank)\n",
    "            if did not in rep:\n",
    "                rep[did] = d\n",
    "\n",
    "    fused = []\n",
    "    for did, s in scores.items():\n",
    "        base = rep[did]\n",
    "        item = {\n",
    "            \"doc_id\": did,\n",
    "            \"rrf_score\": float(s),\n",
    "        }\n",
    "        if \"snippet\" in base:\n",
    "            item[\"snippet\"] = base[\"snippet\"]\n",
    "        if \"_norm_snippet\" in base:\n",
    "            item[\"_norm_snippet\"] = base[\"_norm_snippet\"]\n",
    "        if \"score\" in base:\n",
    "            item[\"orig_score\"] = base[\"score\"]\n",
    "        fused.append(item)\n",
    "\n",
    "    fused.sort(key=lambda x: x[\"rrf_score\"], reverse=True)\n",
    "    return fused\n",
    "\n",
    "# ---------- Metrics ----------\n",
    "\n",
    "def average_precision_at_k_from_binary(rel: List[int], k: int) -> float:\n",
    "    \"\"\"\n",
    "    rel: list of 0/1 indicating relevance at each rank position (already aligned with retrieved order).\n",
    "    \"\"\"\n",
    "    # AP@k for binary relevance with unknown total relevant count -> standard IR AP:\n",
    "    # sum(precision@i when rel[i]=1)/num_relevant\n",
    "    # If no relevant, return 0\n",
    "    rel_at_k = rel[:k]\n",
    "    num_relevant = sum(rel)\n",
    "    if num_relevant == 0:\n",
    "        return 0.0\n",
    "    hits = 0\n",
    "    ap_sum = 0.0\n",
    "    for i, r in enumerate(rel_at_k, start=1):\n",
    "        if r == 1:\n",
    "            hits += 1\n",
    "            ap_sum += hits / i\n",
    "    return ap_sum / num_relevant\n",
    "\n",
    "def dcg_at_k_from_binary(rel: List[int], k: int) -> float:\n",
    "    dcg = 0.0\n",
    "    for i, g in enumerate(rel[:k], start=1):\n",
    "        if g:\n",
    "            dcg += 1.0 / math.log2(i + 1)\n",
    "    return dcg\n",
    "\n",
    "def ndcg_at_k_from_binary(rel: List[int], k: int) -> float:\n",
    "    dcg = dcg_at_k_from_binary(rel, k)\n",
    "    ideal = sorted(rel, reverse=True)  # all 1s first\n",
    "    idcg = dcg_at_k_from_binary(ideal, k)\n",
    "    return 0.0 if idcg == 0 else dcg / idcg\n",
    "\n",
    "# ---------- Main pipeline ----------\n",
    "\n",
    "def hybrid_rrf_with_eval(\n",
    "    splade_csv: str,\n",
    "    mpnet_csv: str,\n",
    "    join_key: str = \"question\",\n",
    "    answer_col: str = \"answer\",\n",
    "    passage_col: str = \"passage\",  # GT text\n",
    "    splade_docs_col: str = \"splade_ret_docs\",\n",
    "    mpnet_docs_col: str = \"mpnet_ret_docs\",\n",
    "    k_rrf: int = 60,\n",
    "    topk_to_keep: int = 10\n",
    ") -> pd.DataFrame:\n",
    "    # Read\n",
    "    s = pd.read_csv(splade_csv)\n",
    "    d = pd.read_csv(mpnet_csv)\n",
    "\n",
    "    # Keep needed columns\n",
    "    s_cols = [c for c in [join_key, answer_col, passage_col, splade_docs_col] if c in s.columns]\n",
    "    d_cols = [c for c in [join_key, answer_col, passage_col, mpnet_docs_col] if c in d.columns]\n",
    "    s_small = s[s_cols].copy()\n",
    "    d_small = d[d_cols].copy()\n",
    "\n",
    "    # Rename duplicates from d_small before merge (except join key)\n",
    "    for col in d_cols:\n",
    "        if col != join_key and col in s_small.columns:\n",
    "            d_small = d_small.rename(columns={col: f\"{col}__d\"})\n",
    "\n",
    "    # Merge\n",
    "    df = s_small.merge(d_small, on=join_key, how=\"outer\")\n",
    "\n",
    "    def pick(row, primary: str, secondary: str) -> Any:\n",
    "        if primary in row and not is_nan_like(row[primary]):\n",
    "            return row[primary]\n",
    "        if secondary in row and not is_nan_like(row[secondary]):\n",
    "            return row[secondary]\n",
    "        return None\n",
    "\n",
    "    rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        question = row.get(join_key)\n",
    "        answer = row.get(answer_col) if answer_col in row else row.get(f\"{answer_col}__d\")\n",
    "\n",
    "        # Retrieved lists\n",
    "        splade_cell = pick(row, splade_docs_col, f\"{splade_docs_col}__d\")\n",
    "        mpnet_cell = pick(row, mpnet_docs_col, f\"{mpnet_docs_col}__d\")\n",
    "        splade_docs = parse_docs(splade_cell)\n",
    "        mpnet_docs = parse_docs(mpnet_cell)\n",
    "\n",
    "        # Fusion\n",
    "        fused = rrf_fuse_lists([splade_docs, mpnet_docs], k_rrf=k_rrf)\n",
    "        hybrid_ret_docs = fused[:topk_to_keep]\n",
    "\n",
    "        # Ground-truth texts from 'passage'\n",
    "        passage_cell = pick(row, passage_col, f\"{passage_col}__d\")\n",
    "        gt_texts = parse_passage_as_text_list(passage_cell)\n",
    "\n",
    "        # Binary relevance per retrieved doc by text match\n",
    "        rel_binary = build_binary_relevance_from_text(hybrid_ret_docs, gt_texts)\n",
    "\n",
    "        # Metrics\n",
    "        map3 = average_precision_at_k_from_binary(rel_binary, 3)\n",
    "        ndcg3 = ndcg_at_k_from_binary(rel_binary, 3)\n",
    "        map5 = average_precision_at_k_from_binary(rel_binary, 5)\n",
    "        ndcg5 = ndcg_at_k_from_binary(rel_binary, 5)\n",
    "        map10 = average_precision_at_k_from_binary(rel_binary, 10)\n",
    "        ndcg10 = ndcg_at_k_from_binary(rel_binary, 10)\n",
    "\n",
    "        rows.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            # Keep original passage text(s) (normalized for clarity). If you prefer raw, store passage_cell instead.\n",
    "            \"passage_norm\": gt_texts,\n",
    "            \"hybrid_ret_docs\": hybrid_ret_docs,\n",
    "            \"MAP@3\": map3,\n",
    "            \"NDCG@3\": ndcg3,\n",
    "            \"MAP@5\": map5,\n",
    "            \"NDCG@5\": ndcg5,\n",
    "            \"MAP@10\": map10,\n",
    "            \"NDCG@10\": ndcg10\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# -------- Example run and save --------\n",
    "result = hybrid_rrf_with_eval(\n",
    "    splade_csv=\"prior_results/halubench_splade.csv\",\n",
    "    mpnet_csv=\"prior_results/halubench_mpnet.csv\",\n",
    "    join_key=\"question\",\n",
    "    answer_col=\"answer\",\n",
    "    passage_col=\"passage\",\n",
    "    splade_docs_col=\"splade_ret_docs\",\n",
    "    mpnet_docs_col=\"mpnet_ret_docs\",\n",
    "    k_rrf=60,\n",
    "    topk_to_keep=10\n",
    ")\n",
    "result.to_csv(\"hybrid_rrf_with_metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Halubench Results:\n",
      "  MAP@3: 0.5016, NDCG@3: 0.5057\n",
      "  MAP@5: 0.5038, NDCG@5: 0.5093\n",
      "  MAP@10: 0.5059, NDCG@10: 0.5136\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# results summary\n",
    "import pandas as pd\n",
    "for name, path in [(\"Hybrid Halubench\",\"hybrid_rrf_with_metrics.csv\")]:\n",
    "    df = pd.read_csv(path)\n",
    "    print(name, \"Results:\")\n",
    "    for k in (3,5,10):\n",
    "        print(f\"  MAP@{k}: {pd.to_numeric(df[f'MAP@{k}'], errors='coerce').mean():.4f}, \"\n",
    "              f\"NDCG@{k}: {pd.to_numeric(df[f'NDCG@{k}'], errors='coerce').mean():.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid all pipelines on Hotpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ast\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "# ============ CONFIG (paths relative to the notebook's folder) ============\n",
    "# Make sure your current working directory is the \"hybrid_retrieval\" folder (where the notebook lives).\n",
    "SPLADE_CSV = \"./prior_results/hotpotqa_splade.csv\"\n",
    "MPNET_CSV  = \"./prior_results/hotpotqa_mpnet.csv\"\n",
    "OUTPUT_CSV = \"./hybrid_weighted_score_sum_combmnz.csv\"\n",
    "\n",
    "# Baseline RRF parameter\n",
    "RRF_K = 60\n",
    "\n",
    "# Original optional rank-from-score normalization for RRF ranking\n",
    "NORMALIZE_SCORES = False\n",
    "\n",
    "# Fusion config\n",
    "# \"base\" == plain RRF (exactly your original hybrid)\n",
    "# Other options: \"rrf\" (alias of base), \"weighted_rrf\", \"weighted_score_sum\", \"weighted_score_sum_combmnz\"\n",
    "FUSION_MODE = \"weighted_score_sum_combmnz\"\n",
    "SCORE_NORM_PER_LIST = True     # used only for score-sum fusion modes: min-max per list\n",
    "WEIGHT_TEMPERATURE = 1.17      # temperature applied to QC logits; 1.0 no change; >1 smoother; <1 sharper\n",
    "LOG_WEIGHTS = True             # include weights & fusion meta in the hybrid output entries\n",
    "# ========================================================================\n",
    "\n",
    "\n",
    "# ---------- QC Classifier (BERT) ----------\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Local folder next to the notebook\n",
    "QC_MODEL_FOLDER = \"./bert_model_QC_finetuned\"\n",
    "\n",
    "_qc_tokenizer = None\n",
    "_qc_model = None\n",
    "\n",
    "def _lazy_load_qc():\n",
    "    \"\"\"\n",
    "    Lazily load the local QC model and tokenizer exactly from QC_MODEL_FOLDER.\n",
    "    Will not hit Hugging Face Hub (local_files_only=True).\n",
    "    \"\"\"\n",
    "    global _qc_tokenizer, _qc_model\n",
    "    if _qc_tokenizer is None or _qc_model is None:\n",
    "        if not os.path.isdir(QC_MODEL_FOLDER):\n",
    "            raise OSError(\n",
    "                f\"QC model folder not found: {QC_MODEL_FOLDER}. \"\n",
    "                f\"CWD: {os.getcwd()}. \"\n",
    "                f\"Ensure the directory exists and contains tokenizer + model files \"\n",
    "                f\"(e.g., config.json, pytorch_model.bin, tokenizer.json or vocab.txt, \"\n",
    "                f\"tokenizer_config.json, special_tokens_map.json).\"\n",
    "            )\n",
    "        _qc_tokenizer = BertTokenizer.from_pretrained(QC_MODEL_FOLDER, local_files_only=True)\n",
    "        _qc_model = BertForSequenceClassification.from_pretrained(QC_MODEL_FOLDER, local_files_only=True)\n",
    "        _qc_model.eval()\n",
    "\n",
    "def get_qc_weights(question: str, temperature: float = 1.0) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Run the QC classifier on a single question and return (w_sparse, w_dense).\n",
    "    Assumes class 0 = sparse, class 1 = dense.\n",
    "    Temperature is applied to logits before softmax.\n",
    "    \"\"\"\n",
    "    _lazy_load_qc()\n",
    "    inputs = _qc_tokenizer(question, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = _qc_model(**inputs)\n",
    "        logits = outputs.logits\n",
    "    probs = torch.softmax(logits / float(temperature), dim=-1)[0]  # shape: (2,)\n",
    "    w_sparse = float(probs[0].item())\n",
    "    w_dense = float(probs[1].item())\n",
    "    s = w_sparse + w_dense\n",
    "    if s <= 0:\n",
    "        return 0.5, 0.5\n",
    "    return w_sparse / s, w_dense / s\n",
    "\n",
    "\n",
    "# ---------- Utilities ----------\n",
    "\n",
    "def normalize_text(s: Any) -> str:\n",
    "    if s is None or (isinstance(s, float) and np.isnan(s)):\n",
    "        return \"\"\n",
    "    s = str(s)\n",
    "    s = s.strip().lower()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def safe_parse_list(val: Any) -> Any:\n",
    "    if isinstance(val, (list, dict)):\n",
    "        return val\n",
    "    if val is None or (isinstance(val, float) and np.isnan(val)):\n",
    "        return []\n",
    "    s = str(val).strip()\n",
    "    if s == \"\":\n",
    "        return []\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return ast.literal_eval(s)\n",
    "        except Exception:\n",
    "            return []\n",
    "\n",
    "def normalize_groundtruth_str(gt_field: Any) -> str:\n",
    "    data = safe_parse_list(gt_field)\n",
    "    if isinstance(data, list):\n",
    "        norm_items = [normalize_text(x) for x in data]\n",
    "        return json.dumps(norm_items, ensure_ascii=False)\n",
    "    return json.dumps([normalize_text(str(gt_field))], ensure_ascii=False)\n",
    "\n",
    "def parse_ret_list(ret_field: Any) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Parse retrieval list into a list of dicts: {doc_id, score, full_text}\n",
    "    \"\"\"\n",
    "    data = safe_parse_list(ret_field)\n",
    "    out = []\n",
    "    if isinstance(data, list):\n",
    "        for d in data:\n",
    "            if isinstance(d, dict) and \"doc_id\" in d:\n",
    "                doc_id = str(d.get(\"doc_id\"))\n",
    "                score = float(d.get(\"score\", 0.0))\n",
    "                # Use full_text (authoritative), ignore preview_snippet\n",
    "                full_text = d.get(\"full_text\", \"\")\n",
    "                out.append({\"doc_id\": doc_id, \"score\": score, \"full_text\": full_text})\n",
    "    return out\n",
    "\n",
    "def parse_groundtruth_list(gt_field: Any) -> List[str]:\n",
    "    data = safe_parse_list(gt_field)\n",
    "    if isinstance(data, list):\n",
    "        return [str(x) for x in data]\n",
    "    s = str(gt_field).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    if \",\" in s:\n",
    "        return [t.strip() for t in s.split(\",\") if t.strip()]\n",
    "    return s.split()\n",
    "\n",
    "def min_max_normalize(scores: List[float]) -> List[float]:\n",
    "    if not scores:\n",
    "        return []\n",
    "    mn, mx = min(scores), max(scores)\n",
    "    if mx == mn:\n",
    "        return [0.0 for _ in scores]\n",
    "    return [(x - mn) / (mx - mn) for x in scores]\n",
    "\n",
    "def rank_from_scores(items: List[Dict[str, Any]], normalize: bool = False) -> Dict[str, int]:\n",
    "    if not items:\n",
    "        return {}\n",
    "    arr = items.copy()\n",
    "    if normalize:\n",
    "        norm_scores = min_max_normalize([x[\"score\"] for x in arr])\n",
    "        for i, ns in enumerate(norm_scores):\n",
    "            arr[i][\"_tmp_score\"] = ns\n",
    "        key = \"_tmp_score\"\n",
    "    else:\n",
    "        key = \"score\"\n",
    "    arr_sorted = sorted(arr, key=lambda x: x.get(key, 0.0), reverse=True)\n",
    "    return {it[\"doc_id\"]: idx for idx, it in enumerate(arr_sorted, start=1)}\n",
    "\n",
    "\n",
    "# ---------- Fusion Methods ----------\n",
    "\n",
    "def rrf_fuse_detailed(lists: List[List[Dict[str, Any]]], k: int = 60, normalize_scores: bool = False):\n",
    "    \"\"\"\n",
    "    Reciprocal Rank Fusion with detailed output.\n",
    "    Returns a list sorted by RRF score desc:\n",
    "      [\n",
    "        {\n",
    "          'doc_id': str,\n",
    "          'rrf_score': float,\n",
    "          'ranks': [rank_in_list0_or_None, rank_in_list1_or_None, ...]\n",
    "        }, ...\n",
    "      ]\n",
    "    \"\"\"\n",
    "    rank_maps = [rank_from_scores(lst, normalize=normalize_scores) for lst in lists]\n",
    "    all_doc_ids = set().union(*[set(rm.keys()) for rm in rank_maps])\n",
    "\n",
    "    fused = []\n",
    "    for d in all_doc_ids:\n",
    "        ranks = [rm.get(d) for rm in rank_maps]\n",
    "        rrf_score = sum(1.0 / (k + r) for r in ranks if r is not None)\n",
    "        fused.append({\"doc_id\": d, \"rrf_score\": rrf_score, \"ranks\": ranks})\n",
    "\n",
    "    fused.sort(key=lambda x: x[\"rrf_score\"], reverse=True)\n",
    "    return fused\n",
    "\n",
    "def rrf_fuse_weighted(lists: List[List[Dict[str, Any]]], weights: List[float], k: int = 60, normalize_scores: bool = False):\n",
    "    \"\"\"\n",
    "    Weighted RRF: sum_i w_i * 1/(k + rank_i)\n",
    "    \"\"\"\n",
    "    rank_maps = [rank_from_scores(lst, normalize=normalize_scores) for lst in lists]\n",
    "    all_doc_ids = set().union(*[set(rm.keys()) for rm in rank_maps])\n",
    "\n",
    "    fused = []\n",
    "    for d in all_doc_ids:\n",
    "        ranks = [rm.get(d) for rm in rank_maps]\n",
    "        rrf_score = 0.0\n",
    "        for i, r in enumerate(ranks):\n",
    "            if r is not None:\n",
    "                rrf_score += float(weights[i]) * (1.0 / (k + r))\n",
    "        fused.append({\"doc_id\": d, \"rrf_score\": rrf_score, \"ranks\": ranks})\n",
    "\n",
    "    fused.sort(key=lambda x: x[\"rrf_score\"], reverse=True)\n",
    "    return fused\n",
    "\n",
    "def score_sum_fuse_weighted(lists: List[List[Dict[str, Any]]], weights: List[float], normalize_per_list: bool = True, combmnz: bool = False):\n",
    "    \"\"\"\n",
    "    Weighted linear score fusion:\n",
    "      fused_score(d) = sum_i w_i * s_i(d)\n",
    "    where s_i(d) are per-list scores, optionally min-max normalized within each list.\n",
    "    COMBMNZ variant multiplies by the count of lists where the doc appears:\n",
    "      fused_mnz(d) = fused_score(d) * (#systems that retrieved d)\n",
    "    \"\"\"\n",
    "    # Build doc_id -> score per list\n",
    "    score_maps = []\n",
    "    for lst in lists:\n",
    "        if normalize_per_list:\n",
    "            norm_scores = min_max_normalize([x[\"score\"] for x in lst])\n",
    "            m = {lst[i][\"doc_id\"]: norm_scores[i] for i in range(len(lst))}\n",
    "        else:\n",
    "            m = {x[\"doc_id\"]: float(x[\"score\"]) for x in lst}\n",
    "        score_maps.append(m)\n",
    "\n",
    "    all_doc_ids = set().union(*[set(m.keys()) for m in score_maps])\n",
    "\n",
    "    fused = []\n",
    "    for d in all_doc_ids:\n",
    "        per_list_scores = [m.get(d, 0.0) for m in score_maps]\n",
    "        fused_score = sum(float(weights[i]) * per_list_scores[i] for i in range(len(per_list_scores)))\n",
    "        num_systems_present = sum(1 for s in per_list_scores if s > 0.0)\n",
    "        if combmnz:\n",
    "            fused_score *= max(1, num_systems_present)\n",
    "        fused.append({\n",
    "            \"doc_id\": d,\n",
    "            \"fused_score\": fused_score,\n",
    "            \"per_list_scores\": per_list_scores,\n",
    "            \"num_systems\": num_systems_present\n",
    "        })\n",
    "\n",
    "    fused.sort(key=lambda x: x[\"fused_score\"], reverse=True)\n",
    "    return fused\n",
    "\n",
    "\n",
    "# ---------- Metrics (text-based evaluation) ----------\n",
    "\n",
    "def apk(actual: List[str], predicted: List[str], k: int) -> float:\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "    pred_k = predicted[:k]\n",
    "    hits, score = 0, 0.0\n",
    "    seen = set()\n",
    "    for i, p in enumerate(pred_k, start=1):\n",
    "        if p in actual and p not in seen:\n",
    "            hits += 1\n",
    "            score += hits / i\n",
    "            seen.add(p)\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual_list: List[List[str]], predicted_list: List[List[str]], k: int) -> float:\n",
    "    scores = [apk(a, p, k) for a, p in zip(actual_list, predicted_list)]\n",
    "    return float(np.mean(scores)) if scores else 0.0\n",
    "\n",
    "def dcg_at_k(predicted: List[str], ideal_texts: List[str], k: int) -> float:\n",
    "    pred_k = predicted[:k]\n",
    "    dcg = 0.0\n",
    "    for i, p in enumerate(pred_k, start=1):\n",
    "        rel = 1.0 if p in ideal_texts else 0.0\n",
    "        if rel:\n",
    "            dcg += rel / np.log2(i + 1)\n",
    "    return dcg\n",
    "\n",
    "def idcg_at_k(ideal_texts: List[str], k: int) -> float:\n",
    "    g = min(len(ideal_texts), k)\n",
    "    return sum(1.0 / np.log2(i + 1) for i in range(1, g + 1))\n",
    "\n",
    "def ndcg_at_k(predicted: List[str], ideal_texts: List[str], k: int) -> float:\n",
    "    idcg = idcg_at_k(ideal_texts, k)\n",
    "    if idcg == 0.0:\n",
    "        return 0.0\n",
    "    return dcg_at_k(predicted, ideal_texts, k) / idcg\n",
    "\n",
    "\n",
    "# ---------- Main Pipeline ----------\n",
    "\n",
    "def main():\n",
    "    # Optional sanity check\n",
    "    print(\"CWD:\", os.getcwd())\n",
    "    print(\"Expecting QC model folder at:\", os.path.abspath(QC_MODEL_FOLDER))\n",
    "\n",
    "    # Load files\n",
    "    df_sparse = pd.read_csv(SPLADE_CSV)\n",
    "    df_dense = pd.read_csv(MPNET_CSV)\n",
    "\n",
    "    # Preserve originals for output\n",
    "    df_sparse[\"_q_orig\"] = df_sparse[\"question\"]\n",
    "    df_sparse[\"_a_orig\"] = df_sparse[\"answer\"]\n",
    "    df_sparse[\"_gt_orig\"] = df_sparse[\"groundtruth_docs\"]\n",
    "\n",
    "    df_dense[\"_q_orig\"] = df_dense[\"question\"]\n",
    "    df_dense[\"_a_orig\"] = df_dense[\"answer\"]\n",
    "    df_dense[\"_gt_orig\"] = df_dense[\"groundtruth_docs\"]\n",
    "\n",
    "    # Normalized join keys\n",
    "    df_sparse[\"_q_norm\"] = df_sparse[\"question\"].apply(normalize_text)\n",
    "    df_sparse[\"_a_norm\"] = df_sparse[\"answer\"].apply(normalize_text)\n",
    "    df_sparse[\"_gt_norm\"] = df_sparse[\"groundtruth_docs\"].apply(normalize_groundtruth_str)\n",
    "\n",
    "    df_dense[\"_q_norm\"] = df_dense[\"question\"].apply(normalize_text)\n",
    "    df_dense[\"_a_norm\"] = df_dense[\"answer\"].apply(normalize_text)\n",
    "    df_dense[\"_gt_norm\"] = df_dense[\"groundtruth_docs\"].apply(normalize_groundtruth_str)\n",
    "\n",
    "    # Merge on normalized keys\n",
    "    df = pd.merge(\n",
    "        df_sparse,\n",
    "        df_dense,\n",
    "        on=[\"_q_norm\", \"_a_norm\", \"_gt_norm\"],\n",
    "        suffixes=(\"_splade\", \"_mpnet\"),\n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "    if df.empty:\n",
    "        raise ValueError(\"No rows matched after normalization. Check groundtruth formats across CSVs.\")\n",
    "\n",
    "    hybrid_ret_docs_col = []\n",
    "    map3_list, ndcg3_list = [], []\n",
    "    map5_list, ndcg5_list = [], []\n",
    "    map10_list, ndcg10_list = [], []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        # Parse lists\n",
    "        splade_list = parse_ret_list(row[\"splade_ret_docs\"])\n",
    "        mpnet_list  = parse_ret_list(row[\"mpnet_ret_docs\"])\n",
    "        gt_list_raw = parse_groundtruth_list(row[\"_gt_orig_splade\"])  # use either side; same after join\n",
    "\n",
    "        # Build full_text lookup for ID->full_text\n",
    "        fulltext_map = {}\n",
    "        for d in splade_list:\n",
    "            fulltext_map.setdefault(d[\"doc_id\"], d.get(\"full_text\", \"\"))\n",
    "        for d in mpnet_list:\n",
    "            fulltext_map.setdefault(d[\"doc_id\"], d.get(\"full_text\", \"\"))\n",
    "\n",
    "        # QC weights for this query (only needed for weighted modes)\n",
    "        question_text = row[\"_q_orig_splade\"]\n",
    "        if FUSION_MODE in (\"weighted_rrf\", \"weighted_score_sum\", \"weighted_score_sum_combmnz\"):\n",
    "            w_sparse, w_dense = get_qc_weights(question_text, temperature=WEIGHT_TEMPERATURE)\n",
    "            weights = [w_sparse, w_dense]  # [splade, mpnet]\n",
    "        else:\n",
    "            # Not used in base/rrf\n",
    "            w_sparse, w_dense = None, None\n",
    "            weights = None\n",
    "\n",
    "        # Fusion\n",
    "        # \"base\" is the original plain RRF (same as \"rrf\")\n",
    "        mode = FUSION_MODE\n",
    "        if mode == \"base\" or mode == \"rrf\":\n",
    "            fused = rrf_fuse_detailed([splade_list, mpnet_list], k=RRF_K, normalize_scores=NORMALIZE_SCORES)\n",
    "            fusion_label = \"base\"\n",
    "            for f in fused:\n",
    "                f[\"_fused_score\"] = f[\"rrf_score\"]\n",
    "        elif mode == \"weighted_rrf\":\n",
    "            fused = rrf_fuse_weighted([splade_list, mpnet_list], weights=weights, k=RRF_K, normalize_scores=NORMALIZE_SCORES)\n",
    "            fusion_label = \"weighted_rrf\"\n",
    "            for f in fused:\n",
    "                f[\"_fused_score\"] = f[\"rrf_score\"]\n",
    "        elif mode == \"weighted_score_sum\":\n",
    "            fused_score = score_sum_fuse_weighted([splade_list, mpnet_list], weights=weights, normalize_per_list=SCORE_NORM_PER_LIST, combmnz=False)\n",
    "            rank_maps = [rank_from_scores(splade_list, normalize=NORMALIZE_SCORES),\n",
    "                         rank_from_scores(mpnet_list, normalize=NORMALIZE_SCORES)]\n",
    "            fused = []\n",
    "            for item in fused_score:\n",
    "                d = item[\"doc_id\"]\n",
    "                fused.append({\n",
    "                    \"doc_id\": d,\n",
    "                    \"_fused_score\": item[\"fused_score\"],\n",
    "                    \"ranks\": [rank_maps[0].get(d), rank_maps[1].get(d)]\n",
    "                })\n",
    "            fused.sort(key=lambda x: x[\"_fused_score\"], reverse=True)\n",
    "            fusion_label = \"weighted_score_sum\"\n",
    "        elif mode == \"weighted_score_sum_combmnz\":\n",
    "            fused_score = score_sum_fuse_weighted([splade_list, mpnet_list], weights=weights, normalize_per_list=SCORE_NORM_PER_LIST, combmnz=True)\n",
    "            rank_maps = [rank_from_scores(splade_list, normalize=NORMALIZE_SCORES),\n",
    "                         rank_from_scores(mpnet_list, normalize=NORMALIZE_SCORES)]\n",
    "            fused = []\n",
    "            for item in fused_score:\n",
    "                d = item[\"doc_id\"]\n",
    "                fused.append({\n",
    "                    \"doc_id\": d,\n",
    "                    \"_fused_score\": item[\"fused_score\"],\n",
    "                    \"ranks\": [rank_maps[0].get(d), rank_maps[1].get(d)]\n",
    "                })\n",
    "            fused.sort(key=lambda x: x[\"_fused_score\"], reverse=True)\n",
    "            fusion_label = \"weighted_score_sum_combmnz\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown FUSION_MODE: {FUSION_MODE}\")\n",
    "\n",
    "        # Build hybrid_ret_docs with fused score, rank, full_text, per-model rank, and weights\n",
    "        hybrid_struct = []\n",
    "        fused_norm_fulltexts = []  # for evaluation by text\n",
    "        for idx, item in enumerate(fused, start=1):\n",
    "            doc_id = item[\"doc_id\"]\n",
    "            fused_score = item[\"_fused_score\"]\n",
    "            ranks = item.get(\"ranks\")\n",
    "            if ranks is None:\n",
    "                ranks = [None, None]\n",
    "            full_text = fulltext_map.get(doc_id, \"\")\n",
    "\n",
    "            entry = {\n",
    "                \"doc_id\": doc_id,\n",
    "                \"score\": fused_score,\n",
    "                \"rank\": idx,\n",
    "                \"full_text\": full_text,\n",
    "                \"source_ranks\": {\n",
    "                    \"splade\": ranks[0],\n",
    "                    \"mpnet\": ranks[1],\n",
    "                }\n",
    "            }\n",
    "            if LOG_WEIGHTS:\n",
    "                if weights is not None:\n",
    "                    entry[\"qc_weights\"] = {\"splade\": weights[0], \"mpnet\": weights[1]}\n",
    "                else:\n",
    "                    entry[\"qc_weights\"] = None\n",
    "                entry[\"fusion_mode\"] = fusion_label\n",
    "            hybrid_struct.append(entry)\n",
    "\n",
    "            # Collect normalized full text for evaluation by text\n",
    "            fused_norm_fulltexts.append(normalize_text(full_text))\n",
    "\n",
    "        # Save hybrid struct JSON\n",
    "        hybrid_ret_docs_col.append(json.dumps(hybrid_struct, ensure_ascii=False))\n",
    "\n",
    "        # Prepare normalized GT texts (maintain order for NDCG ideal)\n",
    "        gt_norm_texts = [normalize_text(x) for x in parse_groundtruth_list(row[\"_gt_orig_splade\"])]\n",
    "\n",
    "        # Metrics by text\n",
    "        pred = fused_norm_fulltexts\n",
    "        map3_list.append(apk(gt_norm_texts, pred, 3))\n",
    "        map5_list.append(apk(gt_norm_texts, pred, 5))\n",
    "        map10_list.append(apk(gt_norm_texts, pred, 10))\n",
    "        ndcg3_list.append(ndcg_at_k(pred, gt_norm_texts, 3))\n",
    "        ndcg5_list.append(ndcg_at_k(pred, gt_norm_texts, 5))\n",
    "        ndcg10_list.append(ndcg_at_k(pred, gt_norm_texts, 10))\n",
    "\n",
    "    # Build output with requested columns (use original SPLADE-side text columns)\n",
    "    out = pd.DataFrame({\n",
    "        \"question\": df[\"_q_orig_splade\"],\n",
    "        \"answer\": df[\"_a_orig_splade\"],\n",
    "        \"groundtruth_docs\": df[\"_gt_orig_splade\"],\n",
    "        \"splade_ret_docs\": df[\"splade_ret_docs\"],\n",
    "        \"mpnet_ret_docs\": df[\"mpnet_ret_docs\"],\n",
    "        \"hybrid_ret_docs\": hybrid_ret_docs_col,\n",
    "        \"MAP@3\": map3_list,\n",
    "        \"NDCG@3\": ndcg3_list,\n",
    "        \"MAP@5\": map5_list,\n",
    "        \"NDCG@5\": ndcg5_list,\n",
    "        \"MAP@10\": map10_list,\n",
    "        \"NDCG@10\": ndcg10_list,\n",
    "    })\n",
    "\n",
    "    cols = [\n",
    "        \"question\", \"answer\", \"groundtruth_docs\",\n",
    "        \"splade_ret_docs\", \"mpnet_ret_docs\", \"hybrid_ret_docs\",\n",
    "        \"MAP@3\", \"NDCG@3\", \"MAP@5\", \"NDCG@5\", \"MAP@10\", \"NDCG@10\"\n",
    "    ]\n",
    "    out = out[cols]\n",
    "\n",
    "    out.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    overall = {\n",
    "        \"MAP@3\": float(np.mean(map3_list)) if map3_list else 0.0,\n",
    "        \"NDCG@3\": float(np.mean(ndcg3_list)) if ndcg3_list else 0.0,\n",
    "        \"MAP@5\": float(np.mean(map5_list)) if map5_list else 0.0,\n",
    "        \"NDCG@5\": float(np.mean(ndcg5_list)) if ndcg5_list else 0.0,\n",
    "        \"MAP@10\": float(np.mean(map10_list)) if map10_list else 0.0,\n",
    "        \"NDCG@10\": float(np.mean(ndcg10_list)) if ndcg10_list else 0.0,\n",
    "    }\n",
    "    print(\"Saved:\", OUTPUT_CSV)\n",
    "    print(\"Fusion mode:\", FUSION_MODE)\n",
    "    print(\"QC temperature:\", WEIGHT_TEMPERATURE)\n",
    "    print(\"Hybrid overall averages:\", overall)\n",
    "\n",
    "\n",
    "# If running in a notebook cell, call main() explicitly:\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Saved: ./hybrid_results.csv\n",
    "- Fusion mode: base\n",
    "- QC temperature: 1.17\n",
    "- Hybrid overall averages: {'MAP@3': 0.44218438823684025, 'NDCG@3': 0.5229603376262054, 'MAP@5': 0.4671846504463248, 'NDCG@5': 0.5585157793137547, 'MAP@10': 0.48421740577612526, 'NDCG@10': 0.5866646649926855}\n",
    "\n",
    "- Saved: ./hybrid_weighted_rrf.csv\n",
    "- Fusion mode: weighted_rrf\n",
    "- QC temperature: 1.17\n",
    "- Hybrid overall averages: {'MAP@3': 0.35653002709287396, 'NDCG@3': 0.43031830609708344, 'MAP@5': 0.3723286350448347, 'NDCG@5': 0.45498537614676815, 'MAP@10': 0.38600341152032897, 'NDCG@10': 0.4796897845341999}\n",
    "\n",
    "- Saved: ./hybrid_weighted_score_sum_combmnz.csv\n",
    "- Fusion mode: weighted_score_sum_combmnz\n",
    "- QC temperature: 1.17\n",
    "- Hybrid overall averages: {'MAP@3': 0.381144547559114, 'NDCG@3': 0.4556785730660241, 'MAP@5': 0.39439389803778846, 'NDCG@5': 0.4763792247129293, 'MAP@10': 0.4196332105481169, 'NDCG@10': 0.527365715144966}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- Why is “hybrid (base)” worse than SPLADE alone?\n",
    "\n",
    "\n",
    "Hybrid underperforming SPLADE\n",
    "- RRF is rank-only. If SPLADE is much stronger than MPNet on this dataset (your metrics show that), vanilla RRF can hurt by:\n",
    "- - Elevating MPNet-only docs into the top-k due to reciprocal-rank contributions, displacing SPLADE’s strong hits.\n",
    "- - Using equal contribution from both lists regardless of quality.\n",
    "\n",
    "This is common: when one list is clearly better, naive RRF may degrade metrics.\n",
    "Ways to fix\n",
    "\n",
    "- Use top_k caps per list, then fuse:\n",
    "- - If you feed 10 docs from each, the fused universe is max 20 docs; but more importantly, limiting MPNet’s long tail reduces harm.\n",
    "- Prefer weighted fusion:\n",
    "- - Weighted-RRF with global weights favoring SPLADE (e.g., w_sparse=0.7, w_dense=0.3) or use your QC weights per-query.\n",
    "- - Weighted score fusion (min-max per list), optionally with COMBMNZ.\n",
    "- Re-rank cut: Always take only top-N after fusion (e.g., N=10 for evaluation). Your metrics are computed using the full fused list right now; that’s fine because MAP@k, NDCG@k look at only top-k anyway, but it’s clearer to explicitly cap to K.\n",
    "- Tune RRF K:\n",
    "Lower K makes top ranks dominate more; higher K flattens. Try K in {10, 30, 60, 100}.\n",
    "- Normalize scores before ranking:\n",
    "For rank maps, you currently have NORMALIZE_SCORES=False. Try True to reduce sensitivity to per-list scoring peculiarities before rank extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: /home/csmala/journal_rag/hybrid_pipeline\n",
      "Expecting QC model folder at: /home/csmala/journal_rag/hybrid_pipeline/bert_model_QC_finetuned\n",
      "Saved: ./hybrid_weighted_rrf.csv\n",
      "Fusion mode: weighted_rrf\n",
      "QC temperature: 1.17\n",
      "Hybrid overall averages: {'MAP@3': 0.40284538359668015, 'NDCG@3': 0.4782867077931083, 'MAP@5': 0.41393834538865126, 'NDCG@5': 0.4954578325048874, 'MAP@10': 0.42103152301077024, 'NDCG@10': 0.5075874708412451}\n"
     ]
    }
   ],
   "source": [
    "# New\n",
    "# Implementing 1) How to Make hybrid fusion robust to spiky weights?\n",
    "\n",
    "import json\n",
    "import ast\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "# ============ CONFIG (paths relative to the notebook's folder) ============\n",
    "# Make sure your current working directory is the \"hybrid_retrieval\" folder (where the notebook lives).\n",
    "SPLADE_CSV = \"./prior_results/hotpotqa_splade.csv\"\n",
    "MPNET_CSV  = \"./prior_results/hotpotqa_mpnet.csv\"\n",
    "OUTPUT_CSV = \"./hybrid_weighted_rrf.csv\"\n",
    "\n",
    "# Baseline RRF parameter\n",
    "RRF_K = 60\n",
    "\n",
    "# Original optional rank-from-score normalization for RRF ranking\n",
    "NORMALIZE_SCORES = False\n",
    "\n",
    "# Fusion config\n",
    "# \"base\" == plain RRF (exactly your original hybrid)\n",
    "# Other options: \"rrf\" (alias of base), \"weighted_rrf\", \"weighted_score_sum\", \"weighted_score_sum_combmnz\"\n",
    "FUSION_MODE = \"weighted_rrf\"\n",
    "SCORE_NORM_PER_LIST = True     # used only for score-sum fusion modes: min-max per list\n",
    "WEIGHT_TEMPERATURE = 1.17      # temperature applied to QC logits; 1.0 no change; >1 smoother; <1 sharper\n",
    "LOG_WEIGHTS = True             # include weights & fusion meta in the hybrid output entries\n",
    "\n",
    "# Robustness knobs for spiky weights\n",
    "CLAMP_WEIGHTS = True        # enforce floor/ceiling on per-query weights\n",
    "WEIGHT_FLOOR = 0.25         # lower bound for one retriever\n",
    "WEIGHT_CEIL = 0.75          # upper bound for the other (complements floor)\n",
    "\n",
    "SMOOTH_TOWARD_HALF = True   # blend weights toward 0.5\n",
    "SMOOTH_ALPHA = 0.7          # 0..1; 0 = full 0.5, 1 = original weight. e.g., 0.7 => w = 0.7*w + 0.3*0.5\n",
    "\n",
    "# Control how many items from each retriever go into fusion and how many to keep at the end\n",
    "TOP_K_PER_LIST = 20         # cap each input list before fusion\n",
    "FINAL_TOP_K = 10            # cap the final fused list before saving/evaluating\n",
    "\n",
    "# ========================================================================\n",
    "\n",
    "\n",
    "# ---------- QC Classifier (BERT) ----------\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Local folder next to the notebook\n",
    "QC_MODEL_FOLDER = \"./bert_model_QC_finetuned\"\n",
    "\n",
    "_qc_tokenizer = None\n",
    "_qc_model = None\n",
    "\n",
    "def _lazy_load_qc():\n",
    "    \"\"\"\n",
    "    Lazily load the local QC model and tokenizer exactly from QC_MODEL_FOLDER.\n",
    "    Will not hit Hugging Face Hub (local_files_only=True).\n",
    "    \"\"\"\n",
    "    global _qc_tokenizer, _qc_model\n",
    "    if _qc_tokenizer is None or _qc_model is None:\n",
    "        if not os.path.isdir(QC_MODEL_FOLDER):\n",
    "            raise OSError(\n",
    "                f\"QC model folder not found: {QC_MODEL_FOLDER}. \"\n",
    "                f\"CWD: {os.getcwd()}. \"\n",
    "                f\"Ensure the directory exists and contains tokenizer + model files \"\n",
    "                f\"(e.g., config.json, pytorch_model.bin, tokenizer.json or vocab.txt, \"\n",
    "                f\"tokenizer_config.json, special_tokens_map.json).\"\n",
    "            )\n",
    "        _qc_tokenizer = BertTokenizer.from_pretrained(QC_MODEL_FOLDER, local_files_only=True)\n",
    "        _qc_model = BertForSequenceClassification.from_pretrained(QC_MODEL_FOLDER, local_files_only=True)\n",
    "        _qc_model.eval()\n",
    "\n",
    "def get_qc_weights(question: str, temperature: float = 1.0) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Run the QC classifier on a single question and return (w_sparse, w_dense).\n",
    "    Assumes class 0 = sparse, class 1 = dense.\n",
    "    Temperature is applied to logits before softmax.\n",
    "    \"\"\"\n",
    "    _lazy_load_qc()\n",
    "    inputs = _qc_tokenizer(question, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = _qc_model(**inputs)\n",
    "        logits = outputs.logits\n",
    "    probs = torch.softmax(logits / float(temperature), dim=-1)[0]  # shape: (2,)\n",
    "    w_sparse = float(probs[0].item())\n",
    "    w_dense = float(probs[1].item())\n",
    "    s = w_sparse + w_dense\n",
    "    if s <= 0:\n",
    "        return 0.5, 0.5\n",
    "    return w_sparse / s, w_dense / s\n",
    "\n",
    "def clamp_pair(p0: float, p1: float, lo: float, hi: float) -> Tuple[float, float]:\n",
    "    # clamp first, renormalize, then clamp complement\n",
    "    p0 = max(lo, min(hi, p0))\n",
    "    p1 = 1.0 - p0\n",
    "    # ensure complement respects bounds too (if hi < 0.5 this is redundant but safe)\n",
    "    if p1 < lo:\n",
    "        p1 = lo\n",
    "        p0 = 1.0 - p1\n",
    "    if p1 > hi:\n",
    "        p1 = hi\n",
    "        p0 = 1.0 - p1\n",
    "    return p0, p1\n",
    "\n",
    "def smooth_toward_half(p0: float, p1: float, alpha: float) -> Tuple[float, float]:\n",
    "    # convex mix toward 0.5\n",
    "    p0_s = alpha * p0 + (1.0 - alpha) * 0.5\n",
    "    p1_s = alpha * p1 + (1.0 - alpha) * 0.5\n",
    "    s = p0_s + p1_s\n",
    "    if s <= 0:\n",
    "        return 0.5, 0.5\n",
    "    return p0_s / s, p1_s / s\n",
    "\n",
    "# ---------- Utilities ----------\n",
    "\n",
    "def normalize_text(s: Any) -> str:\n",
    "    if s is None or (isinstance(s, float) and np.isnan(s)):\n",
    "        return \"\"\n",
    "    s = str(s)\n",
    "    s = s.strip().lower()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def safe_parse_list(val: Any) -> Any:\n",
    "    if isinstance(val, (list, dict)):\n",
    "        return val\n",
    "    if val is None or (isinstance(val, float) and np.isnan(val)):\n",
    "        return []\n",
    "    s = str(val).strip()\n",
    "    if s == \"\":\n",
    "        return []\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return ast.literal_eval(s)\n",
    "        except Exception:\n",
    "            return []\n",
    "\n",
    "def normalize_groundtruth_str(gt_field: Any) -> str:\n",
    "    data = safe_parse_list(gt_field)\n",
    "    if isinstance(data, list):\n",
    "        norm_items = [normalize_text(x) for x in data]\n",
    "        return json.dumps(norm_items, ensure_ascii=False)\n",
    "    return json.dumps([normalize_text(str(gt_field))], ensure_ascii=False)\n",
    "\n",
    "def parse_ret_list(ret_field: Any) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Parse retrieval list into a list of dicts: {doc_id, score, full_text}\n",
    "    \"\"\"\n",
    "    data = safe_parse_list(ret_field)\n",
    "    out = []\n",
    "    if isinstance(data, list):\n",
    "        for d in data:\n",
    "            if isinstance(d, dict) and \"doc_id\" in d:\n",
    "                doc_id = str(d.get(\"doc_id\"))\n",
    "                score = float(d.get(\"score\", 0.0))\n",
    "                # Use full_text (authoritative), ignore preview_snippet\n",
    "                full_text = d.get(\"full_text\", \"\")\n",
    "                out.append({\"doc_id\": doc_id, \"score\": score, \"full_text\": full_text})\n",
    "    return out\n",
    "\n",
    "def parse_groundtruth_list(gt_field: Any) -> List[str]:\n",
    "    data = safe_parse_list(gt_field)\n",
    "    if isinstance(data, list):\n",
    "        return [str(x) for x in data]\n",
    "    s = str(gt_field).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    if \",\" in s:\n",
    "        return [t.strip() for t in s.split(\",\") if t.strip()]\n",
    "    return s.split()\n",
    "\n",
    "def min_max_normalize(scores: List[float]) -> List[float]:\n",
    "    if not scores:\n",
    "        return []\n",
    "    mn, mx = min(scores), max(scores)\n",
    "    if mx == mn:\n",
    "        return [0.0 for _ in scores]\n",
    "    return [(x - mn) / (mx - mn) for x in scores]\n",
    "\n",
    "def rank_from_scores(items: List[Dict[str, Any]], normalize: bool = False) -> Dict[str, int]:\n",
    "    if not items:\n",
    "        return {}\n",
    "    arr = items.copy()\n",
    "    if normalize:\n",
    "        norm_scores = min_max_normalize([x[\"score\"] for x in arr])\n",
    "        for i, ns in enumerate(norm_scores):\n",
    "            arr[i][\"_tmp_score\"] = ns\n",
    "        key = \"_tmp_score\"\n",
    "    else:\n",
    "        key = \"score\"\n",
    "    arr_sorted = sorted(arr, key=lambda x: x.get(key, 0.0), reverse=True)\n",
    "    return {it[\"doc_id\"]: idx for idx, it in enumerate(arr_sorted, start=1)}\n",
    "\n",
    "\n",
    "# ---------- Fusion Methods ----------\n",
    "\n",
    "def rrf_fuse_detailed(lists: List[List[Dict[str, Any]]], k: int = 60, normalize_scores: bool = False):\n",
    "    \"\"\"\n",
    "    Reciprocal Rank Fusion with detailed output.\n",
    "    Returns a list sorted by RRF score desc:\n",
    "      [\n",
    "        {\n",
    "          'doc_id': str,\n",
    "          'rrf_score': float,\n",
    "          'ranks': [rank_in_list0_or_None, rank_in_list1_or_None, ...]\n",
    "        }, ...\n",
    "      ]\n",
    "    \"\"\"\n",
    "    rank_maps = [rank_from_scores(lst, normalize=normalize_scores) for lst in lists]\n",
    "    all_doc_ids = set().union(*[set(rm.keys()) for rm in rank_maps])\n",
    "\n",
    "    fused = []\n",
    "    for d in all_doc_ids:\n",
    "        ranks = [rm.get(d) for rm in rank_maps]\n",
    "        rrf_score = sum(1.0 / (k + r) for r in ranks if r is not None)\n",
    "        fused.append({\"doc_id\": d, \"rrf_score\": rrf_score, \"ranks\": ranks})\n",
    "\n",
    "    fused.sort(key=lambda x: x[\"rrf_score\"], reverse=True)\n",
    "    return fused\n",
    "\n",
    "def rrf_fuse_weighted(lists: List[List[Dict[str, Any]]], weights: List[float], k: int = 60, normalize_scores: bool = False):\n",
    "    \"\"\"\n",
    "    Weighted RRF: sum_i w_i * 1/(k + rank_i)\n",
    "    \"\"\"\n",
    "    rank_maps = [rank_from_scores(lst, normalize=normalize_scores) for lst in lists]\n",
    "    all_doc_ids = set().union(*[set(rm.keys()) for rm in rank_maps])\n",
    "\n",
    "    fused = []\n",
    "    for d in all_doc_ids:\n",
    "        ranks = [rm.get(d) for rm in rank_maps]\n",
    "        rrf_score = 0.0\n",
    "        for i, r in enumerate(ranks):\n",
    "            if r is not None:\n",
    "                rrf_score += float(weights[i]) * (1.0 / (k + r))\n",
    "        fused.append({\"doc_id\": d, \"rrf_score\": rrf_score, \"ranks\": ranks})\n",
    "\n",
    "    fused.sort(key=lambda x: x[\"rrf_score\"], reverse=True)\n",
    "    return fused\n",
    "\n",
    "def score_sum_fuse_weighted(lists: List[List[Dict[str, Any]]], weights: List[float], normalize_per_list: bool = True, combmnz: bool = False):\n",
    "    \"\"\"\n",
    "    Weighted linear score fusion:\n",
    "      fused_score(d) = sum_i w_i * s_i(d)\n",
    "    where s_i(d) are per-list scores, optionally min-max normalized within each list.\n",
    "    COMBMNZ variant multiplies by the count of lists where the doc appears:\n",
    "      fused_mnz(d) = fused_score(d) * (#systems that retrieved d)\n",
    "    \"\"\"\n",
    "    # Build doc_id -> score per list\n",
    "    score_maps = []\n",
    "    for lst in lists:\n",
    "        if normalize_per_list:\n",
    "            norm_scores = min_max_normalize([x[\"score\"] for x in lst])\n",
    "            m = {lst[i][\"doc_id\"]: norm_scores[i] for i in range(len(lst))}\n",
    "        else:\n",
    "            m = {x[\"doc_id\"]: float(x[\"score\"]) for x in lst}\n",
    "        score_maps.append(m)\n",
    "\n",
    "    all_doc_ids = set().union(*[set(m.keys()) for m in score_maps])\n",
    "\n",
    "    fused = []\n",
    "    for d in all_doc_ids:\n",
    "        per_list_scores = [m.get(d, 0.0) for m in score_maps]\n",
    "        fused_score = sum(float(weights[i]) * per_list_scores[i] for i in range(len(per_list_scores)))\n",
    "        num_systems_present = sum(1 for s in per_list_scores if s > 0.0)\n",
    "        if combmnz:\n",
    "            fused_score *= max(1, num_systems_present)\n",
    "        fused.append({\n",
    "            \"doc_id\": d,\n",
    "            \"fused_score\": fused_score,\n",
    "            \"per_list_scores\": per_list_scores,\n",
    "            \"num_systems\": num_systems_present\n",
    "        })\n",
    "\n",
    "    fused.sort(key=lambda x: x[\"fused_score\"], reverse=True)\n",
    "    return fused\n",
    "\n",
    "\n",
    "# ---------- Metrics (text-based evaluation) ----------\n",
    "\n",
    "def apk(actual: List[str], predicted: List[str], k: int) -> float:\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "    pred_k = predicted[:k]\n",
    "    hits, score = 0, 0.0\n",
    "    seen = set()\n",
    "    for i, p in enumerate(pred_k, start=1):\n",
    "        if p in actual and p not in seen:\n",
    "            hits += 1\n",
    "            score += hits / i\n",
    "            seen.add(p)\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual_list: List[List[str]], predicted_list: List[List[str]], k: int) -> float:\n",
    "    scores = [apk(a, p, k) for a, p in zip(actual_list, predicted_list)]\n",
    "    return float(np.mean(scores)) if scores else 0.0\n",
    "\n",
    "def dcg_at_k(predicted: List[str], ideal_texts: List[str], k: int) -> float:\n",
    "    pred_k = predicted[:k]\n",
    "    dcg = 0.0\n",
    "    for i, p in enumerate(pred_k, start=1):\n",
    "        rel = 1.0 if p in ideal_texts else 0.0\n",
    "        if rel:\n",
    "            dcg += rel / np.log2(i + 1)\n",
    "    return dcg\n",
    "\n",
    "def idcg_at_k(ideal_texts: List[str], k: int) -> float:\n",
    "    g = min(len(ideal_texts), k)\n",
    "    return sum(1.0 / np.log2(i + 1) for i in range(1, g + 1))\n",
    "\n",
    "def ndcg_at_k(predicted: List[str], ideal_texts: List[str], k: int) -> float:\n",
    "    idcg = idcg_at_k(ideal_texts, k)\n",
    "    if idcg == 0.0:\n",
    "        return 0.0\n",
    "    return dcg_at_k(predicted, ideal_texts, k) / idcg\n",
    "\n",
    "\n",
    "# ---------- Main Pipeline ----------\n",
    "\n",
    "def main():\n",
    "    # Optional sanity check\n",
    "    print(\"CWD:\", os.getcwd())\n",
    "    print(\"Expecting QC model folder at:\", os.path.abspath(QC_MODEL_FOLDER))\n",
    "\n",
    "    # Load files\n",
    "    df_sparse = pd.read_csv(SPLADE_CSV)\n",
    "    df_dense = pd.read_csv(MPNET_CSV)\n",
    "\n",
    "    # Preserve originals for output\n",
    "    df_sparse[\"_q_orig\"] = df_sparse[\"question\"]\n",
    "    df_sparse[\"_a_orig\"] = df_sparse[\"answer\"]\n",
    "    df_sparse[\"_gt_orig\"] = df_sparse[\"groundtruth_docs\"]\n",
    "\n",
    "    df_dense[\"_q_orig\"] = df_dense[\"question\"]\n",
    "    df_dense[\"_a_orig\"] = df_dense[\"answer\"]\n",
    "    df_dense[\"_gt_orig\"] = df_dense[\"groundtruth_docs\"]\n",
    "\n",
    "    # Normalized join keys\n",
    "    df_sparse[\"_q_norm\"] = df_sparse[\"question\"].apply(normalize_text)\n",
    "    df_sparse[\"_a_norm\"] = df_sparse[\"answer\"].apply(normalize_text)\n",
    "    df_sparse[\"_gt_norm\"] = df_sparse[\"groundtruth_docs\"].apply(normalize_groundtruth_str)\n",
    "\n",
    "    df_dense[\"_q_norm\"] = df_dense[\"question\"].apply(normalize_text)\n",
    "    df_dense[\"_a_norm\"] = df_dense[\"answer\"].apply(normalize_text)\n",
    "    df_dense[\"_gt_norm\"] = df_dense[\"groundtruth_docs\"].apply(normalize_groundtruth_str)\n",
    "\n",
    "    # Merge on normalized keys\n",
    "    df = pd.merge(\n",
    "        df_sparse,\n",
    "        df_dense,\n",
    "        on=[\"_q_norm\", \"_a_norm\", \"_gt_norm\"],\n",
    "        suffixes=(\"_splade\", \"_mpnet\"),\n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "    if df.empty:\n",
    "        raise ValueError(\"No rows matched after normalization. Check groundtruth formats across CSVs.\")\n",
    "\n",
    "    hybrid_ret_docs_col = []\n",
    "    map3_list, ndcg3_list = [], []\n",
    "    map5_list, ndcg5_list = [], []\n",
    "    map10_list, ndcg10_list = [], []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        # Parse lists\n",
    "        splade_list = parse_ret_list(row[\"splade_ret_docs\"])[:TOP_K_PER_LIST]\n",
    "        mpnet_list  = parse_ret_list(row[\"mpnet_ret_docs\"])[:TOP_K_PER_LIST]\n",
    "\n",
    "        # Build full_text lookup for ID->full_text\n",
    "        fulltext_map = {}\n",
    "        for d in splade_list:\n",
    "            fulltext_map.setdefault(d[\"doc_id\"], d.get(\"full_text\", \"\"))\n",
    "        for d in mpnet_list:\n",
    "            fulltext_map.setdefault(d[\"doc_id\"], d.get(\"full_text\", \"\"))\n",
    "\n",
    "        # QC weights for this query (only needed for weighted modes)\n",
    "        question_text = row[\"_q_orig_splade\"]\n",
    "        if FUSION_MODE in (\"weighted_rrf\", \"weighted_score_sum\", \"weighted_score_sum_combmnz\"):\n",
    "            w_sparse, w_dense = get_qc_weights(question_text, temperature=WEIGHT_TEMPERATURE)\n",
    "\n",
    "            # 1) optional smoothing toward 0.5\n",
    "            if SMOOTH_TOWARD_HALF:\n",
    "                w_sparse, w_dense = smooth_toward_half(w_sparse, w_dense, SMOOTH_ALPHA)\n",
    "\n",
    "            # 2) optional clamping to guarantee minimum mix of both retrievers\n",
    "            if CLAMP_WEIGHTS:\n",
    "                w_sparse, w_dense = clamp_pair(w_sparse, w_dense, WEIGHT_FLOOR, WEIGHT_CEIL)\n",
    "\n",
    "            # final sanity renormalization\n",
    "            s = w_sparse + w_dense\n",
    "            if s <= 0:\n",
    "                w_sparse, w_dense = 0.5, 0.5\n",
    "            else:\n",
    "                w_sparse, w_dense = w_sparse / s, w_dense / s\n",
    "\n",
    "            weights = [float(w_sparse), float(w_dense)]\n",
    "        else:\n",
    "            w_sparse, w_dense = None, None\n",
    "            weights = None\n",
    "\n",
    "        # Fusion\n",
    "        # \"base\" is the original plain RRF (same as \"rrf\")\n",
    "        mode = FUSION_MODE\n",
    "        if mode == \"base\" or mode == \"rrf\":\n",
    "            fused = rrf_fuse_detailed([splade_list, mpnet_list], k=RRF_K, normalize_scores=NORMALIZE_SCORES)\n",
    "            fusion_label = \"base\"\n",
    "            for f in fused:\n",
    "                f[\"_fused_score\"] = f[\"rrf_score\"]\n",
    "            # cap final list here\n",
    "            fused = fused[:FINAL_TOP_K]\n",
    "        elif mode == \"weighted_rrf\":\n",
    "            fused = rrf_fuse_weighted([splade_list, mpnet_list], weights=weights, k=RRF_K, normalize_scores=NORMALIZE_SCORES)\n",
    "            fusion_label = \"weighted_rrf\"\n",
    "            for f in fused:\n",
    "                f[\"_fused_score\"] = f[\"rrf_score\"]\n",
    "            # cap final list here\n",
    "            fused = fused[:FINAL_TOP_K]\n",
    "        elif mode == \"weighted_score_sum\":\n",
    "            fused_score = score_sum_fuse_weighted([splade_list, mpnet_list], weights=weights, normalize_per_list=SCORE_NORM_PER_LIST, combmnz=False)\n",
    "            rank_maps = [rank_from_scores(splade_list, normalize=NORMALIZE_SCORES),\n",
    "                         rank_from_scores(mpnet_list, normalize=NORMALIZE_SCORES)]\n",
    "            fused = []\n",
    "            for item in fused_score:\n",
    "                d = item[\"doc_id\"]\n",
    "                fused.append({\n",
    "                    \"doc_id\": d,\n",
    "                    \"_fused_score\": item[\"fused_score\"],\n",
    "                    \"ranks\": [rank_maps[0].get(d), rank_maps[1].get(d)]\n",
    "                })\n",
    "            fused.sort(key=lambda x: x[\"_fused_score\"], reverse=True)\n",
    "            fusion_label = \"weighted_score_sum\"\n",
    "            # cap final list here\n",
    "            fused = fused[:FINAL_TOP_K]\n",
    "        elif mode == \"weighted_score_sum_combmnz\":\n",
    "            fused_score = score_sum_fuse_weighted([splade_list, mpnet_list], weights=weights, normalize_per_list=SCORE_NORM_PER_LIST, combmnz=True)\n",
    "            rank_maps = [rank_from_scores(splade_list, normalize=NORMALIZE_SCORES),\n",
    "                         rank_from_scores(mpnet_list, normalize=NORMALIZE_SCORES)]\n",
    "            fused = []\n",
    "            for item in fused_score:\n",
    "                d = item[\"doc_id\"]\n",
    "                fused.append({\n",
    "                    \"doc_id\": d,\n",
    "                    \"_fused_score\": item[\"fused_score\"],\n",
    "                    \"ranks\": [rank_maps[0].get(d), rank_maps[1].get(d)]\n",
    "                })\n",
    "            fused.sort(key=lambda x: x[\"_fused_score\"], reverse=True)\n",
    "            fusion_label = \"weighted_score_sum_combmnz\"\n",
    "            # cap final list here\n",
    "            fused = fused[:FINAL_TOP_K]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown FUSION_MODE: {FUSION_MODE}\")\n",
    "\n",
    "        # Build hybrid_ret_docs with fused score, rank, full_text, per-model rank, and weights\n",
    "        hybrid_struct = []\n",
    "        fused_norm_fulltexts = []  # for evaluation by text\n",
    "        for idx, item in enumerate(fused, start=1):\n",
    "            doc_id = item[\"doc_id\"]\n",
    "            fused_score = item[\"_fused_score\"]\n",
    "            ranks = item.get(\"ranks\")\n",
    "            if ranks is None:\n",
    "                ranks = [None, None]\n",
    "            full_text = fulltext_map.get(doc_id, \"\")\n",
    "\n",
    "            entry = {\n",
    "                \"doc_id\": doc_id,\n",
    "                \"score\": fused_score,\n",
    "                \"rank\": idx,\n",
    "                \"full_text\": full_text,\n",
    "                \"source_ranks\": {\n",
    "                    \"splade\": ranks[0],\n",
    "                    \"mpnet\": ranks[1],\n",
    "                }\n",
    "            }\n",
    "            if LOG_WEIGHTS:\n",
    "                if weights is not None:\n",
    "                    entry[\"qc_weights\"] = {\"splade\": weights[0], \"mpnet\": weights[1]}\n",
    "                else:\n",
    "                    entry[\"qc_weights\"] = None\n",
    "                entry[\"fusion_mode\"] = fusion_label\n",
    "            hybrid_struct.append(entry)\n",
    "\n",
    "            # Collect normalized full text for evaluation by text\n",
    "            fused_norm_fulltexts.append(normalize_text(full_text))\n",
    "\n",
    "        # Save hybrid struct JSON\n",
    "        hybrid_ret_docs_col.append(json.dumps(hybrid_struct, ensure_ascii=False))\n",
    "\n",
    "        # Prepare normalized GT texts (maintain order for NDCG ideal)\n",
    "        gt_norm_texts = [normalize_text(x) for x in parse_groundtruth_list(row[\"_gt_orig_splade\"])]\n",
    "\n",
    "        # Metrics by text\n",
    "        pred = fused_norm_fulltexts\n",
    "        map3_list.append(apk(gt_norm_texts, pred, 3))\n",
    "        map5_list.append(apk(gt_norm_texts, pred, 5))\n",
    "        map10_list.append(apk(gt_norm_texts, pred, 10))\n",
    "        ndcg3_list.append(ndcg_at_k(pred, gt_norm_texts, 3))\n",
    "        ndcg5_list.append(ndcg_at_k(pred, gt_norm_texts, 5))\n",
    "        ndcg10_list.append(ndcg_at_k(pred, gt_norm_texts, 10))\n",
    "\n",
    "    # Build output with requested columns (use original SPLADE-side text columns)\n",
    "    out = pd.DataFrame({\n",
    "        \"question\": df[\"_q_orig_splade\"],\n",
    "        \"answer\": df[\"_a_orig_splade\"],\n",
    "        \"groundtruth_docs\": df[\"_gt_orig_splade\"],\n",
    "        \"splade_ret_docs\": df[\"splade_ret_docs\"],\n",
    "        \"mpnet_ret_docs\": df[\"mpnet_ret_docs\"],\n",
    "        \"hybrid_ret_docs\": hybrid_ret_docs_col,\n",
    "        \"MAP@3\": map3_list,\n",
    "        \"NDCG@3\": ndcg3_list,\n",
    "        \"MAP@5\": map5_list,\n",
    "        \"NDCG@5\": ndcg5_list,\n",
    "        \"MAP@10\": map10_list,\n",
    "        \"NDCG@10\": ndcg10_list,\n",
    "    })\n",
    "\n",
    "    cols = [\n",
    "        \"question\", \"answer\", \"groundtruth_docs\",\n",
    "        \"splade_ret_docs\", \"mpnet_ret_docs\", \"hybrid_ret_docs\",\n",
    "        \"MAP@3\", \"NDCG@3\", \"MAP@5\", \"NDCG@5\", \"MAP@10\", \"NDCG@10\"\n",
    "    ]\n",
    "    out = out[cols]\n",
    "\n",
    "    out.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    overall = {\n",
    "        \"MAP@3\": float(np.mean(map3_list)) if map3_list else 0.0,\n",
    "        \"NDCG@3\": float(np.mean(ndcg3_list)) if ndcg3_list else 0.0,\n",
    "        \"MAP@5\": float(np.mean(map5_list)) if map5_list else 0.0,\n",
    "        \"NDCG@5\": float(np.mean(ndcg5_list)) if ndcg5_list else 0.0,\n",
    "        \"MAP@10\": float(np.mean(map10_list)) if map10_list else 0.0,\n",
    "        \"NDCG@10\": float(np.mean(ndcg10_list)) if ndcg10_list else 0.0,\n",
    "    }\n",
    "    print(\"Saved:\", OUTPUT_CSV)\n",
    "    print(\"Fusion mode:\", FUSION_MODE)\n",
    "    print(\"QC temperature:\", WEIGHT_TEMPERATURE)\n",
    "    print(\"Hybrid overall averages:\", overall)\n",
    "\n",
    "\n",
    "# If running in a notebook cell, call main() explicitly:\n",
    "main() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saved: ./hybrid_weighted_score_sum_combmnz.csv\n",
    "Fusion mode: weighted_score_sum_combmnz\n",
    "QC temperature: 1.17\n",
    "Hybrid overall averages: {'MAP@3': 0.40784726645032804, 'NDCG@3': 0.4839240380520279, 'MAP@5': 0.4282661318856842, 'NDCG@5': 0.5155112511570251, 'MAP@10': 0.4536183281168094, 'NDCG@10': 0.5602454339260866}\n",
    "\n",
    "\n",
    "Saved: ./hybrid_weighted_rrf.csv\n",
    "Fusion mode: weighted_rrf\n",
    "QC temperature: 1.17\n",
    "Hybrid overall averages: {'MAP@3': 0.40284538359668015, 'NDCG@3': 0.4782867077931083, 'MAP@5': 0.41393834538865126, 'NDCG@5': 0.4954578325048874, 'MAP@10': 0.42103152301077024, 'NDCG@10': 0.5075874708412451}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: /home/csmala/journal_rag/hybrid_pipeline\n",
      "Saved: ./hybrid_linear_interpolation.csv\n",
      "Fusion mode: weighted_score_sum_combmnz\n",
      "Global weights (λ, 1-λ): [0.7, 0.3]\n",
      "TOP_K_PER_LIST: 20 FINAL_TOP_K: 10\n",
      "Hybrid overall averages: {'MAP@3': 0.48431292269432563, 'NDCG@3': 0.5631704059232733, 'MAP@5': 0.5018147202509123, 'NDCG@5': 0.5883772764828926, 'MAP@10': 0.5156350223348495, 'NDCG@10': 0.6111674154268684}\n"
     ]
    }
   ],
   "source": [
    "# 2. try linear interpolation with global weights\n",
    " \n",
    "import json\n",
    "import ast\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "# ============ CONFIG (paths relative to the notebook's folder) ============\n",
    "# Make sure your current working directory is the \"hybrid_retrieval\" folder (where the notebook lives).\n",
    "SPLADE_CSV = \"./prior_results/hotpotqa_splade.csv\"\n",
    "MPNET_CSV  = \"./prior_results/hotpotqa_mpnet.csv\"\n",
    "OUTPUT_CSV = \"./hybrid_linear_interpolation.csv\"\n",
    "\n",
    "# Fusion mode: use score-based linear interpolation\n",
    "# \"weighted_score_sum\" = λ·S_splade + (1−λ)·S_mpnet (after per-list min-max normalization)\n",
    "# \"weighted_score_sum_combmnz\" = same as above, then multiply by number of lists containing the doc\n",
    "FUSION_MODE = \"weighted_score_sum_combmnz\"  # \"weighted_score_sum\" | \"weighted_score_sum_combmnz\"\n",
    "\n",
    "# Global weights (no QC). Set λ = GLOBAL_WEIGHTS[0], (1-λ) = GLOBAL_WEIGHTS[1]\n",
    "USE_GLOBAL_WEIGHTS = True\n",
    "GLOBAL_WEIGHTS = (0.7, 0.3)  # bias toward SPLADE which seems stronger on your dataset\n",
    "\n",
    "# Per-list score normalization for score-sum fusions\n",
    "SCORE_NORM_PER_LIST = True  # keep True for linear interpolation\n",
    "\n",
    "# Input/output caps\n",
    "TOP_K_PER_LIST = 20   # take top-N from each retriever before fusion\n",
    "FINAL_TOP_K = 10      # keep only top-10 after fusion for storage/eval\n",
    "\n",
    "# Logging options\n",
    "LOG_WEIGHTS = True  # include weights & fusion meta in the hybrid output entries\n",
    "\n",
    "# RRF options (not used here, but leaving for completeness if you switch modes)\n",
    "RRF_K = 90\n",
    "NORMALIZE_SCORES = False\n",
    "# ========================================================================\n",
    "\n",
    "\n",
    "# ---------- Utilities ----------\n",
    "\n",
    "def normalize_text(s: Any) -> str:\n",
    "    if s is None or (isinstance(s, float) and np.isnan(s)):\n",
    "        return \"\"\n",
    "    s = str(s)\n",
    "    s = s.strip().lower()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def safe_parse_list(val: Any) -> Any:\n",
    "    if isinstance(val, (list, dict)):\n",
    "        return val\n",
    "    if val is None or (isinstance(val, float) and np.isnan(val)):\n",
    "        return []\n",
    "    s = str(val).strip()\n",
    "    if s == \"\":\n",
    "        return []\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return ast.literal_eval(s)\n",
    "        except Exception:\n",
    "            return []\n",
    "\n",
    "def normalize_groundtruth_str(gt_field: Any) -> str:\n",
    "    data = safe_parse_list(gt_field)\n",
    "    if isinstance(data, list):\n",
    "        norm_items = [normalize_text(x) for x in data]\n",
    "        return json.dumps(norm_items, ensure_ascii=False)\n",
    "    return json.dumps([normalize_text(str(gt_field))], ensure_ascii=False)\n",
    "\n",
    "def parse_ret_list(ret_field: Any) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Parse retrieval list into a list of dicts: {doc_id, score, full_text}\n",
    "    \"\"\"\n",
    "    data = safe_parse_list(ret_field)\n",
    "    out = []\n",
    "    if isinstance(data, list):\n",
    "        for d in data:\n",
    "            if isinstance(d, dict) and \"doc_id\" in d:\n",
    "                doc_id = str(d.get(\"doc_id\"))\n",
    "                score = float(d.get(\"score\", 0.0))\n",
    "                full_text = d.get(\"full_text\", \"\")\n",
    "                out.append({\"doc_id\": doc_id, \"score\": score, \"full_text\": full_text})\n",
    "    return out\n",
    "\n",
    "def parse_groundtruth_list(gt_field: Any) -> List[str]:\n",
    "    data = safe_parse_list(gt_field)\n",
    "    if isinstance(data, list):\n",
    "        return [str(x) for x in data]\n",
    "    s = str(gt_field).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    if \",\" in s:\n",
    "        return [t.strip() for t in s.split(\",\") if t.strip()]\n",
    "    return s.split()\n",
    "\n",
    "def min_max_normalize(scores: List[float]) -> List[float]:\n",
    "    if not scores:\n",
    "        return []\n",
    "    mn, mx = min(scores), max(scores)\n",
    "    if mx == mn:\n",
    "        return [0.0 for _ in scores]\n",
    "    return [(x - mn) / (mx - mn) for x in scores]\n",
    "\n",
    "\n",
    "# ---------- Score-based Fusion (Linear Interpolation) ----------\n",
    "\n",
    "def score_sum_fuse_weighted(lists: List[List[Dict[str, Any]]], weights: List[float], normalize_per_list: bool = True, combmnz: bool = False):\n",
    "    \"\"\"\n",
    "    Weighted linear score fusion:\n",
    "      fused_score(d) = sum_i w_i * s_i(d)\n",
    "    where s_i(d) are per-list scores, optionally min-max normalized within each list.\n",
    "    COMBMNZ variant multiplies by the count of lists where the doc appears:\n",
    "      fused_mnz(d) = fused_score(d) * (#systems that retrieved d)\n",
    "    \"\"\"\n",
    "    # Build doc_id -> score per list\n",
    "    score_maps = []\n",
    "    for lst in lists:\n",
    "        if normalize_per_list:\n",
    "            norm_scores = min_max_normalize([x[\"score\"] for x in lst])\n",
    "            m = {lst[i][\"doc_id\"]: norm_scores[i] for i in range(len(lst))}\n",
    "        else:\n",
    "            m = {x[\"doc_id\"]: float(x[\"score\"]) for x in lst}\n",
    "        score_maps.append(m)\n",
    "\n",
    "    all_doc_ids = set().union(*[set(m.keys()) for m in score_maps])\n",
    "\n",
    "    fused = []\n",
    "    for d in all_doc_ids:\n",
    "        per_list_scores = [m.get(d, 0.0) for m in score_maps]\n",
    "        fused_score = sum(float(weights[i]) * per_list_scores[i] for i in range(len(per_list_scores)))\n",
    "        num_systems_present = sum(1 for s in per_list_scores if s > 0.0)\n",
    "        if combmnz:\n",
    "            fused_score *= max(1, num_systems_present)\n",
    "        fused.append({\n",
    "            \"doc_id\": d,\n",
    "            \"fused_score\": fused_score,\n",
    "            \"per_list_scores\": per_list_scores,\n",
    "            \"num_systems\": num_systems_present\n",
    "        })\n",
    "\n",
    "    fused.sort(key=lambda x: x[\"fused_score\"], reverse=True)\n",
    "    return fused\n",
    "\n",
    "\n",
    "# ---------- Metrics (text-based evaluation) ----------\n",
    "\n",
    "def apk(actual: List[str], predicted: List[str], k: int) -> float:\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "    pred_k = predicted[:k]\n",
    "    hits, score = 0, 0.0\n",
    "    seen = set()\n",
    "    for i, p in enumerate(pred_k, start=1):\n",
    "        if p in actual and p not in seen:\n",
    "            hits += 1\n",
    "            score += hits / i\n",
    "            seen.add(p)\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual_list: List[List[str]], predicted_list: List[List[str]], k: int) -> float:\n",
    "    scores = [apk(a, p, k) for a, p in zip(actual_list, predicted_list)]\n",
    "    return float(np.mean(scores)) if scores else 0.0\n",
    "\n",
    "def dcg_at_k(predicted: List[str], ideal_texts: List[str], k: int) -> float:\n",
    "    pred_k = predicted[:k]\n",
    "    dcg = 0.0\n",
    "    for i, p in enumerate(pred_k, start=1):\n",
    "        rel = 1.0 if p in ideal_texts else 0.0\n",
    "        if rel:\n",
    "            dcg += rel / np.log2(i + 1)\n",
    "    return dcg\n",
    "\n",
    "def idcg_at_k(ideal_texts: List[str], k: int) -> float:\n",
    "    g = min(len(ideal_texts), k)\n",
    "    return sum(1.0 / np.log2(i + 1) for i in range(1, g + 1))\n",
    "\n",
    "def ndcg_at_k(predicted: List[str], ideal_texts: List[str], k: int) -> float:\n",
    "    idcg = idcg_at_k(ideal_texts, k)\n",
    "    if idcg == 0.0:\n",
    "        return 0.0\n",
    "    return dcg_at_k(predicted, ideal_texts, k) / idcg\n",
    "\n",
    "\n",
    "# ---------- Main Pipeline ----------\n",
    "\n",
    "def main():\n",
    "    # Optional sanity check\n",
    "    print(\"CWD:\", os.getcwd())\n",
    "\n",
    "    # Load files\n",
    "    df_sparse = pd.read_csv(SPLADE_CSV)\n",
    "    df_dense = pd.read_csv(MPNET_CSV)\n",
    "\n",
    "    # Preserve originals for output\n",
    "    df_sparse[\"_q_orig\"] = df_sparse[\"question\"]\n",
    "    df_sparse[\"_a_orig\"] = df_sparse[\"answer\"]\n",
    "    df_sparse[\"_gt_orig\"] = df_sparse[\"groundtruth_docs\"]\n",
    "\n",
    "    df_dense[\"_q_orig\"] = df_dense[\"question\"]\n",
    "    df_dense[\"_a_orig\"] = df_dense[\"answer\"]\n",
    "    df_dense[\"_gt_orig\"] = df_dense[\"groundtruth_docs\"]\n",
    "\n",
    "    # Normalized join keys\n",
    "    df_sparse[\"_q_norm\"] = df_sparse[\"question\"].apply(normalize_text)\n",
    "    df_sparse[\"_a_norm\"] = df_sparse[\"answer\"].apply(normalize_text)\n",
    "    df_sparse[\"_gt_norm\"] = df_sparse[\"groundtruth_docs\"].apply(normalize_groundtruth_str)\n",
    "\n",
    "    df_dense[\"_q_norm\"] = df_dense[\"question\"].apply(normalize_text)\n",
    "    df_dense[\"_a_norm\"] = df_dense[\"answer\"].apply(normalize_text)\n",
    "    df_dense[\"_gt_norm\"] = df_dense[\"groundtruth_docs\"].apply(normalize_groundtruth_str)\n",
    "\n",
    "    # Merge on normalized keys\n",
    "    df = pd.merge(\n",
    "        df_sparse,\n",
    "        df_dense,\n",
    "        on=[\"_q_norm\", \"_a_norm\", \"_gt_norm\"],\n",
    "        suffixes=(\"_splade\", \"_mpnet\"),\n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "    if df.empty:\n",
    "        raise ValueError(\"No rows matched after normalization. Check groundtruth formats across CSVs.\")\n",
    "\n",
    "    hybrid_ret_docs_col = []\n",
    "    map3_list, ndcg3_list = [], []\n",
    "    map5_list, ndcg5_list = [], []\n",
    "    map10_list, ndcg10_list = [], []\n",
    "\n",
    "    # Global linear interpolation weights (no QC)\n",
    "    if not USE_GLOBAL_WEIGHTS:\n",
    "        raise ValueError(\"This script expects USE_GLOBAL_WEIGHTS=True. Set it at the top.\")\n",
    "    w_sparse, w_dense = GLOBAL_WEIGHTS\n",
    "    s = w_sparse + w_dense\n",
    "    if s <= 0:\n",
    "        w_sparse, w_dense = 0.5, 0.5\n",
    "    else:\n",
    "        w_sparse, w_dense = float(w_sparse)/s, float(w_dense)/s\n",
    "    weights = [w_sparse, w_dense]\n",
    "    fusion_label = FUSION_MODE\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        # Parse and cap lists\n",
    "        splade_list = parse_ret_list(row[\"splade_ret_docs\"])[:TOP_K_PER_LIST]\n",
    "        mpnet_list  = parse_ret_list(row[\"mpnet_ret_docs\"])[:TOP_K_PER_LIST]\n",
    "        gt_list_raw = parse_groundtruth_list(row[\"_gt_orig_splade\"])  # use either side; same after join\n",
    "\n",
    "        # Build full_text lookup for ID->full_text\n",
    "        fulltext_map = {}\n",
    "        for d in splade_list:\n",
    "            fulltext_map.setdefault(d[\"doc_id\"], d.get(\"full_text\", \"\"))\n",
    "        for d in mpnet_list:\n",
    "            fulltext_map.setdefault(d[\"doc_id\"], d.get(\"full_text\", \"\"))\n",
    "\n",
    "        # Score-based fusion with global weights\n",
    "        if FUSION_MODE == \"weighted_score_sum\":\n",
    "            fused_score = score_sum_fuse_weighted(\n",
    "                [splade_list, mpnet_list],\n",
    "                weights=weights,\n",
    "                normalize_per_list=SCORE_NORM_PER_LIST,\n",
    "                combmnz=False\n",
    "            )\n",
    "            # Convert to fused entries with ranks\n",
    "            # Create rank maps for reference (optional)\n",
    "            # Not needed for sorting, only for logging source ranks\n",
    "            # We'll compute ranks from the original lists' scores\n",
    "            # Build rank maps (descending by score)\n",
    "            def rank_from_scores(items):\n",
    "                arr_sorted = sorted(items, key=lambda x: x.get(\"score\", 0.0), reverse=True)\n",
    "                return {it[\"doc_id\"]: idx for idx, it in enumerate(arr_sorted, start=1)}\n",
    "            rank_maps = [rank_from_scores(splade_list), rank_from_scores(mpnet_list)]\n",
    "\n",
    "            fused = []\n",
    "            for item in fused_score:\n",
    "                d = item[\"doc_id\"]\n",
    "                fused.append({\n",
    "                    \"doc_id\": d,\n",
    "                    \"_fused_score\": item[\"fused_score\"],\n",
    "                    \"ranks\": [rank_maps[0].get(d), rank_maps[1].get(d)]\n",
    "                })\n",
    "            fused.sort(key=lambda x: x[\"_fused_score\"], reverse=True)\n",
    "\n",
    "        elif FUSION_MODE == \"weighted_score_sum_combmnz\":\n",
    "            fused_score = score_sum_fuse_weighted(\n",
    "                [splade_list, mpnet_list],\n",
    "                weights=weights,\n",
    "                normalize_per_list=SCORE_NORM_PER_LIST,\n",
    "                combmnz=True\n",
    "            )\n",
    "            def rank_from_scores(items):\n",
    "                arr_sorted = sorted(items, key=lambda x: x.get(\"score\", 0.0), reverse=True)\n",
    "                return {it[\"doc_id\"]: idx for idx, it in enumerate(arr_sorted, start=1)}\n",
    "            rank_maps = [rank_from_scores(splade_list), rank_from_scores(mpnet_list)]\n",
    "\n",
    "            fused = []\n",
    "            for item in fused_score:\n",
    "                d = item[\"doc_id\"]\n",
    "                fused.append({\n",
    "                    \"doc_id\": d,\n",
    "                    \"_fused_score\": item[\"fused_score\"],\n",
    "                    \"ranks\": [rank_maps[0].get(d), rank_maps[1].get(d)]\n",
    "                })\n",
    "            fused.sort(key=lambda x: x[\"_fused_score\"], reverse=True)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported FUSION_MODE for this script: {FUSION_MODE}\")\n",
    "\n",
    "        # Cap final fused to top-K\n",
    "        fused = fused[:FINAL_TOP_K]\n",
    "\n",
    "        # Build hybrid_ret_docs with fused score, rank, full_text, per-model rank, and weights\n",
    "        hybrid_struct = []\n",
    "        fused_norm_fulltexts = []  # for evaluation by text\n",
    "        for idx, item in enumerate(fused, start=1):\n",
    "            doc_id = item[\"doc_id\"]\n",
    "            fused_score_val = item[\"_fused_score\"]\n",
    "            ranks = item.get(\"ranks\")\n",
    "            if ranks is None:\n",
    "                ranks = [None, None]\n",
    "            full_text = fulltext_map.get(doc_id, \"\")\n",
    "\n",
    "            entry = {\n",
    "                \"doc_id\": doc_id,\n",
    "                \"score\": fused_score_val,\n",
    "                \"rank\": idx,\n",
    "                \"full_text\": full_text,\n",
    "                \"source_ranks\": {\n",
    "                    \"splade\": ranks[0],\n",
    "                    \"mpnet\": ranks[1],\n",
    "                }\n",
    "            }\n",
    "            if LOG_WEIGHTS:\n",
    "                entry[\"qc_weights\"] = {\"splade\": weights[0], \"mpnet\": weights[1]}\n",
    "                entry[\"fusion_mode\"] = fusion_label\n",
    "            hybrid_struct.append(entry)\n",
    "\n",
    "            fused_norm_fulltexts.append(normalize_text(full_text))\n",
    "\n",
    "        # Save hybrid struct JSON\n",
    "        hybrid_ret_docs_col.append(json.dumps(hybrid_struct, ensure_ascii=False))\n",
    "\n",
    "        # Prepare normalized GT texts (maintain order for NDCG ideal)\n",
    "        gt_norm_texts = [normalize_text(x) for x in gt_list_raw]\n",
    "\n",
    "        # Metrics by text\n",
    "        pred = fused_norm_fulltexts\n",
    "        map3_list.append(apk(gt_norm_texts, pred, 3))\n",
    "        map5_list.append(apk(gt_norm_texts, pred, 5))\n",
    "        map10_list.append(apk(gt_norm_texts, pred, 10))\n",
    "        ndcg3_list.append(ndcg_at_k(pred, gt_norm_texts, 3))\n",
    "        ndcg5_list.append(ndcg_at_k(pred, gt_norm_texts, 5))\n",
    "        ndcg10_list.append(ndcg_at_k(pred, gt_norm_texts, 10))\n",
    "\n",
    "    # Build output with requested columns (use original SPLADE-side text columns)\n",
    "    out = pd.DataFrame({\n",
    "        \"question\": df[\"_q_orig_splade\"],\n",
    "        \"answer\": df[\"_a_orig_splade\"],\n",
    "        \"groundtruth_docs\": df[\"_gt_orig_splade\"],\n",
    "        \"splade_ret_docs\": df[\"splade_ret_docs\"],\n",
    "        \"mpnet_ret_docs\": df[\"mpnet_ret_docs\"],\n",
    "        \"hybrid_ret_docs\": hybrid_ret_docs_col,\n",
    "        \"MAP@3\": map3_list,\n",
    "        \"NDCG@3\": ndcg3_list,\n",
    "        \"MAP@5\": map5_list,\n",
    "        \"NDCG@5\": ndcg5_list,\n",
    "        \"MAP@10\": map10_list,\n",
    "        \"NDCG@10\": ndcg10_list,\n",
    "    })\n",
    "\n",
    "    cols = [\n",
    "        \"question\", \"answer\", \"groundtruth_docs\",\n",
    "        \"splade_ret_docs\", \"mpnet_ret_docs\", \"hybrid_ret_docs\",\n",
    "        \"MAP@3\", \"NDCG@3\", \"MAP@5\", \"NDCG@5\", \"MAP@10\", \"NDCG@10\"\n",
    "    ]\n",
    "    out = out[cols]\n",
    "\n",
    "    out.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    overall = {\n",
    "        \"MAP@3\": float(np.mean(map3_list)) if map3_list else 0.0,\n",
    "        \"NDCG@3\": float(np.mean(ndcg3_list)) if ndcg3_list else 0.0,\n",
    "        \"MAP@5\": float(np.mean(map5_list)) if map5_list else 0.0,\n",
    "        \"NDCG@5\": float(np.mean(ndcg5_list)) if ndcg5_list else 0.0,\n",
    "        \"MAP@10\": float(np.mean(map10_list)) if map10_list else 0.0,\n",
    "        \"NDCG@10\": float(np.mean(ndcg10_list)) if ndcg10_list else 0.0,\n",
    "    }\n",
    "    print(\"Saved:\", OUTPUT_CSV)\n",
    "    print(\"Fusion mode:\", FUSION_MODE)\n",
    "    print(\"Global weights (λ, 1-λ):\", weights)\n",
    "    print(\"TOP_K_PER_LIST:\", TOP_K_PER_LIST, \"FINAL_TOP_K:\", FINAL_TOP_K)\n",
    "    print(\"Hybrid overall averages:\", overall)\n",
    "\n",
    "\n",
    "# Run\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: /home/csmala/journal_rag/hybrid_pipeline\n",
      "Saved: ./hybrid_crossencoder_rerank.csv\n",
      "Fusion mode: ce_rerank\n",
      "TOP_K_PER_LIST: 20 FINAL_TOP_K: 10\n",
      "Cross-Encoder: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "Hybrid overall averages: {'MAP@3': 0.5191621427632709, 'NDCG@3': 0.5946335494517143, 'MAP@5': 0.5326992033886311, 'NDCG@5': 0.6147403898582319, 'MAP@10': 0.5451409088593357, 'NDCG@10': 0.6352804956790745}\n"
     ]
    }
   ],
   "source": [
    "# 3. implement the cross encoders re-ranker on the fused shortlist?\n",
    "\n",
    "import json\n",
    "import ast\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "# ============ CONFIG ============\n",
    "# Paths relative to the notebook's folder\n",
    "SPLADE_CSV = \"./prior_results/hotpotqa_splade.csv\"\n",
    "MPNET_CSV  = \"./prior_results/hotpotqa_mpnet.csv\"\n",
    "OUTPUT_CSV = \"./hybrid_crossencoder_rerank.csv\"\n",
    "\n",
    "# Candidate generation\n",
    "TOP_K_PER_LIST = 20   # how many from each retriever to consider before re-ranking\n",
    "FINAL_TOP_K = 10      # final top-N after re-ranking to store/evaluate\n",
    "\n",
    "# Cross-encoder model\n",
    "# Option A: HF Hub model name\n",
    "CROSS_ENCODER_MODEL = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "# Option B: Local folder if you have it downloaded (set CE_LOCAL_ONLY=True)\n",
    "CROSS_ENCODER_MODEL_PATH = \"./cross_encoder_ms_marco_minilm_l6_v2\"  # change if needed\n",
    "CE_LOCAL_ONLY = False   # True to force loading from local path/folder only\n",
    "\n",
    "# Inference\n",
    "CE_BATCH_SIZE = 32      # adjust based on CPU/GPU\n",
    "CE_DEVICE = None        # None lets transformers pick automatically; or set \"cuda\" / \"cpu\"\n",
    "\n",
    "# Logging\n",
    "LOG_WEIGHTS = True      # keep for consistency (not used here but we log fusion_mode)\n",
    "FUSION_LABEL = \"ce_rerank\"\n",
    "# =================================\n",
    "\n",
    "\n",
    "# ---------- Utilities ----------\n",
    "\n",
    "def normalize_text(s: Any) -> str:\n",
    "    if s is None or (isinstance(s, float) and np.isnan(s)):\n",
    "        return \"\"\n",
    "    s = str(s)\n",
    "    s = s.strip().lower()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def safe_parse_list(val: Any) -> Any:\n",
    "    if isinstance(val, (list, dict)):\n",
    "        return val\n",
    "    if val is None or (isinstance(val, float) and np.isnan(val)):\n",
    "        return []\n",
    "    s = str(val).strip()\n",
    "    if s == \"\":\n",
    "        return []\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return ast.literal_eval(s)\n",
    "        except Exception:\n",
    "            return []\n",
    "\n",
    "def normalize_groundtruth_str(gt_field: Any) -> str:\n",
    "    data = safe_parse_list(gt_field)\n",
    "    if isinstance(data, list):\n",
    "        norm_items = [normalize_text(x) for x in data]\n",
    "        return json.dumps(norm_items, ensure_ascii=False)\n",
    "    return json.dumps([normalize_text(str(gt_field))], ensure_ascii=False)\n",
    "\n",
    "def parse_ret_list(ret_field: Any) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Parse retrieval list into a list of dicts: {doc_id, score, full_text}\n",
    "    \"\"\"\n",
    "    data = safe_parse_list(ret_field)\n",
    "    out = []\n",
    "    if isinstance(data, list):\n",
    "        for d in data:\n",
    "            if isinstance(d, dict) and \"doc_id\" in d:\n",
    "                doc_id = str(d.get(\"doc_id\"))\n",
    "                score = float(d.get(\"score\", 0.0))\n",
    "                full_text = d.get(\"full_text\", \"\")\n",
    "                out.append({\"doc_id\": doc_id, \"score\": score, \"full_text\": full_text})\n",
    "    return out\n",
    "\n",
    "def parse_groundtruth_list(gt_field: Any) -> List[str]:\n",
    "    data = safe_parse_list(gt_field)\n",
    "    if isinstance(data, list):\n",
    "        return [str(x) for x in data]\n",
    "    s = str(gt_field).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    if \",\" in s:\n",
    "        return [t.strip() for t in s.split(\",\") if t.strip()]\n",
    "    return s.split()\n",
    "\n",
    "# ---------- Metrics (text-based evaluation) ----------\n",
    "\n",
    "def apk(actual: List[str], predicted: List[str], k: int) -> float:\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "    pred_k = predicted[:k]\n",
    "    hits, score = 0, 0.0\n",
    "    seen = set()\n",
    "    for i, p in enumerate(pred_k, start=1):\n",
    "        if p in actual and p not in seen:\n",
    "            hits += 1\n",
    "            score += hits / i\n",
    "            seen.add(p)\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual_list: List[List[str]], predicted_list: List[List[str]], k: int) -> float:\n",
    "    scores = [apk(a, p, k) for a, p in zip(actual_list, predicted_list)]\n",
    "    return float(np.mean(scores)) if scores else 0.0\n",
    "\n",
    "def dcg_at_k(predicted: List[str], ideal_texts: List[str], k: int) -> float:\n",
    "    pred_k = predicted[:k]\n",
    "    dcg = 0.0\n",
    "    for i, p in enumerate(pred_k, start=1):\n",
    "        rel = 1.0 if p in ideal_texts else 0.0\n",
    "        if rel:\n",
    "            dcg += rel / np.log2(i + 1)\n",
    "    return dcg\n",
    "\n",
    "def idcg_at_k(ideal_texts: List[str], k: int) -> float:\n",
    "    g = min(len(ideal_texts), k)\n",
    "    return sum(1.0 / np.log2(i + 1) for i in range(1, g + 1))\n",
    "\n",
    "def ndcg_at_k(predicted: List[str], ideal_texts: List[str], k: int) -> float:\n",
    "    idcg = idcg_at_k(ideal_texts, k)\n",
    "    if idcg == 0.0:\n",
    "        return 0.0\n",
    "    return dcg_at_k(predicted, ideal_texts, k) / idcg\n",
    "\n",
    "\n",
    "# ---------- Cross-Encoder Loading and Scoring ----------\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "_ce_tokenizer = None\n",
    "_ce_model = None\n",
    "\n",
    "def load_cross_encoder():\n",
    "    global _ce_tokenizer, _ce_model\n",
    "    if _ce_tokenizer is not None and _ce_model is not None:\n",
    "        return\n",
    "    if CE_LOCAL_ONLY:\n",
    "        if not os.path.isdir(CROSS_ENCODER_MODEL_PATH):\n",
    "            raise OSError(\n",
    "                f\"Local CE folder not found: {CROSS_ENCODER_MODEL_PATH}. \"\n",
    "                f\"Set CE_LOCAL_ONLY=False to load from HF Hub or place the model locally.\"\n",
    "            )\n",
    "        _ce_tokenizer = AutoTokenizer.from_pretrained(CROSS_ENCODER_MODEL_PATH, local_files_only=True)\n",
    "        _ce_model = AutoModelForSequenceClassification.from_pretrained(CROSS_ENCODER_MODEL_PATH, local_files_only=True)\n",
    "    else:\n",
    "        # Try local path first if exists, else HF Hub\n",
    "        if os.path.isdir(CROSS_ENCODER_MODEL_PATH):\n",
    "            _ce_tokenizer = AutoTokenizer.from_pretrained(CROSS_ENCODER_MODEL_PATH)\n",
    "            _ce_model = AutoModelForSequenceClassification.from_pretrained(CROSS_ENCODER_MODEL_PATH)\n",
    "        else:\n",
    "            _ce_tokenizer = AutoTokenizer.from_pretrained(CROSS_ENCODER_MODEL)\n",
    "            _ce_model = AutoModelForSequenceClassification.from_pretrained(CROSS_ENCODER_MODEL)\n",
    "    _ce_model.eval()\n",
    "    if CE_DEVICE:\n",
    "        _ce_model.to(CE_DEVICE)\n",
    "\n",
    "def ce_score_pairs(pairs: List[Tuple[str, str]], batch_size: int = 32) -> List[float]:\n",
    "    \"\"\"\n",
    "    pairs: list of (query, full_text)\n",
    "    returns: list of float scores (higher = more relevant)\n",
    "    \"\"\"\n",
    "    load_cross_encoder()\n",
    "    scores = []\n",
    "    device = CE_DEVICE or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    _ce_model.to(device)\n",
    "\n",
    "    # Some CE heads output logits with shape [batch, 1] or [batch, 2]\n",
    "    # We’ll take the positive class logit or the single logit.\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(pairs), batch_size):\n",
    "            batch = pairs[i:i+batch_size]\n",
    "            texts = list(batch)  # [(q, d), ...]\n",
    "            inputs = _ce_tokenizer(\n",
    "                [t[0] for t in texts],\n",
    "                [t[1] for t in texts],\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=512\n",
    "            )\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = _ce_model(**inputs)\n",
    "            logits = outputs.logits  # [B, C] or [B]\n",
    "            if logits.dim() == 1:\n",
    "                batch_scores = logits.detach().float().cpu().tolist()\n",
    "            else:\n",
    "                # If binary, take the logit of the positive class (assume index 1)\n",
    "                if logits.size(-1) == 1:\n",
    "                    batch_scores = logits.squeeze(-1).detach().float().cpu().tolist()\n",
    "                else:\n",
    "                    batch_scores = logits[:, -1].detach().float().cpu().tolist()\n",
    "            scores.extend(batch_scores)\n",
    "    return scores\n",
    "\n",
    "\n",
    "# ---------- Main Pipeline with CE re-ranking ----------\n",
    "\n",
    "def main():\n",
    "    print(\"CWD:\", os.getcwd())\n",
    "    # Load CSVs\n",
    "    df_sparse = pd.read_csv(SPLADE_CSV)\n",
    "    df_dense = pd.read_csv(MPNET_CSV)\n",
    "\n",
    "    # Preserve originals\n",
    "    df_sparse[\"_q_orig\"] = df_sparse[\"question\"]\n",
    "    df_sparse[\"_a_orig\"] = df_sparse[\"answer\"]\n",
    "    df_sparse[\"_gt_orig\"] = df_sparse[\"groundtruth_docs\"]\n",
    "\n",
    "    df_dense[\"_q_orig\"] = df_dense[\"question\"]\n",
    "    df_dense[\"_a_orig\"] = df_dense[\"answer\"]\n",
    "    df_dense[\"_gt_orig\"] = df_dense[\"groundtruth_docs\"]\n",
    "\n",
    "    # Normalized join keys\n",
    "    def normalize_groundtruth_str(gt_field: Any) -> str:\n",
    "        data = safe_parse_list(gt_field)\n",
    "        if isinstance(data, list):\n",
    "            norm_items = [normalize_text(x) for x in data]\n",
    "            return json.dumps(norm_items, ensure_ascii=False)\n",
    "        return json.dumps([normalize_text(str(gt_field))], ensure_ascii=False)\n",
    "\n",
    "    df_sparse[\"_q_norm\"] = df_sparse[\"question\"].apply(normalize_text)\n",
    "    df_sparse[\"_a_norm\"] = df_sparse[\"answer\"].apply(normalize_text)\n",
    "    df_sparse[\"_gt_norm\"] = df_sparse[\"groundtruth_docs\"].apply(normalize_groundtruth_str)\n",
    "\n",
    "    df_dense[\"_q_norm\"] = df_dense[\"question\"].apply(normalize_text)\n",
    "    df_dense[\"_a_norm\"] = df_dense[\"answer\"].apply(normalize_text)\n",
    "    df_dense[\"_gt_norm\"] = df_dense[\"groundtruth_docs\"].apply(normalize_groundtruth_str)\n",
    "\n",
    "    # Merge\n",
    "    df = pd.merge(\n",
    "        df_sparse,\n",
    "        df_dense,\n",
    "        on=[\"_q_norm\", \"_a_norm\", \"_gt_norm\"],\n",
    "        suffixes=(\"_splade\", \"_mpnet\"),\n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "    if df.empty:\n",
    "        raise ValueError(\"No rows matched after normalization. Check groundtruth formats across CSVs.\")\n",
    "\n",
    "    hybrid_ret_docs_col = []\n",
    "    map3_list, ndcg3_list = [], []\n",
    "    map5_list, ndcg5_list = [], []\n",
    "    map10_list, ndcg10_list = [], []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        # Parse and cap lists\n",
    "        splade_list = parse_ret_list(row[\"splade_ret_docs\"])[:TOP_K_PER_LIST]\n",
    "        mpnet_list  = parse_ret_list(row[\"mpnet_ret_docs\"])[:TOP_K_PER_LIST]\n",
    "        gt_list_raw = parse_groundtruth_list(row[\"_gt_orig_splade\"])\n",
    "\n",
    "        # Build candidate pool = union by doc_id, preserving first seen full_text\n",
    "        cand_map = {}\n",
    "        for d in splade_list:\n",
    "            if d[\"doc_id\"] not in cand_map:\n",
    "                cand_map[d[\"doc_id\"]] = {\"doc_id\": d[\"doc_id\"], \"full_text\": d.get(\"full_text\", \"\"), \"sources\": {\"splade\": True}}\n",
    "            else:\n",
    "                cand_map[d[\"doc_id\"]][\"sources\"][\"splade\"] = True\n",
    "        for d in mpnet_list:\n",
    "            if d[\"doc_id\"] not in cand_map:\n",
    "                cand_map[d[\"doc_id\"]] = {\"doc_id\": d[\"doc_id\"], \"full_text\": d.get(\"full_text\", \"\"), \"sources\": {\"mpnet\": True}}\n",
    "            else:\n",
    "                cand_map[d[\"doc_id\"]][\"sources\"][\"mpnet\"] = True\n",
    "\n",
    "        candidates = list(cand_map.values())\n",
    "        # Re-rank with cross-encoder\n",
    "        question_text = row[\"_q_orig_splade\"]\n",
    "        pairs = [(question_text, c[\"full_text\"]) for c in candidates]\n",
    "        if pairs:\n",
    "            scores = ce_score_pairs(pairs, batch_size=CE_BATCH_SIZE)\n",
    "        else:\n",
    "            scores = []\n",
    "\n",
    "        # Attach scores and sort\n",
    "        for i, c in enumerate(candidates):\n",
    "            c[\"_ce_score\"] = float(scores[i]) if i < len(scores) else float(\"-inf\")\n",
    "\n",
    "        candidates.sort(key=lambda x: x[\"_ce_score\"], reverse=True)\n",
    "        reranked = candidates[:FINAL_TOP_K]\n",
    "\n",
    "        # Build hybrid_ret_docs with CE score and rank\n",
    "        hybrid_struct = []\n",
    "        fused_norm_fulltexts = []\n",
    "        for idx, c in enumerate(reranked, start=1):\n",
    "            entry = {\n",
    "                \"doc_id\": c[\"doc_id\"],\n",
    "                \"score\": c[\"_ce_score\"],\n",
    "                \"rank\": idx,\n",
    "                \"full_text\": c.get(\"full_text\", \"\"),\n",
    "                \"source_ranks\": {  # we don’t compute per-list ranks for CE; mark presence\n",
    "                    \"splade\": 1 if c[\"sources\"].get(\"splade\") else None,\n",
    "                    \"mpnet\": 1 if c[\"sources\"].get(\"mpnet\") else None,\n",
    "                },\n",
    "                \"fusion_mode\": FUSION_LABEL\n",
    "            }\n",
    "            if LOG_WEIGHTS:\n",
    "                entry[\"qc_weights\"] = None\n",
    "            hybrid_struct.append(entry)\n",
    "            fused_norm_fulltexts.append(normalize_text(entry[\"full_text\"]))\n",
    "\n",
    "        hybrid_ret_docs_col.append(json.dumps(hybrid_struct, ensure_ascii=False))\n",
    "\n",
    "        # Metrics by text\n",
    "        gt_norm_texts = [normalize_text(x) for x in gt_list_raw]\n",
    "        pred = fused_norm_fulltexts\n",
    "        map3_list.append(apk(gt_norm_texts, pred, 3))\n",
    "        map5_list.append(apk(gt_norm_texts, pred, 5))\n",
    "        map10_list.append(apk(gt_norm_texts, pred, 10))\n",
    "        ndcg3_list.append(ndcg_at_k(pred, gt_norm_texts, 3))\n",
    "        ndcg5_list.append(ndcg_at_k(pred, gt_norm_texts, 5))\n",
    "        ndcg10_list.append(ndcg_at_k(pred, gt_norm_texts, 10))\n",
    "\n",
    "    # Output\n",
    "    out = pd.DataFrame({\n",
    "        \"question\": df[\"_q_orig_splade\"],\n",
    "        \"answer\": df[\"_a_orig_splade\"],\n",
    "        \"groundtruth_docs\": df[\"_gt_orig_splade\"],\n",
    "        \"splade_ret_docs\": df[\"splade_ret_docs\"],\n",
    "        \"mpnet_ret_docs\": df[\"mpnet_ret_docs\"],\n",
    "        \"hybrid_ret_docs\": hybrid_ret_docs_col,\n",
    "        \"MAP@3\": map3_list,\n",
    "        \"NDCG@3\": ndcg3_list,\n",
    "        \"MAP@5\": map5_list,\n",
    "        \"NDCG@5\": ndcg5_list,\n",
    "        \"MAP@10\": map10_list,\n",
    "        \"NDCG@10\": ndcg10_list,\n",
    "    })\n",
    "\n",
    "    cols = [\n",
    "        \"question\", \"answer\", \"groundtruth_docs\",\n",
    "        \"splade_ret_docs\", \"mpnet_ret_docs\", \"hybrid_ret_docs\",\n",
    "        \"MAP@3\", \"NDCG@3\", \"MAP@5\", \"NDCG@5\", \"MAP@10\", \"NDCG@10\"\n",
    "    ]\n",
    "    out = out[cols]\n",
    "    out.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    overall = {\n",
    "        \"MAP@3\": float(np.mean(map3_list)) if map3_list else 0.0,\n",
    "        \"NDCG@3\": float(np.mean(ndcg3_list)) if ndcg3_list else 0.0,\n",
    "        \"MAP@5\": float(np.mean(map5_list)) if map5_list else 0.0,\n",
    "        \"NDCG@5\": float(np.mean(ndcg5_list)) if ndcg5_list else 0.0,\n",
    "        \"MAP@10\": float(np.mean(map10_list)) if map10_list else 0.0,\n",
    "        \"NDCG@10\": float(np.mean(ndcg10_list)) if ndcg10_list else 0.0,\n",
    "    }\n",
    "    print(\"Saved:\", OUTPUT_CSV)\n",
    "    print(\"Fusion mode:\", FUSION_LABEL)\n",
    "    print(\"TOP_K_PER_LIST:\", TOP_K_PER_LIST, \"FINAL_TOP_K:\", FINAL_TOP_K)\n",
    "    print(\"Cross-Encoder:\", CROSS_ENCODER_MODEL if not CE_LOCAL_ONLY else CROSS_ENCODER_MODEL_PATH)\n",
    "    print(\"Hybrid overall averages:\", overall)\n",
    "\n",
    "\n",
    "# Run\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fusion mode: ce_rerank\n",
    "TOP_K_PER_LIST: 20 FINAL_TOP_K: 10\n",
    "Cross-Encoder: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
    "Hybrid overall averages: {'MAP@3': 0.5191621427632709, 'NDCG@3': 0.5946335494517143, 'MAP@5': 0.5326992033886311, 'NDCG@5': 0.6147403898582319, 'MAP@10': 0.5451409088593357, 'NDCG@10': 0.6352804956790745}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross encoders are working better so far. I am trying to optimise their performance by adding other methods on top of that.\n",
    "\n",
    "- Candidate generation:\n",
    "- - Weighted RRF over SPLADE + MPNet (with global weights favoring SPLADE modestly).\n",
    "- - Then score-based linear interpolation with COMBMNZ on the union to reinforce agreement.\n",
    "- - This “two-step fusion” sharpens the shortlist before CE.\n",
    "\n",
    "- Cross-encoder re-ranking:\n",
    "- - Use cross-encoder/ms-marco-MiniLM-L-12-v2 (stronger than L-6, still efficient).\n",
    "- - Re-rank top-N from fusion (N typically 60).\n",
    "\n",
    "- Post-CE blending:\n",
    "- - FinalScore = α·CE + (1−α)·Fused (both min-max normalized per query).\n",
    "- - This stabilizes CE variability and often yields extra gains.\n",
    "\n",
    "- Practical defaults:\n",
    "- - Weighted RRF K=60, λ=(0.65, 0.35), Interp+COMBMNZ, candidate_pool_size=60, CE=L-12, α=0.85.\n",
    "- - You can raise candidate_pool_size to 80 if your hardware allows; it often helps slightly.\n",
    "\n",
    "Tuning tips to push toward +7–8% over SPLADE\n",
    "\n",
    "Increase CANDIDATE_POOL_SIZE to 80 if you can afford the CE pass; often yields a further lift.\n",
    "Try INTERP_GLOBAL_WEIGHTS = (0.6, 0.4) if MPNet is somewhat complementary on your dataset.\n",
    "If CE latency is acceptable, test CROSS_ENCODER_MODEL = \"cross-encoder/ms-marco-MiniLM-L-12-v2\" (already set). If still underwhelming, try \"cross-encoder/ms-marco-electra-base\".\n",
    "Sweep ALPHA_CE_BLEND in {0.8, 0.85, 0.9}. 0.85 is a good default; 0.9 biases more to CE.\n",
    "If you have memory/time, test TOP_K_PER_LIST=50 and/or RRF_K in {30, 60, 120}.\n",
    "Why this should outperform prior attempts\n",
    "\n",
    "RRF reduces sensitivity to raw scores and brings complementary docs up.\n",
    "Score interpolation with COMBMNZ rewards agreement and sharpens the pool.\n",
    "CE L-12 is materially stronger than L-6; blending stabilizes edge cases where CE alone may shuffle near-ties suboptimally.\n",
    "This stacking structure (RRF -> Interp+COMBMNZ -> CE -> Blend) is a proven recipe in IR stacks to get consistent gains over strong sparse baselines.\n",
    "\n",
    "A tiny sweep harness to run a short grid on:\n",
    "\n",
    "λ for RRF and interpolation,\n",
    "COMBMNZ on/off,\n",
    "candidate_pool_size ∈ {60, 80},\n",
    "α ∈ {0.8, 0.85, 0.9},\n",
    "and print a summary table so you can pick the best configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csmala/miniconda3/envs/halu_rag/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: /home/csmala/journal_rag/hybrid_pipeline\n",
      "Saved: ./hybrid_optimized_fusion_ce.csv\n",
      "Pipeline: weighted_rrf -> score_interp_combmnz -> CE(L12) -> blend\n",
      "Weighted RRF: K = 60 weights = (0.65, 0.35)\n",
      "Interpolation weights = (0.7, 0.3) COMBMNZ = True\n",
      "Candidate pool size = 60 | FINAL_TOP_K = 10\n",
      "Cross-Encoder: cross-encoder/ms-marco-MiniLM-L-12-v2\n",
      "Post-CE blending: True alpha = 0.85\n",
      "Hybrid overall averages: {'MAP@3': 0.5144442675320212, 'NDCG@3': 0.5901077866678365, 'MAP@5': 0.5279396747591463, 'NDCG@5': 0.6103883965954874, 'MAP@10': 0.5401260917400604, 'NDCG@10': 0.6306136362160123}\n"
     ]
    }
   ],
   "source": [
    "# Cross encoders are working better so far. I am trying to optimise their performance by adding other methods on top of that.\n",
    "\n",
    "\n",
    "import json\n",
    "import ast\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "# =================== CONFIG ===================\n",
    "# Input/Output\n",
    "SPLADE_CSV = \"./prior_results/hotpotqa_splade.csv\"\n",
    "MPNET_CSV  = \"./prior_results/hotpotqa_mpnet.csv\"\n",
    "OUTPUT_CSV = \"./hybrid_optimized_fusion_ce.csv\"\n",
    "\n",
    "# Candidate generation caps\n",
    "TOP_K_PER_LIST = 40         # take top-N from each retriever before initial fusion\n",
    "CANDIDATE_POOL_SIZE = 60    # number of candidates passed to CE after fusion (try 60 or 80)\n",
    "FINAL_TOP_K = 10            # final top-N to save/evaluate\n",
    "\n",
    "# Weighted RRF settings (stage A)\n",
    "USE_WEIGHTED_RRF = True\n",
    "RRF_K = 60\n",
    "RRF_GLOBAL_WEIGHTS = (0.65, 0.35)  # (w_splade, w_mpnet). Slight SPLADE bias but still lets MPNet matter\n",
    "\n",
    "# Score interpolation settings (stage B) – applied on the union after RRF\n",
    "USE_SCORE_INTERP = True\n",
    "SCORE_NORM_PER_LIST = True     # min-max normalize per list before interpolation\n",
    "INTERP_GLOBAL_WEIGHTS = (0.7, 0.3)  # λ for (splade, mpnet)\n",
    "USE_COMBMNZ = True             # multiply interpolated score by #systems that retrieved the doc (reinforce agreement)\n",
    "\n",
    "# Cross-encoder model (stage C)\n",
    "# Stronger than L-6: slightly slower but typically better MAP/NDCG\n",
    "CROSS_ENCODER_MODEL = \"cross-encoder/ms-marco-MiniLM-L-12-v2\"\n",
    "CROSS_ENCODER_MODEL_PATH = \"./ce_ms_marco_minilm_l12_v2\"  # optional local cache/folder\n",
    "CE_LOCAL_ONLY = False\n",
    "CE_BATCH_SIZE = 32\n",
    "CE_DEVICE = None  # \"cuda\" or \"cpu\" or None (auto)\n",
    "\n",
    "# Post-CE blending (stage D)\n",
    "USE_POST_CE_BLENDING = True\n",
    "ALPHA_CE_BLEND = 0.85  # Final = α·CE + (1−α)·Fused (both min-max normalized per query)\n",
    "\n",
    "# Logging\n",
    "LOG_RUN_META = True\n",
    "# ==============================================\n",
    "\n",
    "\n",
    "# --------------- Utilities ---------------\n",
    "\n",
    "def normalize_text(s: Any) -> str:\n",
    "    if s is None or (isinstance(s, float) and np.isnan(s)):\n",
    "        return \"\"\n",
    "    s = str(s)\n",
    "    s = s.strip().lower()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def safe_parse_list(val: Any) -> Any:\n",
    "    if isinstance(val, (list, dict)):\n",
    "        return val\n",
    "    if val is None or (isinstance(val, float) and np.isnan(val)):\n",
    "        return []\n",
    "    s = str(val).strip()\n",
    "    if s == \"\":\n",
    "        return []\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return ast.literal_eval(s)\n",
    "        except Exception:\n",
    "            return []\n",
    "\n",
    "def normalize_groundtruth_str(gt_field: Any) -> str:\n",
    "    data = safe_parse_list(gt_field)\n",
    "    if isinstance(data, list):\n",
    "        norm_items = [normalize_text(x) for x in data]\n",
    "        return json.dumps(norm_items, ensure_ascii=False)\n",
    "    return json.dumps([normalize_text(str(gt_field))], ensure_ascii=False)\n",
    "\n",
    "def parse_ret_list(ret_field: Any) -> List[Dict[str, Any]]:\n",
    "    data = safe_parse_list(ret_field)\n",
    "    out = []\n",
    "    if isinstance(data, list):\n",
    "        for d in data:\n",
    "            if isinstance(d, dict) and \"doc_id\" in d:\n",
    "                doc_id = str(d.get(\"doc_id\"))\n",
    "                score = float(d.get(\"score\", 0.0))\n",
    "                full_text = d.get(\"full_text\", \"\")\n",
    "                out.append({\"doc_id\": doc_id, \"score\": score, \"full_text\": full_text})\n",
    "    return out\n",
    "\n",
    "def parse_groundtruth_list(gt_field: Any) -> List[str]:\n",
    "    data = safe_parse_list(gt_field)\n",
    "    if isinstance(data, list):\n",
    "        return [str(x) for x in data]\n",
    "    s = str(gt_field).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    if \",\" in s:\n",
    "        return [t.strip() for t in s.split(\",\") if t.strip()]\n",
    "    return s.split()\n",
    "\n",
    "def min_max_normalize(values: List[float]) -> List[float]:\n",
    "    if not values:\n",
    "        return []\n",
    "    mn, mx = min(values), max(values)\n",
    "    if mx == mn:\n",
    "        return [0.0 for _ in values]\n",
    "    return [(x - mn) / (mx - mn) for x in values]\n",
    "\n",
    "def per_list_minmax_map(items: List[Dict[str, Any]]) -> Dict[str, float]:\n",
    "    scores = [x[\"score\"] for x in items]\n",
    "    norm = min_max_normalize(scores)\n",
    "    return {items[i][\"doc_id\"]: norm[i] for i in range(len(items))}\n",
    "\n",
    "# --------------- Fusion blocks ---------------\n",
    "\n",
    "def rrf_fuse_weighted(lists: List[List[Dict[str, Any]]], weights: List[float], k: int = 60):\n",
    "    \"\"\"\n",
    "    Weighted Reciprocal Rank Fusion:\n",
    "    score(d) = Σ_i w_i * 1 / (k + rank_i(d))\n",
    "    \"\"\"\n",
    "    rank_maps = []\n",
    "    for lst in lists:\n",
    "        ranks = {it[\"doc_id\"]: r for r, it in enumerate(sorted(lst, key=lambda x: x[\"score\"], reverse=True), start=1)}\n",
    "        rank_maps.append(ranks)\n",
    "    all_doc_ids = set().union(*[set(m.keys()) for m in rank_maps])\n",
    "\n",
    "    fused = []\n",
    "    for d in all_doc_ids:\n",
    "        s = 0.0\n",
    "        for i, ranks in enumerate(rank_maps):\n",
    "            r = ranks.get(d)\n",
    "            if r is not None:\n",
    "                s += float(weights[i]) * (1.0 / (k + r))\n",
    "        fused.append({\"doc_id\": d, \"fused_rrf\": s})\n",
    "    fused.sort(key=lambda x: x[\"fused_rrf\"], reverse=True)\n",
    "    return fused\n",
    "\n",
    "def score_sum_fuse_weighted(lists: List[List[Dict[str, Any]]], weights: List[float], normalize_per_list: bool = True, combmnz: bool = False):\n",
    "    \"\"\"\n",
    "    Weighted linear interpolation with optional COMBMNZ.\n",
    "    \"\"\"\n",
    "    score_maps = []\n",
    "    for lst in lists:\n",
    "        if normalize_per_list:\n",
    "            m = per_list_minmax_map(lst)\n",
    "        else:\n",
    "            m = {x[\"doc_id\"]: float(x[\"score\"]) for x in lst}\n",
    "        score_maps.append(m)\n",
    "\n",
    "    all_doc_ids = set().union(*[set(m.keys()) for m in score_maps])\n",
    "    fused = []\n",
    "    for d in all_doc_ids:\n",
    "        per_scores = [m.get(d, 0.0) for m in score_maps]\n",
    "        val = sum(float(weights[i]) * per_scores[i] for i in range(len(per_scores)))\n",
    "        if combmnz:\n",
    "            c = sum(1 for s in per_scores if s > 0.0)\n",
    "            val *= max(1, c)\n",
    "        fused.append({\"doc_id\": d, \"fused_interp\": val})\n",
    "    fused.sort(key=lambda x: x[\"fused_interp\"], reverse=True)\n",
    "    return fused\n",
    "\n",
    "# --------------- Metrics ---------------\n",
    "\n",
    "def apk(actual: List[str], predicted: List[str], k: int) -> float:\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "    pred_k = predicted[:k]\n",
    "    hits, score = 0, 0.0\n",
    "    seen = set()\n",
    "    for i, p in enumerate(pred_k, start=1):\n",
    "        if p in actual and p not in seen:\n",
    "            hits += 1\n",
    "            score += hits / i\n",
    "            seen.add(p)\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def ndcg_at_k(predicted: List[str], ideal_texts: List[str], k: int) -> float:\n",
    "    pred_k = predicted[:k]\n",
    "    dcg = 0.0\n",
    "    for i, p in enumerate(pred_k, start=1):\n",
    "        rel = 1.0 if p in ideal_texts else 0.0\n",
    "        if rel:\n",
    "            dcg += rel / np.log2(i + 1)\n",
    "    g = min(len(ideal_texts), k)\n",
    "    idcg = sum(1.0 / np.log2(i + 1) for i in range(1, g + 1))\n",
    "    return (dcg / idcg) if idcg > 0.0 else 0.0\n",
    "\n",
    "# --------------- Cross-Encoder ---------------\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "_ce_tokenizer = None\n",
    "_ce_model = None\n",
    "\n",
    "def load_cross_encoder():\n",
    "    global _ce_tokenizer, _ce_model\n",
    "    if _ce_tokenizer is not None and _ce_model is not None:\n",
    "        return\n",
    "    if CE_LOCAL_ONLY:\n",
    "        if not os.path.isdir(CROSS_ENCODER_MODEL_PATH):\n",
    "            raise OSError(\n",
    "                f\"Local CE folder not found: {CROSS_ENCODER_MODEL_PATH}. \"\n",
    "                f\"Set CE_LOCAL_ONLY=False to load from HF Hub or place the model locally.\"\n",
    "            )\n",
    "        _ce_tokenizer = AutoTokenizer.from_pretrained(CROSS_ENCODER_MODEL_PATH, local_files_only=True)\n",
    "        _ce_model = AutoModelForSequenceClassification.from_pretrained(CROSS_ENCODER_MODEL_PATH, local_files_only=True)\n",
    "    else:\n",
    "        if os.path.isdir(CROSS_ENCODER_MODEL_PATH):\n",
    "            _ce_tokenizer = AutoTokenizer.from_pretrained(CROSS_ENCODER_MODEL_PATH)\n",
    "            _ce_model = AutoModelForSequenceClassification.from_pretrained(CROSS_ENCODER_MODEL_PATH)\n",
    "        else:\n",
    "            _ce_tokenizer = AutoTokenizer.from_pretrained(CROSS_ENCODER_MODEL)\n",
    "            _ce_model = AutoModelForSequenceClassification.from_pretrained(CROSS_ENCODER_MODEL)\n",
    "    _ce_model.eval()\n",
    "    if CE_DEVICE:\n",
    "        _ce_model.to(CE_DEVICE)\n",
    "\n",
    "def ce_score_pairs(pairs: List[Tuple[str, str]], batch_size: int = 32) -> List[float]:\n",
    "    load_cross_encoder()\n",
    "    device = CE_DEVICE or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    _ce_model.to(device)\n",
    "\n",
    "    scores = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(pairs), batch_size):\n",
    "            batch = pairs[i:i+batch_size]\n",
    "            inputs = _ce_tokenizer(\n",
    "                [q for q, _ in batch],\n",
    "                [d for _, d in batch],\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=512\n",
    "            )\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = _ce_model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            if logits.dim() == 1:\n",
    "                batch_scores = logits.detach().float().cpu().tolist()\n",
    "            else:\n",
    "                if logits.size(-1) == 1:\n",
    "                    batch_scores = logits.squeeze(-1).detach().float().cpu().tolist()\n",
    "                else:\n",
    "                    batch_scores = logits[:, -1].detach().float().cpu().tolist()\n",
    "            scores.extend(batch_scores)\n",
    "    return scores\n",
    "\n",
    "# --------------- Main: Optimized pipeline ---------------\n",
    "\n",
    "def main():\n",
    "    print(\"CWD:\", os.getcwd())\n",
    "\n",
    "    # Load CSVs\n",
    "    df_sparse = pd.read_csv(SPLADE_CSV)\n",
    "    df_dense = pd.read_csv(MPNET_CSV)\n",
    "\n",
    "    # Preserve originals\n",
    "    df_sparse[\"_q_orig\"] = df_sparse[\"question\"]\n",
    "    df_sparse[\"_a_orig\"] = df_sparse[\"answer\"]\n",
    "    df_sparse[\"_gt_orig\"] = df_sparse[\"groundtruth_docs\"]\n",
    "\n",
    "    df_dense[\"_q_orig\"] = df_dense[\"question\"]\n",
    "    df_dense[\"_a_orig\"] = df_dense[\"answer\"]\n",
    "    df_dense[\"_gt_orig\"] = df_dense[\"groundtruth_docs\"]\n",
    "\n",
    "    # Normalize join keys\n",
    "    df_sparse[\"_q_norm\"] = df_sparse[\"question\"].apply(normalize_text)\n",
    "    df_sparse[\"_a_norm\"] = df_sparse[\"answer\"].apply(normalize_text)\n",
    "    df_sparse[\"_gt_norm\"] = df_sparse[\"groundtruth_docs\"].apply(normalize_groundtruth_str)\n",
    "\n",
    "    df_dense[\"_q_norm\"] = df_dense[\"question\"].apply(normalize_text)\n",
    "    df_dense[\"_a_norm\"] = df_dense[\"answer\"].apply(normalize_text)\n",
    "    df_dense[\"_gt_norm\"] = df_dense[\"groundtruth_docs\"].apply(normalize_groundtruth_str)\n",
    "\n",
    "    # Merge\n",
    "    df = pd.merge(\n",
    "        df_sparse, df_dense,\n",
    "        on=[\"_q_norm\", \"_a_norm\", \"_gt_norm\"],\n",
    "        suffixes=(\"_splade\", \"_mpnet\"),\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    if df.empty:\n",
    "        raise ValueError(\"No rows matched after normalization. Check groundtruth formats across CSVs.\")\n",
    "\n",
    "    # Normalize weights\n",
    "    def norm_pair(w0, w1):\n",
    "        s = float(w0) + float(w1)\n",
    "        return (0.5, 0.5) if s <= 0 else (float(w0)/s, float(w1)/s)\n",
    "\n",
    "    w_rrf = norm_pair(*RRF_GLOBAL_WEIGHTS)\n",
    "    w_interp = norm_pair(*INTERP_GLOBAL_WEIGHTS)\n",
    "\n",
    "    hybrid_ret_docs_col = []\n",
    "    map3_list, map5_list, map10_list = [], [], []\n",
    "    ndcg3_list, ndcg5_list, ndcg10_list = [], [], []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        # Parse and cap inputs\n",
    "        splade_list = parse_ret_list(row[\"splade_ret_docs\"])[:TOP_K_PER_LIST]\n",
    "        mpnet_list  = parse_ret_list(row[\"mpnet_ret_docs\"])[:TOP_K_PER_LIST]\n",
    "        gt_list_raw = parse_groundtruth_list(row[\"_gt_orig_splade\"])\n",
    "        q_text = row[\"_q_orig_splade\"]\n",
    "\n",
    "        # Stage A: Weighted RRF to get an initial global ranking (conservative blend)\n",
    "        if USE_WEIGHTED_RRF:\n",
    "            rrf_fused = rrf_fuse_weighted([splade_list, mpnet_list], weights=w_rrf, k=RRF_K)\n",
    "        else:\n",
    "            # If disabled, start with union pool\n",
    "            rrf_fused = [{\"doc_id\": x[\"doc_id\"], \"fused_rrf\": 0.0} for x in (splade_list + mpnet_list)]\n",
    "\n",
    "        # Collect union set and maps\n",
    "        splade_map = {x[\"doc_id\"]: x for x in splade_list}\n",
    "        mpnet_map  = {x[\"doc_id\"]: x for x in mpnet_list}\n",
    "\n",
    "        # Attach a conservative fused score for stage B fallback\n",
    "        # Build initial candidate list ordered by RRF\n",
    "        candidates = []\n",
    "        seen = set()\n",
    "        for item in rrf_fused:\n",
    "            d = item[\"doc_id\"]\n",
    "            if d in seen:\n",
    "                continue\n",
    "            seen.add(d)\n",
    "            full_text = splade_map.get(d, mpnet_map.get(d, {})).get(\"full_text\", \"\")\n",
    "            candidates.append({\n",
    "                \"doc_id\": d,\n",
    "                \"full_text\": full_text,\n",
    "                \"rrf_score\": item.get(\"fused_rrf\", 0.0)\n",
    "            })\n",
    "\n",
    "        # Also ensure everything from both lists is included (union), with very small rrf if missing\n",
    "        for dct in (splade_list + mpnet_list):\n",
    "            d = dct[\"doc_id\"]\n",
    "            if d not in seen:\n",
    "                seen.add(d)\n",
    "                candidates.append({\n",
    "                    \"doc_id\": d,\n",
    "                    \"full_text\": dct.get(\"full_text\", \"\"),\n",
    "                    \"rrf_score\": 0.0\n",
    "                })\n",
    "\n",
    "        # Stage B: Score interpolation with COMBMNZ on the union, reinforce agreement\n",
    "        if USE_SCORE_INTERP:\n",
    "            # Build fused interpolation score on union\n",
    "            # Prepare per-list normalized score maps\n",
    "            s_map = per_list_minmax_map(splade_list) if SCORE_NORM_PER_LIST else {x[\"doc_id\"]: float(x[\"score\"]) for x in splade_list}\n",
    "            m_map = per_list_minmax_map(mpnet_list)  if SCORE_NORM_PER_LIST else {x[\"doc_id\"]: float(x[\"score\"]) for x in mpnet_list}\n",
    "\n",
    "            interp_list = []\n",
    "            for c in candidates:\n",
    "                d = c[\"doc_id\"]\n",
    "                s = s_map.get(d, 0.0)\n",
    "                m = m_map.get(d, 0.0)\n",
    "                fused = w_interp[0] * s + w_interp[1] * m\n",
    "                if USE_COMBMNZ:\n",
    "                    present = (1 if s > 0.0 else 0) + (1 if m > 0.0 else 0)\n",
    "                    fused *= max(1, present)\n",
    "                interp_list.append({\"doc_id\": d, \"full_text\": c[\"full_text\"], \"interp_score\": fused, \"rrf_score\": c[\"rrf_score\"]})\n",
    "\n",
    "            # Combine interpolation and RRF as a pre-CE ranking\n",
    "            # We can simply rank by interp_score; if ties, use rrf_score\n",
    "            interp_list.sort(key=lambda x: (x[\"interp_score\"], x[\"rrf_score\"]), reverse=True)\n",
    "            pre_ce = interp_list\n",
    "        else:\n",
    "            # Fallback to RRF ordering\n",
    "            pre_ce = [{\"doc_id\": c[\"doc_id\"], \"full_text\": c[\"full_text\"], \"interp_score\": c[\"rrf_score\"], \"rrf_score\": c[\"rrf_score\"]} for c in candidates]\n",
    "\n",
    "        # Truncate to the candidate pool for CE\n",
    "        pre_ce = pre_ce[:CANDIDATE_POOL_SIZE]\n",
    "\n",
    "        # Stage C: Cross-Encoder re-rank\n",
    "        pairs = [(q_text, it[\"full_text\"]) for it in pre_ce]\n",
    "        if pairs:\n",
    "            ce_scores = ce_score_pairs(pairs, batch_size=CE_BATCH_SIZE)\n",
    "        else:\n",
    "            ce_scores = []\n",
    "\n",
    "        for i, it in enumerate(pre_ce):\n",
    "            it[\"_ce_score\"] = float(ce_scores[i]) if i < len(ce_scores) else float(\"-inf\")\n",
    "\n",
    "        # Stage D: Post-CE blending with pre-CE fused scores (stabilizes and often boosts)\n",
    "        if USE_POST_CE_BLENDING:\n",
    "            # Normalize CE and preCE scores per query\n",
    "            ce_vals = [it[\"_ce_score\"] for it in pre_ce]\n",
    "            fused_vals = [it[\"interp_score\"] for it in pre_ce]\n",
    "            ce_norm = min_max_normalize(ce_vals)\n",
    "            fused_norm = min_max_normalize(fused_vals)\n",
    "            for i, it in enumerate(pre_ce):\n",
    "                it[\"_final_score\"] = ALPHA_CE_BLEND * ce_norm[i] + (1.0 - ALPHA_CE_BLEND) * fused_norm[i]\n",
    "        else:\n",
    "            # Use CE scores only\n",
    "            for it in pre_ce:\n",
    "                it[\"_final_score\"] = it[\"_ce_score\"]\n",
    "\n",
    "        # Final sort and slice\n",
    "        pre_ce.sort(key=lambda x: x[\"_final_score\"], reverse=True)\n",
    "        final = pre_ce[:FINAL_TOP_K]\n",
    "\n",
    "        # Build output struct and compute metrics (by normalized full_text)\n",
    "        hybrid_struct = []\n",
    "        pred_texts = []\n",
    "        for rank_idx, it in enumerate(final, start=1):\n",
    "            entry = {\n",
    "                \"doc_id\": it[\"doc_id\"],\n",
    "                \"score\": it[\"_final_score\"],\n",
    "                \"rank\": rank_idx,\n",
    "                \"full_text\": it[\"full_text\"],\n",
    "                \"fusion_mode\": \"weighted_rrf -> score_interp_combmnz -> CE(L12) -> blend\"\n",
    "            }\n",
    "            if LOG_RUN_META:\n",
    "                entry[\"meta\"] = {\n",
    "                    \"rrf_k\": RRF_K,\n",
    "                    \"rrf_weights\": {\"splade\": w_rrf[0], \"mpnet\": w_rrf[1]},\n",
    "                    \"interp_weights\": {\"splade\": w_interp[0], \"mpnet\": w_interp[1]},\n",
    "                    \"combmnz\": USE_COMBMNZ,\n",
    "                    \"candidate_pool_size\": CANDIDATE_POOL_SIZE,\n",
    "                    \"ce_model\": CROSS_ENCODER_MODEL if not CE_LOCAL_ONLY else CROSS_ENCODER_MODEL_PATH,\n",
    "                    \"alpha_ce_blend\": ALPHA_CE_BLEND if USE_POST_CE_BLENDING else None\n",
    "                }\n",
    "            hybrid_struct.append(entry)\n",
    "            pred_texts.append(normalize_text(it[\"full_text\"]))\n",
    "\n",
    "        hybrid_ret_docs = json.dumps(hybrid_struct, ensure_ascii=False)\n",
    "\n",
    "        # Metrics by text\n",
    "        gt_norm_texts = [normalize_text(x) for x in gt_list_raw]\n",
    "        map3_list.append(apk(gt_norm_texts, pred_texts, 3))\n",
    "        map5_list.append(apk(gt_norm_texts, pred_texts, 5))\n",
    "        map10_list.append(apk(gt_norm_texts, pred_texts, 10))\n",
    "        ndcg3_list.append(ndcg_at_k(pred_texts, gt_norm_texts, 3))\n",
    "        ndcg5_list.append(ndcg_at_k(pred_texts, gt_norm_texts, 5))\n",
    "        ndcg10_list.append(ndcg_at_k(pred_texts, gt_norm_texts, 10))\n",
    "\n",
    "        # Store per-row\n",
    "        row[\"_hybrid_ret_docs\"] = hybrid_ret_docs\n",
    "\n",
    "        # Assign back (collect later)\n",
    "        # We’ll not mutate df in-place per cell to avoid SettingWithCopy; collect a list instead\n",
    "        if \"_hybrid_rows\" not in locals():\n",
    "            _hybrid_rows = []\n",
    "        _hybrid_rows.append(hybrid_ret_docs)\n",
    "\n",
    "    # Output dataframe\n",
    "    out = pd.DataFrame({\n",
    "        \"question\": df[\"_q_orig_splade\"],\n",
    "        \"answer\": df[\"_a_orig_splade\"],\n",
    "        \"groundtruth_docs\": df[\"_gt_orig_splade\"],\n",
    "        \"splade_ret_docs\": df[\"splade_ret_docs\"],\n",
    "        \"mpnet_ret_docs\": df[\"mpnet_ret_docs\"],\n",
    "        \"hybrid_ret_docs\": _hybrid_rows,\n",
    "        \"MAP@3\": map3_list,\n",
    "        \"NDCG@3\": ndcg3_list,\n",
    "        \"MAP@5\": map5_list,\n",
    "        \"NDCG@5\": ndcg5_list,\n",
    "        \"MAP@10\": map10_list,\n",
    "        \"NDCG@10\": ndcg10_list,\n",
    "    })\n",
    "\n",
    "    cols = [\n",
    "        \"question\", \"answer\", \"groundtruth_docs\",\n",
    "        \"splade_ret_docs\", \"mpnet_ret_docs\", \"hybrid_ret_docs\",\n",
    "        \"MAP@3\", \"NDCG@3\", \"MAP@5\", \"NDCG@5\", \"MAP@10\", \"NDCG@10\"\n",
    "    ]\n",
    "    out = out[cols]\n",
    "    out.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    overall = {\n",
    "        \"MAP@3\": float(np.mean(map3_list)) if map3_list else 0.0,\n",
    "        \"NDCG@3\": float(np.mean(ndcg3_list)) if ndcg3_list else 0.0,\n",
    "        \"MAP@5\": float(np.mean(map5_list)) if map5_list else 0.0,\n",
    "        \"NDCG@5\": float(np.mean(ndcg5_list)) if ndcg5_list else 0.0,\n",
    "        \"MAP@10\": float(np.mean(map10_list)) if map10_list else 0.0,\n",
    "        \"NDCG@10\": float(np.mean(ndcg10_list)) if ndcg10_list else 0.0,\n",
    "    }\n",
    "\n",
    "    print(\"Saved:\", OUTPUT_CSV)\n",
    "    print(\"Pipeline: weighted_rrf -> score_interp_combmnz -> CE(L12) -> blend\")\n",
    "    print(\"Weighted RRF: K =\", RRF_K, \"weights =\", w_rrf)\n",
    "    print(\"Interpolation weights =\", w_interp, \"COMBMNZ =\", USE_COMBMNZ)\n",
    "    print(\"Candidate pool size =\", CANDIDATE_POOL_SIZE, \"| FINAL_TOP_K =\", FINAL_TOP_K)\n",
    "    print(\"Cross-Encoder:\", CROSS_ENCODER_MODEL if not CE_LOCAL_ONLY else CROSS_ENCODER_MODEL_PATH)\n",
    "    print(\"Post-CE blending:\", USE_POST_CE_BLENDING, \"alpha =\", ALPHA_CE_BLEND)\n",
    "    print(\"Hybrid overall averages:\", overall)\n",
    "\n",
    "# Run\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time taken: 47m 43.1s\n",
    "Pipeline: weighted_rrf -> score_interp_combmnz -> CE(L12) -> blend\n",
    "Weighted RRF: K = 60 weights = (0.65, 0.35)\n",
    "Interpolation weights = (0.7, 0.3) COMBMNZ = True\n",
    "Candidate pool size = 60 | FINAL_TOP_K = 10\n",
    "Cross-Encoder: cross-encoder/ms-marco-MiniLM-L-12-v2\n",
    "Post-CE blending: True alpha = 0.85\n",
    "Hybrid overall averages: {'MAP@3': 0.5144442675320212, 'NDCG@3': 0.5901077866678365, 'MAP@5': 0.5279396747591463, 'NDCG@5': 0.6103883965954874, 'MAP@10': 0.5401260917400604, 'NDCG@10': 0.6306136362160123}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code is for adaptive weighting scheme driven by a simple proxy for “keywordiness” of the query, which tends to correlate with sparse vs. dense effectiveness:\n",
    "\n",
    "Keyword-heavy queries (many informative/high-IDF terms) → favor SPLADE (sparse).\n",
    "Keyword-light/natural-language queries (few informative terms) → favor MPNet (dense).\n",
    "This avoids your BERT QC’s overconfidence while still being dynamic per query.\n",
    "\n",
    "How it differs from fixed linear interpolation\n",
    "\n",
    "Linear interpolation with a fixed λ uses the same weights for every query.\n",
    "Your TF-IDF-based approach computes λ dynamically from the query’s term statistics. It’s closer to a “soft decision rule” for query complexity, but simple, stable, and explainable.\n",
    "A practical, robust way to implement it\n",
    "\n",
    "Compute a “keywordiness” score from the query via TF-IDF or IDF lookup:\n",
    "Example metric: average IDF of tokens present in the query.\n",
    "Normalize it to [0, 1], then map to a weight λ for sparse.\n",
    "Clamp λ to a band (e.g., [0.3, 0.7]) to avoid extreme swings.\n",
    "Use that λ in score-based fusion (min–max normalize per list, then λ·S_splade + (1−λ)·S_mpnet).\n",
    "Optionally apply COMBMNZ to reward agreement.\n",
    "Then pass the fused shortlist to your cross-encoder for re-ranking and post-CE blending (as in your working pipeline).\n",
    "In this script that:\n",
    "\n",
    "Builds an IDF dictionary on-the-fly from your retriever corpora proxy (document full_texts across SPLADE/MPNet outputs).\n",
    "Computes a per-query λ from average IDF of the query tokens.\n",
    "Fuses using dynamic λ, then CE re-ranks and blends.\n",
    "\n",
    "How to tune:\n",
    "\n",
    "CANDIDATE_POOL_SIZE: 60 → 80 often gives a small boost.\n",
    "LAMBDA_MIN/LAMBDA_MAX: Try (0.4, 0.8) if SPLADE is generally stronger; or (0.3, 0.7) as a conservative band.\n",
    "USE_COMBMNZ: Try on/off; it often helps when systems agree on correct docs.\n",
    "ALPHA_CE_BLEND: Try 0.8 vs 0.9; 0.85 is a strong default.\n",
    "MIN_DF: 2 or 3; too high may ignore useful IDF signals.\n",
    "Why this can hit +7–8%\n",
    "\n",
    "Adaptive λ picks the right balance per query without relying on a brittle classifier.\n",
    "CE L-12 re-ranking plus blending typically adds several points.\n",
    "The IDF-driven weighting is stable (bounded) and explainable, avoiding the “spiky” QC issue.\n",
    "\n",
    "We can also test:\n",
    "\n",
    "λ band {(0.3,0.7), (0.4,0.8)}\n",
    "CANDIDATE_POOL_SIZE {60, 80}\n",
    "ALPHA {0.8, 0.85, 0.9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: /home/csmala/journal_rag/hybrid_pipeline\n",
      "Saved: ./hybrid_tfidf_adaptive_ce.csv\n",
      "Pipeline: adaptive_tfidf_interp -> CE(L12) -> blend\n",
      "λ band (splade): (0.3, 0.7)\n",
      "Candidate pool: 60 | FINAL_TOP_K: 10\n",
      "CE: cross-encoder/ms-marco-MiniLM-L-12-v2\n",
      "Blend α: 0.85 | COMBMNZ: True\n",
      "Hybrid overall averages: {'MAP@3': 0.5134620876676876, 'NDCG@3': 0.5892316464471192, 'MAP@5': 0.5271900810258897, 'NDCG@5': 0.6097992628793352, 'MAP@10': 0.5393110351771699, 'NDCG@10': 0.6299182972580143}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import ast\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Tuple, Iterable, DefaultDict\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# =================== CONFIG ===================\n",
    "SPLADE_CSV = \"./prior_results/hotpotqa_splade.csv\"\n",
    "MPNET_CSV  = \"./prior_results/hotpotqa_mpnet.csv\"\n",
    "OUTPUT_CSV = \"./hybrid_tfidf_adaptive_ce.csv\"\n",
    "\n",
    "# Candidate generation caps\n",
    "TOP_K_PER_LIST = 40         # items taken from each retriever before fusion\n",
    "CANDIDATE_POOL_SIZE = 60    # items passed to CE\n",
    "FINAL_TOP_K = 10            # final top-N to save/evaluate\n",
    "\n",
    "# Adaptive λ band\n",
    "LAMBDA_MIN = 0.3            # min weight on SPLADE\n",
    "LAMBDA_MAX = 0.7            # max weight on SPLADE\n",
    "\n",
    "# IDF construction\n",
    "# Build a lightweight IDF dictionary from all candidate texts seen across rows\n",
    "# This is a proxy for the underlying corpus; good enough to guide λ adaptively.\n",
    "MIN_DF = 2                  # ignore terms appearing in < MIN_DF docs\n",
    "MAX_VOCAB = 200_000         # safety cap\n",
    "\n",
    "# Tokenization\n",
    "TOKEN_PATTERN = r\"[A-Za-z0-9_]+\"  # simple alnum tokens, adjust if needed\n",
    "LOWERCASE = True\n",
    "STOPWORDS = set([\n",
    "    \"the\",\"a\",\"an\",\"of\",\"and\",\"or\",\"to\",\"in\",\"on\",\"for\",\"is\",\"are\",\"was\",\"were\",\"as\",\"with\",\"by\",\"at\",\"from\",\"that\",\"this\",\"it\",\"be\",\"has\",\"have\",\"had\"\n",
    "])  # extend if needed\n",
    "\n",
    "# Cross-encoder model\n",
    "CROSS_ENCODER_MODEL = \"cross-encoder/ms-marco-MiniLM-L-12-v2\"\n",
    "CROSS_ENCODER_MODEL_PATH = \"./ce_ms_marco_minilm_l12_v2\"\n",
    "CE_LOCAL_ONLY = False\n",
    "CE_BATCH_SIZE = 32\n",
    "CE_DEVICE = None  # \"cuda\" or \"cpu\" or None\n",
    "\n",
    "# Post-CE blending\n",
    "USE_POST_CE_BLENDING = True\n",
    "ALPHA_CE_BLEND = 0.85       # Final = α·CE + (1−α)·Fused\n",
    "\n",
    "# Interpolation fusion options\n",
    "SCORE_NORM_PER_LIST = True\n",
    "USE_COMBMNZ = True\n",
    "\n",
    "# Logging\n",
    "LOG_RUN_META = True\n",
    "# ==============================================\n",
    "\n",
    "\n",
    "# --------------- Text utils ---------------\n",
    "\n",
    "def normalize_text(s: Any) -> str:\n",
    "    if s is None or (isinstance(s, float) and np.isnan(s)):\n",
    "        return \"\"\n",
    "    s = str(s)\n",
    "    s = s.strip().lower()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def safe_parse_list(val: Any) -> Any:\n",
    "    if isinstance(val, (list, dict)):\n",
    "        return val\n",
    "    if val is None or (isinstance(val, float) and np.isnan(val)):\n",
    "        return []\n",
    "    s = str(val).strip()\n",
    "    if s == \"\":\n",
    "        return []\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return ast.literal_eval(s)\n",
    "        except Exception:\n",
    "            return []\n",
    "\n",
    "def parse_ret_list(ret_field: Any) -> List[Dict[str, Any]]:\n",
    "    data = safe_parse_list(ret_field)\n",
    "    out = []\n",
    "    if isinstance(data, list):\n",
    "        for d in data:\n",
    "            if isinstance(d, dict) and \"doc_id\" in d:\n",
    "                doc_id = str(d.get(\"doc_id\"))\n",
    "                score = float(d.get(\"score\", 0.0))\n",
    "                full_text = d.get(\"full_text\", \"\")\n",
    "                out.append({\"doc_id\": doc_id, \"score\": score, \"full_text\": full_text})\n",
    "    return out\n",
    "\n",
    "def parse_groundtruth_list(gt_field: Any) -> List[str]:\n",
    "    data = safe_parse_list(gt_field)\n",
    "    if isinstance(data, list):\n",
    "        return [str(x) for x in data]\n",
    "    s = str(gt_field).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    if \",\" in s:\n",
    "        return [t.strip() for t in s.split(\",\") if t.strip()]\n",
    "    return s.split()\n",
    "\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    if text is None:\n",
    "        return []\n",
    "    if LOWERCASE:\n",
    "        text = text.lower()\n",
    "    toks = re.findall(TOKEN_PATTERN, text)\n",
    "    toks = [t for t in toks if t and t not in STOPWORDS]\n",
    "    return toks\n",
    "\n",
    "def min_max_normalize(values: List[float]) -> List[float]:\n",
    "    if not values:\n",
    "        return []\n",
    "    mn, mx = min(values), max(values)\n",
    "    if mx == mn:\n",
    "        return [0.0 for _ in values]\n",
    "    return [(x - mn) / (mx - mn) for x in values]\n",
    "\n",
    "def per_list_minmax_map(items: List[Dict[str, Any]]) -> Dict[str, float]:\n",
    "    scores = [x[\"score\"] for x in items]\n",
    "    norm = min_max_normalize(scores)\n",
    "    return {items[i][\"doc_id\"]: norm[i] for i in range(len(items))}\n",
    "\n",
    "\n",
    "# --------------- Adaptive λ from IDF ---------------\n",
    "\n",
    "def build_idf_dict(all_docs: Iterable[str], min_df: int = 2, max_vocab: int = MAX_VOCAB) -> Dict[str, float]:\n",
    "    # Compute DF (document frequency) over the provided documents (strings)\n",
    "    df_counter: DefaultDict[str, int] = defaultdict(int)\n",
    "    total_docs = 0\n",
    "    for txt in all_docs:\n",
    "        total_docs += 1\n",
    "        toks = set(tokenize(txt))\n",
    "        for t in toks:\n",
    "            df_counter[t] += 1\n",
    "    # Compute IDF\n",
    "    idf = {}\n",
    "    for t, df in df_counter.items():\n",
    "        if df >= min_df:\n",
    "            idf[t] = math.log((1 + total_docs) / (1 + df)) + 1.0  # smooth idf\n",
    "    # Trim vocab if huge\n",
    "    if len(idf) > max_vocab:\n",
    "        # Keep highest IDF terms first (rare terms)\n",
    "        idf = dict(sorted(idf.items(), key=lambda kv: kv[1], reverse=True)[:max_vocab])\n",
    "    return idf\n",
    "\n",
    "def query_keywordiness_idf_avg(query: str, idf: Dict[str, float]) -> float:\n",
    "    toks = tokenize(query)\n",
    "    if not toks:\n",
    "        return 0.0\n",
    "    vals = [idf.get(t, 0.0) for t in toks]\n",
    "    if not vals:\n",
    "        return 0.0\n",
    "    return float(sum(vals) / len(vals))\n",
    "\n",
    "def adaptive_lambda_from_idf(avg_idf: float, min_idf: float, max_idf: float, lam_min: float, lam_max: float) -> float:\n",
    "    # Map avg_idf in [min_idf, max_idf] -> λ in [lam_min, lam_max]\n",
    "    if max_idf <= min_idf:\n",
    "        return (lam_min + lam_max) / 2.0\n",
    "    z = (avg_idf - min_idf) / (max_idf - min_idf)\n",
    "    z = max(0.0, min(1.0, z))\n",
    "    lam = lam_min + z * (lam_max - lam_min)\n",
    "    return float(lam)\n",
    "\n",
    "\n",
    "# --------------- Fusion (score interpolation + COMBMNZ) ---------------\n",
    "\n",
    "def score_sum_fuse_weighted(lists: List[List[Dict[str, Any]]], weights: List[float], normalize_per_list: bool = True, combmnz: bool = False):\n",
    "    score_maps = []\n",
    "    for lst in lists:\n",
    "        if normalize_per_list:\n",
    "            m = per_list_minmax_map(lst)\n",
    "        else:\n",
    "            m = {x[\"doc_id\"]: float(x[\"score\"]) for x in lst}\n",
    "        score_maps.append(m)\n",
    "\n",
    "    all_doc_ids = set().union(*[set(m.keys()) for m in score_maps])\n",
    "    fused = []\n",
    "    for d in all_doc_ids:\n",
    "        per_scores = [m.get(d, 0.0) for m in score_maps]\n",
    "        val = sum(float(weights[i]) * per_scores[i] for i in range(len(per_scores)))\n",
    "        if combmnz:\n",
    "            c = sum(1 for s in per_scores if s > 0.0)\n",
    "            val *= max(1, c)\n",
    "        fused.append({\"doc_id\": d, \"fused_interp\": val})\n",
    "    fused.sort(key=lambda x: x[\"fused_interp\"], reverse=True)\n",
    "    return fused\n",
    "\n",
    "\n",
    "# --------------- Metrics ---------------\n",
    "\n",
    "def apk(actual: List[str], predicted: List[str], k: int) -> float:\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "    pred_k = predicted[:k]\n",
    "    hits, score = 0, 0.0\n",
    "    seen = set()\n",
    "    for i, p in enumerate(pred_k, start=1):\n",
    "        if p in actual and p not in seen:\n",
    "            hits += 1\n",
    "            score += hits / i\n",
    "            seen.add(p)\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def ndcg_at_k(predicted: List[str], ideal_texts: List[str], k: int) -> float:\n",
    "    pred_k = predicted[:k]\n",
    "    dcg = 0.0\n",
    "    for i, p in enumerate(pred_k, start=1):\n",
    "        rel = 1.0 if p in ideal_texts else 0.0\n",
    "        if rel:\n",
    "            dcg += rel / np.log2(i + 1)\n",
    "    g = min(len(ideal_texts), k)\n",
    "    idcg = sum(1.0 / np.log2(i + 1) for i in range(1, g + 1))\n",
    "    return (dcg / idcg) if idcg > 0.0 else 0.0\n",
    "\n",
    "\n",
    "# --------------- Cross-Encoder ---------------\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "_ce_tokenizer = None\n",
    "_ce_model = None\n",
    "\n",
    "def load_cross_encoder():\n",
    "    global _ce_tokenizer, _ce_model\n",
    "    if _ce_tokenizer is not None and _ce_model is not None:\n",
    "        return\n",
    "    if CE_LOCAL_ONLY:\n",
    "        if not os.path.isdir(CROSS_ENCODER_MODEL_PATH):\n",
    "            raise OSError(\n",
    "                f\"Local CE folder not found: {CROSS_ENCODER_MODEL_PATH}. \"\n",
    "                f\"Set CE_LOCAL_ONLY=False to load from HF Hub or place the model locally.\"\n",
    "            )\n",
    "        _ce_tokenizer = AutoTokenizer.from_pretrained(CROSS_ENCODER_MODEL_PATH, local_files_only=True)\n",
    "        _ce_model = AutoModelForSequenceClassification.from_pretrained(CROSS_ENCODER_MODEL_PATH, local_files_only=True)\n",
    "    else:\n",
    "        if os.path.isdir(CROSS_ENCODER_MODEL_PATH):\n",
    "            _ce_tokenizer = AutoTokenizer.from_pretrained(CROSS_ENCODER_MODEL_PATH)\n",
    "            _ce_model = AutoModelForSequenceClassification.from_pretrained(CROSS_ENCODER_MODEL_PATH)\n",
    "        else:\n",
    "            _ce_tokenizer = AutoTokenizer.from_pretrained(CROSS_ENCODER_MODEL)\n",
    "            _ce_model = AutoModelForSequenceClassification.from_pretrained(CROSS_ENCODER_MODEL)\n",
    "    _ce_model.eval()\n",
    "    if CE_DEVICE:\n",
    "        _ce_model.to(CE_DEVICE)\n",
    "\n",
    "def ce_score_pairs(pairs: List[Tuple[str, str]], batch_size: int = 32) -> List[float]:\n",
    "    load_cross_encoder()\n",
    "    device = CE_DEVICE or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    _ce_model.to(device)\n",
    "\n",
    "    scores = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(pairs), batch_size):\n",
    "            batch = pairs[i:i+batch_size]\n",
    "            inputs = _ce_tokenizer(\n",
    "                [q for q, _ in batch],\n",
    "                [d for _, d in batch],\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=512\n",
    "            )\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = _ce_model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            if logits.dim() == 1:\n",
    "                batch_scores = logits.detach().float().cpu().tolist()\n",
    "            else:\n",
    "                if logits.size(-1) == 1:\n",
    "                    batch_scores = logits.squeeze(-1).detach().float().cpu().tolist()\n",
    "                else:\n",
    "                    batch_scores = logits[:, -1].detach().float().cpu().tolist()\n",
    "            scores.extend(batch_scores)\n",
    "    return scores\n",
    "\n",
    "\n",
    "# --------------- Main: Adaptive λ + CE re-rank ---------------\n",
    "\n",
    "def main():\n",
    "    print(\"CWD:\", os.getcwd())\n",
    "\n",
    "    # Load CSVs\n",
    "    df_sparse = pd.read_csv(SPLADE_CSV)\n",
    "    df_dense = pd.read_csv(MPNET_CSV)\n",
    "\n",
    "    # Preserve originals\n",
    "    df_sparse[\"_q_orig\"] = df_sparse[\"question\"]\n",
    "    df_sparse[\"_a_orig\"] = df_sparse[\"answer\"]\n",
    "    df_sparse[\"_gt_orig\"] = df_sparse[\"groundtruth_docs\"]\n",
    "\n",
    "    df_dense[\"_q_orig\"] = df_dense[\"question\"]\n",
    "    df_dense[\"_a_orig\"] = df_dense[\"answer\"]\n",
    "    df_dense[\"_gt_orig\"] = df_dense[\"groundtruth_docs\"]\n",
    "\n",
    "    # Normalize join keys\n",
    "    def normalize_groundtruth_str(gt_field: Any) -> str:\n",
    "        data = safe_parse_list(gt_field)\n",
    "        if isinstance(data, list):\n",
    "            norm_items = [normalize_text(x) for x in data]\n",
    "            return json.dumps(norm_items, ensure_ascii=False)\n",
    "        return json.dumps([normalize_text(str(gt_field))], ensure_ascii=False)\n",
    "\n",
    "    df_sparse[\"_q_norm\"] = df_sparse[\"question\"].apply(normalize_text)\n",
    "    df_sparse[\"_a_norm\"] = df_sparse[\"answer\"].apply(normalize_text)\n",
    "    df_sparse[\"_gt_norm\"] = df_sparse[\"groundtruth_docs\"].apply(normalize_groundtruth_str)\n",
    "\n",
    "    df_dense[\"_q_norm\"] = df_dense[\"question\"].apply(normalize_text)\n",
    "    df_dense[\"_a_norm\"] = df_dense[\"answer\"].apply(normalize_text)\n",
    "    df_dense[\"_gt_norm\"] = df_dense[\"groundtruth_docs\"].apply(normalize_groundtruth_str)\n",
    "\n",
    "    # Merge\n",
    "    df = pd.merge(\n",
    "        df_sparse, df_dense,\n",
    "        on=[\"_q_norm\", \"_a_norm\", \"_gt_norm\"],\n",
    "        suffixes=(\"_splade\", \"_mpnet\"),\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    if df.empty:\n",
    "        raise ValueError(\"No rows matched after normalization. Check groundtruth formats across CSVs.\")\n",
    "\n",
    "    # Build an IDF dictionary from all candidate texts seen across rows\n",
    "    # Gather a corpus proxy from top lists to estimate DF robustly\n",
    "    doc_texts_for_idf = []\n",
    "    for _, row in df.iterrows():\n",
    "        splade_list = parse_ret_list(row[\"splade_ret_docs\"])[:TOP_K_PER_LIST]\n",
    "        mpnet_list  = parse_ret_list(row[\"mpnet_ret_docs\"])[:TOP_K_PER_LIST]\n",
    "        for d in splade_list:\n",
    "            if d.get(\"full_text\"):\n",
    "                doc_texts_for_idf.append(d[\"full_text\"])\n",
    "        for d in mpnet_list:\n",
    "            if d.get(\"full_text\"):\n",
    "                doc_texts_for_idf.append(d[\"full_text\"])\n",
    "    idf_dict = build_idf_dict(doc_texts_for_idf, min_df=MIN_DF, max_vocab=MAX_VOCAB)\n",
    "\n",
    "    # For normalization of avg_idf -> λ, we need rough bounds across queries\n",
    "    # Precompute average idf per query to estimate min/max\n",
    "    avg_idfs_all = []\n",
    "    q_texts = df[\"_q_orig_splade\"].tolist()\n",
    "    for q in q_texts:\n",
    "        avg_idfs_all.append(query_keywordiness_idf_avg(q, idf_dict))\n",
    "    # Handle degenerate case\n",
    "    if not avg_idfs_all:\n",
    "        avg_idfs_all = [0.0]\n",
    "    global_min_idf = float(min(avg_idfs_all))\n",
    "    global_max_idf = float(max(avg_idfs_all)) if max(avg_idfs_all) > global_min_idf else (global_min_idf + 1.0)\n",
    "\n",
    "    hybrid_ret_docs_col = []\n",
    "    map3_list, map5_list, map10_list = [], [], []\n",
    "    ndcg3_list, ndcg5_list, ndcg10_list = [], [], []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        # Parse inputs\n",
    "        splade_list = parse_ret_list(row[\"splade_ret_docs\"])[:TOP_K_PER_LIST]\n",
    "        mpnet_list  = parse_ret_list(row[\"mpnet_ret_docs\"])[:TOP_K_PER_LIST]\n",
    "        gt_list_raw = parse_groundtruth_list(row[\"_gt_orig_splade\"])\n",
    "        q_text = row[\"_q_orig_splade\"]\n",
    "\n",
    "        # Compute adaptive λ from query keywordiness\n",
    "        avg_idf = query_keywordiness_idf_avg(q_text, idf_dict)\n",
    "        lam_splade = adaptive_lambda_from_idf(avg_idf, global_min_idf, global_max_idf, LAMBDA_MIN, LAMBDA_MAX)\n",
    "        lam_mpnet = 1.0 - lam_splade\n",
    "        weights = [lam_splade, lam_mpnet]\n",
    "\n",
    "        # Fusion by score interpolation + COMBMNZ on union\n",
    "        fused_list = score_sum_fuse_weighted(\n",
    "            [splade_list, mpnet_list],\n",
    "            weights=weights,\n",
    "            normalize_per_list=SCORE_NORM_PER_LIST,\n",
    "            combmnz=USE_COMBMNZ\n",
    "        )\n",
    "\n",
    "        # Build candidate pool with full_texts\n",
    "        splade_map = {x[\"doc_id\"]: x for x in splade_list}\n",
    "        mpnet_map  = {x[\"doc_id\"]: x for x in mpnet_list}\n",
    "        candidates = []\n",
    "        for item in fused_list:\n",
    "            d = item[\"doc_id\"]\n",
    "            ft = splade_map.get(d, mpnet_map.get(d, {})).get(\"full_text\", \"\")\n",
    "            candidates.append({\n",
    "                \"doc_id\": d,\n",
    "                \"full_text\": ft,\n",
    "                \"pre_fused_score\": item[\"fused_interp\"]\n",
    "            })\n",
    "\n",
    "        # Truncate for CE\n",
    "        candidates = candidates[:CANDIDATE_POOL_SIZE]\n",
    "\n",
    "        # CE re-rank\n",
    "        pairs = [(q_text, c[\"full_text\"]) for c in candidates]\n",
    "        if pairs:\n",
    "            ce_scores = ce_score_pairs(pairs, batch_size=CE_BATCH_SIZE)\n",
    "        else:\n",
    "            ce_scores = []\n",
    "\n",
    "        for i, c in enumerate(candidates):\n",
    "            c[\"_ce_score\"] = float(ce_scores[i]) if i < len(ce_scores) else float(\"-inf\")\n",
    "\n",
    "        # Post-CE blending\n",
    "        if USE_POST_CE_BLENDING:\n",
    "            ce_vals = [c[\"_ce_score\"] for c in candidates]\n",
    "            pre_vals = [c[\"pre_fused_score\"] for c in candidates]\n",
    "            ce_norm = min_max_normalize(ce_vals)\n",
    "            pre_norm = min_max_normalize(pre_vals)\n",
    "            for i, c in enumerate(candidates):\n",
    "                c[\"_final_score\"] = ALPHA_CE_BLEND * ce_norm[i] + (1.0 - ALPHA_CE_BLEND) * pre_norm[i]\n",
    "        else:\n",
    "            for c in candidates:\n",
    "                c[\"_final_score\"] = c[\"_ce_score\"]\n",
    "\n",
    "        candidates.sort(key=lambda x: x[\"_final_score\"], reverse=True)\n",
    "        final = candidates[:FINAL_TOP_K]\n",
    "\n",
    "        # Output struct and metrics\n",
    "        hybrid_struct = []\n",
    "        pred_texts = []\n",
    "        for rnk, c in enumerate(final, start=1):\n",
    "            entry = {\n",
    "                \"doc_id\": c[\"doc_id\"],\n",
    "                \"score\": c[\"_final_score\"],\n",
    "                \"rank\": rnk,\n",
    "                \"full_text\": c[\"full_text\"],\n",
    "                \"fusion_mode\": \"adaptive_tfidf_interp -> CE(L12) -> blend\"\n",
    "            }\n",
    "            if LOG_RUN_META:\n",
    "                entry[\"meta\"] = {\n",
    "                    \"lambda_splade\": lam_splade,\n",
    "                    \"lambda_mpnet\": lam_mpnet,\n",
    "                    \"cand_pool\": CANDIDATE_POOL_SIZE,\n",
    "                    \"combmnz\": USE_COMBMNZ,\n",
    "                    \"ce_model\": CROSS_ENCODER_MODEL if not CE_LOCAL_ONLY else CROSS_ENCODER_MODEL_PATH,\n",
    "                    \"alpha_ce_blend\": ALPHA_CE_BLEND,\n",
    "                    \"avg_idf\": avg_idf\n",
    "                }\n",
    "            hybrid_struct.append(entry)\n",
    "            pred_texts.append(normalize_text(c[\"full_text\"]))\n",
    "\n",
    "        hybrid_ret_docs_col.append(json.dumps(hybrid_struct, ensure_ascii=False))\n",
    "\n",
    "        gt_norm_texts = [normalize_text(x) for x in gt_list_raw]\n",
    "        map3_list.append(apk(gt_norm_texts, pred_texts, 3))\n",
    "        map5_list.append(apk(gt_norm_texts, pred_texts, 5))\n",
    "        map10_list.append(apk(gt_norm_texts, pred_texts, 10))\n",
    "        ndcg3_list.append(ndcg_at_k(pred_texts, gt_norm_texts, 3))\n",
    "        ndcg5_list.append(ndcg_at_k(pred_texts, gt_norm_texts, 5))\n",
    "        ndcg10_list.append(ndcg_at_k(pred_texts, gt_norm_texts, 10))\n",
    "\n",
    "    # Output\n",
    "    out = pd.DataFrame({\n",
    "        \"question\": df[\"_q_orig_splade\"],\n",
    "        \"answer\": df[\"_a_orig_splade\"],\n",
    "        \"groundtruth_docs\": df[\"_gt_orig_splade\"],\n",
    "        \"splade_ret_docs\": df[\"splade_ret_docs\"],\n",
    "        \"mpnet_ret_docs\": df[\"mpnet_ret_docs\"],\n",
    "        \"hybrid_ret_docs\": hybrid_ret_docs_col,\n",
    "        \"MAP@3\": map3_list,\n",
    "        \"NDCG@3\": ndcg3_list,\n",
    "        \"MAP@5\": map5_list,\n",
    "        \"NDCG@5\": ndcg5_list,\n",
    "        \"MAP@10\": map10_list,\n",
    "        \"NDCG@10\": ndcg10_list,\n",
    "    })\n",
    "    cols = [\n",
    "        \"question\",\"answer\",\"groundtruth_docs\",\n",
    "        \"splade_ret_docs\",\"mpnet_ret_docs\",\"hybrid_ret_docs\",\n",
    "        \"MAP@3\",\"NDCG@3\",\"MAP@5\",\"NDCG@5\",\"MAP@10\",\"NDCG@10\"\n",
    "    ]\n",
    "    out = out[cols]\n",
    "    out.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    overall = {\n",
    "        \"MAP@3\": float(np.mean(map3_list)) if map3_list else 0.0,\n",
    "        \"NDCG@3\": float(np.mean(ndcg3_list)) if ndcg3_list else 0.0,\n",
    "        \"MAP@5\": float(np.mean(map5_list)) if map5_list else 0.0,\n",
    "        \"NDCG@5\": float(np.mean(ndcg5_list)) if ndcg5_list else 0.0,\n",
    "        \"MAP@10\": float(np.mean(map10_list)) if map10_list else 0.0,\n",
    "        \"NDCG@10\": float(np.mean(ndcg10_list)) if ndcg10_list else 0.0,\n",
    "    }\n",
    "    print(\"Saved:\", OUTPUT_CSV)\n",
    "    print(\"Pipeline: adaptive_tfidf_interp -> CE(L12) -> blend\")\n",
    "    print(\"λ band (splade):\", (LAMBDA_MIN, LAMBDA_MAX))\n",
    "    print(\"Candidate pool:\", CANDIDATE_POOL_SIZE, \"| FINAL_TOP_K:\", FINAL_TOP_K)\n",
    "    print(\"CE:\", CROSS_ENCODER_MODEL if not CE_LOCAL_ONLY else CROSS_ENCODER_MODEL_PATH)\n",
    "    print(\"Blend α:\", ALPHA_CE_BLEND, \"| COMBMNZ:\", USE_COMBMNZ)\n",
    "    print(\"Hybrid overall averages:\", overall)\n",
    "\n",
    "# Run\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid with modified metrics on HotpotQA (Current Working directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['question', 'answer', 'passage', 'groundtruth_docs', 'splade_ret_docs',\n",
       "       'passages_with_ids', 'groundtruth_with_ids', 'MAP@3', 'NDCG@3', 'MAP@5',\n",
       "       'NDCG@5', 'MAP@10', 'NDCG@10'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./prior_results/hotpotqa_splade_modified_metrics.csv\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['question', 'answer', 'passage', 'groundtruth_docs',\n",
       "       'all-mpnet-base-v2_ret_docs', 'passages_with_ids',\n",
       "       'groundtruth_with_ids', 'MAP@3', 'NDCG@3', 'MAP@5', 'NDCG@5', 'MAP@10',\n",
       "       'NDCG@10'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./prior_results/hotpotqa_mpnet_modified_metrics.csv\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged shape: (90447, 6)\n",
      "Saved to: hotpotqa_merged_splade_mpnet.csv\n",
      "                                                                                                                       question                  answer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           passages_with_ids                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           groundtruth_with_ids                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   splade_ret_docs                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     mpnet_ret_docs\n",
      "                                                         Which magazine was started first Arthur's Magazine or First for Women?       Arthur's Magazine                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           [{'doc_id': '0_0', 'full_text': \"arthur's magazine (1844–1846) was an american literary periodical published in philadelphia in the 19th century.\"}, {'doc_id': '0_1', 'full_text': 'edited by t.s. arthur, it featured work by edgar a. poe, j.h. ingraham, sarah josepha hale, thomas g. spear, and others.'}, {'doc_id': '0_2', 'full_text': \"in may 1846 it was merged into godey's lady's book.\"}, {'doc_id': '0_3', 'full_text': \"first for women is a woman's magazine published by bauer media group in the usa.\"}, {'doc_id': '0_4', 'full_text': 'the magazine was started in 1989.'}, {'doc_id': '0_5', 'full_text': 'it is based in englewood cliffs, new jersey.'}, {'doc_id': '0_6', 'full_text': 'in 2011 the circulation of the magazine was 1,310,696 copies.'}]                                                                                                                                                                                                                                                                                                                                       [{'doc_id': '0_0', 'full_text': \"arthur's magazine (1844–1846) was an american literary periodical published in philadelphia in the 19th century.\"}, {'doc_id': '0_3', 'full_text': \"first for women is a woman's magazine published by bauer media group in the usa.\"}]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    [{'doc_id': '8728_0', 'score': 23.7684, 'full_text': \"arthur's home magazine (1852-ca.1898) or ladies' home magazine was an american periodical published in philadelphia by timothy shay arthur.\", 'preview_snippet': \"arthur's home magazine (1852-ca.1898) or ladies' home magazine was an american periodical published in philadelphia by timothy shay arthur.\"}, {'doc_id': '0_0', 'score': 20.8886, 'full_text': \"arthur's magazine (1844–1846) was an american literary periodical published in philadelphia in the 19th century.\", 'preview_snippet': \"arthur's magazine (1844–1846) was an american literary periodical published in philadelphia in the 19th century.\"}, {'doc_id': '75860_0', 'score': 19.6315, 'full_text': \"the ladies' magazine, an early magazine for women, was first published in 1828 in boston, massachusetts.\", 'preview_snippet': \"the ladies' magazine, an early magazine for women, was first published in 1828 in boston, massachusetts.\"}, {'doc_id': '4162_0', 'score': 19.5296, 'full_text': \"gynaika magazine (greek γυναικα), first published on 1 february 1952 by evangelos terzopoulos publishing enterprises s.a., was the first greek women's magazine.\", 'preview_snippet': \"gynaika magazine (greek γυναικα), first published on 1 february 1952 by evangelos terzopoulos publishing enterprises s.a., was the first greek women's magazine.\"}, {'doc_id': '34407_0', 'score': 18.996, 'full_text': \"ettelā'āt-e bānuvān (persian: اطلاعات بانوان\\u200e \\u200e ) or banovan was the first women's magazine published in tehran.\", 'preview_snippet': \"ettelā'āt-e bānuvān (persian: اطلاعات بانوان\\u200e \\u200e ) or banovan was the first women's magazine published in tehran.\"}, {'doc_id': '56238_6', 'score': 18.8329, 'full_text': 'womensports magazine was the first magazine dedicated to women in sports.', 'preview_snippet': 'womensports magazine was the first magazine dedicated to women in sports.'}, {'doc_id': '30064_7', 'score': 18.4626, 'full_text': \"it first appeared on february 16, 1883, and eventually became one of the leading women's magazines of the 20th century in the united states.\", 'preview_snippet': \"it first appeared on february 16, 1883, and eventually became one of the leading women's magazines of the 20th century in the united states.\"}, {'doc_id': '51507_1', 'score': 18.1113, 'full_text': \"woman's realm was a british weekly women's magazine first published in 1958.\", 'preview_snippet': \"woman's realm was a british weekly women's magazine first published in 1958.\"}, {'doc_id': '0_3', 'score': 18.0135, 'full_text': \"first for women is a woman's magazine published by bauer media group in the usa.\", 'preview_snippet': \"first for women is a woman's magazine published by bauer media group in the usa.\"}, {'doc_id': '8834_1', 'score': 17.4967, 'full_text': \"formerly titled the cosmopolitan, the magazine was first published in 1886 in the united states as a family magazine; it was later transformed into a literary magazine and eventually became a women's magazine since 1965.\", 'preview_snippet': \"formerly titled the cosmopolitan, the magazine was first published in 1886 in the united states as a family magazine; it was later transformed into a literary magazine and eventually became a women's \"}]                                                                                                                                                                                                                                                                                                                                                 [{'doc_id': '8728_0', 'score': 0.744406, 'full_text': \"arthur's home magazine (1852-ca.1898) or ladies' home magazine was an american periodical published in philadelphia by timothy shay arthur.\", 'preview_snippet': \"arthur's home magazine (1852-ca.1898) or ladies' home magazine was an american periodical published in philadelphia by timothy shay arthur.\"}, {'doc_id': '30064_7', 'score': 0.664687, 'full_text': \"it first appeared on february 16, 1883, and eventually became one of the leading women's magazines of the 20th century in the united states.\", 'preview_snippet': \"it first appeared on february 16, 1883, and eventually became one of the leading women's magazines of the 20th century in the united states.\"}, {'doc_id': '0_0', 'score': 0.652942, 'full_text': \"arthur's magazine (1844–1846) was an american literary periodical published in philadelphia in the 19th century.\", 'preview_snippet': \"arthur's magazine (1844–1846) was an american literary periodical published in philadelphia in the 19th century.\"}, {'doc_id': '75860_0', 'score': 0.652174, 'full_text': \"the ladies' magazine, an early magazine for women, was first published in 1828 in boston, massachusetts.\", 'preview_snippet': \"the ladies' magazine, an early magazine for women, was first published in 1828 in boston, massachusetts.\"}, {'doc_id': '4434_3', 'score': 0.649145, 'full_text': 'was an american teen magazine first published by the laufer company in 1972 with editor/creator judy wieder and art director william cragun.', 'preview_snippet': 'was an american teen magazine first published by the laufer company in 1972 with editor/creator judy wieder and art director william cragun.'}, {'doc_id': '62857_4', 'score': 0.642792, 'full_text': 'it is the oldest and largest magazine for women in the country.', 'preview_snippet': 'it is the oldest and largest magazine for women in the country.'}, {'doc_id': '8834_1', 'score': 0.638451, 'full_text': \"formerly titled the cosmopolitan, the magazine was first published in 1886 in the united states as a family magazine; it was later transformed into a literary magazine and eventually became a women's magazine since 1965.\", 'preview_snippet': \"formerly titled the cosmopolitan, the magazine was first published in 1886 in the united states as a family magazine; it was later transformed into a literary magazine and eventually became a women's \"}, {'doc_id': '76497_3', 'score': 0.637633, 'full_text': 'magazine.', 'preview_snippet': 'magazine.'}, {'doc_id': '3556_3', 'score': 0.631953, 'full_text': 'the magazine was launched by mary baker eddy in 1898.', 'preview_snippet': 'the magazine was launched by mary baker eddy in 1898.'}, {'doc_id': '56238_6', 'score': 0.627356, 'full_text': 'womensports magazine was the first magazine dedicated to women in sports.', 'preview_snippet': 'womensports magazine was the first magazine dedicated to women in sports.'}]\n",
      "                                              The Oberoi family is part of a hotel company that has a head office in what city?                   Delhi                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   [{'doc_id': '1_0', 'full_text': 'the oberoi family is an indian family that is famous for its involvement in hotels, namely through the oberoi group.'}, {'doc_id': '1_1', 'full_text': 'the oberoi group is a hotel company with its head office in delhi.'}, {'doc_id': '1_2', 'full_text': 'founded in 1934, the company owns and/or operates 30+ luxury hotels and two river cruise ships in six countries, primarily under its oberoi hotels & resorts and trident hotels brands.'}]                                                                                                                                                                                                                                                                                                                                                 [{'doc_id': '1_0', 'full_text': 'the oberoi family is an indian family that is famous for its involvement in hotels, namely through the oberoi group.'}, {'doc_id': '1_1', 'full_text': 'the oberoi group is a hotel company with its head office in delhi.'}]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  [{'doc_id': '1_1', 'score': 27.0939, 'full_text': 'the oberoi group is a hotel company with its head office in delhi.', 'preview_snippet': 'the oberoi group is a hotel company with its head office in delhi.'}, {'doc_id': '1_0', 'score': 25.6576, 'full_text': 'the oberoi family is an indian family that is famous for its involvement in hotels, namely through the oberoi group.', 'preview_snippet': 'the oberoi family is an indian family that is famous for its involvement in hotels, namely through the oberoi group.'}, {'doc_id': '1_2', 'score': 17.4786, 'full_text': 'founded in 1934, the company owns and/or operates 30+ luxury hotels and two river cruise ships in six countries, primarily under its oberoi hotels & resorts and trident hotels brands.', 'preview_snippet': 'founded in 1934, the company owns and/or operates 30+ luxury hotels and two river cruise ships in six countries, primarily under its oberoi hotels & resorts and trident hotels brands.'}, {'doc_id': '51892_9', 'score': 13.1005, 'full_text': 'nakuul mehta, kunal jaisingh and leenesh mattoo respectively portray shivaay, omkara and rudra, the three heirs of the oberoi family.', 'preview_snippet': 'nakuul mehta, kunal jaisingh and leenesh mattoo respectively portray shivaay, omkara and rudra, the three heirs of the oberoi family.'}, {'doc_id': '25436_7', 'score': 12.7594, 'full_text': \"the company's head office is in toronto, ontario.\", 'preview_snippet': \"the company's head office is in toronto, ontario.\"}, {'doc_id': '60619_5', 'score': 12.6775, 'full_text': \"the company's head office is located in the 15th arrondissement of paris.\", 'preview_snippet': \"the company's head office is located in the 15th arrondissement of paris.\"}, {'doc_id': '44064_4', 'score': 12.075, 'full_text': 'it forms part of the intercontinental hotels group family of brands, which include intercontinental hotels & resorts and holiday inn hotels & resorts, and operates in 52 countries with more than 400 hotels, usually located in city centers, resorts, coastal towns or near major airports.', 'preview_snippet': 'it forms part of the intercontinental hotels group family of brands, which include intercontinental hotels & resorts and holiday inn hotels & resorts, and operates in 52 countries with more than 400 h'}, {'doc_id': '15566_7', 'score': 11.7909, 'full_text': 'its head office is located on 55 water street in lower manhattan, new york city.', 'preview_snippet': 'its head office is located on 55 water street in lower manhattan, new york city.'}, {'doc_id': '81857_2', 'score': 11.097, 'full_text': 'the company has its registered office in kolkata and head office in mumbai with presence in twenty-five cities across three countries.', 'preview_snippet': 'the company has its registered office in kolkata and head office in mumbai with presence in twenty-five cities across three countries.'}, {'doc_id': '67824_2', 'score': 11.0129, 'full_text': 'she is best known for her satirical column called erratica in the newspaper and as the author of the best selling title dare to dream: a life of m.s. oberoi.', 'preview_snippet': 'she is best known for her satirical column called erratica in the newspaper and as the author of the best selling title dare to dream: a life of m.s. oberoi.'}]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              [{'doc_id': '1_0', 'score': 0.877031, 'full_text': 'the oberoi family is an indian family that is famous for its involvement in hotels, namely through the oberoi group.', 'preview_snippet': 'the oberoi family is an indian family that is famous for its involvement in hotels, namely through the oberoi group.'}, {'doc_id': '1_1', 'score': 0.853292, 'full_text': 'the oberoi group is a hotel company with its head office in delhi.', 'preview_snippet': 'the oberoi group is a hotel company with its head office in delhi.'}, {'doc_id': '1_2', 'score': 0.713862, 'full_text': 'founded in 1934, the company owns and/or operates 30+ luxury hotels and two river cruise ships in six countries, primarily under its oberoi hotels & resorts and trident hotels brands.', 'preview_snippet': 'founded in 1934, the company owns and/or operates 30+ luxury hotels and two river cruise ships in six countries, primarily under its oberoi hotels & resorts and trident hotels brands.'}, {'doc_id': '65312_3', 'score': 0.597352, 'full_text': 'the hotel is managed by intercontinental hotels group.', 'preview_snippet': 'the hotel is managed by intercontinental hotels group.'}, {'doc_id': '30171_1', 'score': 0.588978, 'full_text': 'it is the 3rd largest hotel chain in india with 75 hotels across india and overseas.', 'preview_snippet': 'it is the 3rd largest hotel chain in india with 75 hotels across india and overseas.'}, {'doc_id': '5629_4', 'score': 0.576686, 'full_text': 'the management of the hotel is delegated to hoteliers from india and abroad.', 'preview_snippet': 'the management of the hotel is delegated to hoteliers from india and abroad.'}, {'doc_id': '3481_5', 'score': 0.570338, 'full_text': 'the hotel chain is based in atlanta, georgia.', 'preview_snippet': 'the hotel chain is based in atlanta, georgia.'}, {'doc_id': '35682_4', 'score': 0.558499, 'full_text': 'it operates more than 100 hotels worldwide.', 'preview_snippet': 'it operates more than 100 hotels worldwide.'}, {'doc_id': '177_4', 'score': 0.534635, 'full_text': 'it is part of a series of office blocks and hotels.', 'preview_snippet': 'it is part of a series of office blocks and hotels.'}, {'doc_id': '5629_2', 'score': 0.534539, 'full_text': 'the hotel is part of the intercontinental hotels group, an international hotel chain hotel founded in 1946.', 'preview_snippet': 'the hotel is part of the intercontinental hotels group, an international hotel chain hotel founded in 1946.'}]\n",
      "Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who? President Richard Nixon [{'doc_id': '2_0', 'full_text': 'allison beth allie goertz (born march 2, 1991) is an american musician.'}, {'doc_id': '2_1', 'full_text': 'goertz is known for her satirical songs based on various pop culture topics.'}, {'doc_id': '2_2', 'full_text': 'her videos are posted on youtube under the name of cossbysweater.'}, {'doc_id': '2_3', 'full_text': 'subjects of her songs have included the film the room, the character milhouse from the television show the simpsons, and the game dungeons & dragons.'}, {'doc_id': '2_4', 'full_text': 'her style has been compared to that of bo burnham.'}, {'doc_id': '2_5', 'full_text': \"in december 2015, goertz released a concept album based on the adult swim series rick and morty, sad dance songs, with the album's cover emulating the animation and logo of the series.\"}, {'doc_id': '2_6', 'full_text': 'the album was made possible through kickstarter.'}, {'doc_id': '2_7', 'full_text': \"she is co-host of everything's coming up podcast, a simpsons-focused podcast along with julia prescott.\"}, {'doc_id': '2_8', 'full_text': \"milhouse mussolini van houten is a fictional character featured in the animated television series the simpsons, voiced by pamela hayden, and created by matt groening who named the character after president richard nixon's middle name.\"}, {'doc_id': '2_9', 'full_text': 'later in the series, it is revealed that milhouse\\'s middle name is \"mussolini.'}] [{'doc_id': '2_0', 'full_text': 'allison beth allie goertz (born march 2, 1991) is an american musician.'}, {'doc_id': '2_1', 'full_text': 'goertz is known for her satirical songs based on various pop culture topics.'}, {'doc_id': '2_2', 'full_text': 'her videos are posted on youtube under the name of cossbysweater.'}, {'doc_id': '2_8', 'full_text': \"milhouse mussolini van houten is a fictional character featured in the animated television series the simpsons, voiced by pamela hayden, and created by matt groening who named the character after president richard nixon's middle name.\"}] [{'doc_id': '2_8', 'score': 35.9644, 'full_text': \"milhouse mussolini van houten is a fictional character featured in the animated television series the simpsons, voiced by pamela hayden, and created by matt groening who named the character after president richard nixon's middle name.\", 'preview_snippet': 'milhouse mussolini van houten is a fictional character featured in the animated television series the simpsons, voiced by pamela hayden, and created by matt groening who named the character after pres'}, {'doc_id': '11400_2', 'score': 25.6403, 'full_text': 'the simpsons is an american animated sitcom, aimed at adolescents and adults, created by matt groening for the fox broadcasting company.', 'preview_snippet': 'the simpsons is an american animated sitcom, aimed at adolescents and adults, created by matt groening for the fox broadcasting company.'}, {'doc_id': '23126_9', 'score': 25.1595, 'full_text': 'the simpsons creator matt groening made a minor uncredited cameo appearance as a sports commentator shouting goal!', 'preview_snippet': 'the simpsons creator matt groening made a minor uncredited cameo appearance as a sports commentator shouting goal!'}, {'doc_id': '25975_8', 'score': 22.8034, 'full_text': 'they live at 742 evergreen terrace in the fictional town of springfield, united states and they were created by cartoonist matt groening who conceived the characters after his own family members, substituting bart for his own name.', 'preview_snippet': 'they live at 742 evergreen terrace in the fictional town of springfield, united states and they were created by cartoonist matt groening who conceived the characters after his own family members, subs'}, {'doc_id': '38860_2', 'score': 22.2046, 'full_text': 'the series was created by matt groening, who designed the simpson family and wrote many of the shorts.', 'preview_snippet': 'the series was created by matt groening, who designed the simpson family and wrote many of the shorts.'}, {'doc_id': '2_0', 'score': 21.3562, 'full_text': 'allison beth allie goertz (born march 2, 1991) is an american musician.', 'preview_snippet': 'allison beth allie goertz (born march 2, 1991) is an american musician.'}, {'doc_id': '87348_0', 'score': 21.0114, 'full_text': 'zongo comics was founded and published in 1995 by simpsons and futurama creator matt groening.', 'preview_snippet': 'zongo comics was founded and published in 1995 by simpsons and futurama creator matt groening.'}, {'doc_id': '13322_7', 'score': 20.9613, 'full_text': 'the series was envisioned by groening in the mid-1990s while working on the simpsons; he later brought david x. cohen aboard to develop storylines and characters to pitch the show to fox.', 'preview_snippet': 'the series was envisioned by groening in the mid-1990s while working on the simpsons; he later brought david x. cohen aboard to develop storylines and characters to pitch the show to fox.'}, {'doc_id': '2_3', 'score': 20.3457, 'full_text': 'subjects of her songs have included the film the room, the character milhouse from the television show the simpsons, and the game dungeons & dragons.', 'preview_snippet': 'subjects of her songs have included the film the room, the character milhouse from the television show the simpsons, and the game dungeons & dragons.'}, {'doc_id': '23126_4', 'score': 19.5651, 'full_text': 'in this futuristic installment, bart goes to a clinic to rid himself of his feelings for his ex-wife jenda (who is now dating a xenomorph-like alien named jerry), lisa must choose whether or not to cure her zombie husband milhouse after he gets bitten by a homeless zombie, and marge (after putting up with years of homer dying and being cloned back to life by professor frink) loads homer onto a flatscreen monitor and throws him out of the house.', 'preview_snippet': 'in this futuristic installment, bart goes to a clinic to rid himself of his feelings for his ex-wife jenda (who is now dating a xenomorph-like alien named jerry), lisa must choose whether or not to cu'}] [{'doc_id': '2_8', 'score': 0.564879, 'full_text': \"milhouse mussolini van houten is a fictional character featured in the animated television series the simpsons, voiced by pamela hayden, and created by matt groening who named the character after president richard nixon's middle name.\", 'preview_snippet': 'milhouse mussolini van houten is a fictional character featured in the animated television series the simpsons, voiced by pamela hayden, and created by matt groening who named the character after pres'}, {'doc_id': '50253_9', 'score': 0.55953, 'full_text': 'he named the elder simpson daughter after his younger sister lisa groening.', 'preview_snippet': 'he named the elder simpson daughter after his younger sister lisa groening.'}, {'doc_id': '38860_2', 'score': 0.559434, 'full_text': 'the series was created by matt groening, who designed the simpson family and wrote many of the shorts.', 'preview_snippet': 'the series was created by matt groening, who designed the simpson family and wrote many of the shorts.'}, {'doc_id': '51_5', 'score': 0.550252, 'full_text': 'he is best known for his work scoring many episodes of the simpsons, of which he had been the sole composer between 1990 and 2017.', 'preview_snippet': 'he is best known for his work scoring many episodes of the simpsons, of which he had been the sole composer between 1990 and 2017.'}, {'doc_id': '12939_1', 'score': 0.545625, 'full_text': \"ventimilia co-wrote the simpsons episode simpson tide (with joshua sternin) and the teleplay of the episode 'round springfield, based on a story idea by al jean and mike reiss.\", 'preview_snippet': \"ventimilia co-wrote the simpsons episode simpson tide (with joshua sternin) and the teleplay of the episode 'round springfield, based on a story idea by al jean and mike reiss.\"}, {'doc_id': '23126_9', 'score': 0.536259, 'full_text': 'the simpsons creator matt groening made a minor uncredited cameo appearance as a sports commentator shouting goal!', 'preview_snippet': 'the simpsons creator matt groening made a minor uncredited cameo appearance as a sports commentator shouting goal!'}, {'doc_id': '25975_4', 'score': 0.529221, 'full_text': 'he named the character after his father, homer groening.', 'preview_snippet': 'he named the character after his father, homer groening.'}, {'doc_id': '2_3', 'score': 0.528068, 'full_text': 'subjects of her songs have included the film the room, the character milhouse from the television show the simpsons, and the game dungeons & dragons.', 'preview_snippet': 'subjects of her songs have included the film the room, the character milhouse from the television show the simpsons, and the game dungeons & dragons.'}, {'doc_id': '32289_9', 'score': 0.527242, 'full_text': \"marge was created and designed by cartoonist matt groening while he was waiting in the lobby of james l. brooks' office.\", 'preview_snippet': \"marge was created and designed by cartoonist matt groening while he was waiting in the lobby of james l. brooks' office.\"}, {'doc_id': '2_1', 'score': 0.517244, 'full_text': 'goertz is known for her satirical songs based on various pop culture topics.', 'preview_snippet': 'goertz is known for her satirical songs based on various pop culture topics.'}]\n"
     ]
    }
   ],
   "source": [
    "# Merging sparse and dense as single file for passing into hybrid \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# ------------- Load -------------\n",
    "splade_path = './prior_results/hotpotqa_splade_modified_metrics.csv'\n",
    "mpnet_path  = './prior_results/hotpotqa_mpnet_modified_metrics.csv'\n",
    "\n",
    "splade_df = pd.read_csv(splade_path)\n",
    "mpnet_df  = pd.read_csv(mpnet_path)\n",
    "\n",
    "# Normalize column name if needed\n",
    "if 'all-mpnet-base-v2_ret_docs' in mpnet_df.columns:\n",
    "    mpnet_df = mpnet_df.rename(columns={'all-mpnet-base-v2_ret_docs': 'mpnet_ret_docs'})\n",
    "\n",
    "# ------------- Keep only necessary columns -------------\n",
    "splade_keep = ['question', 'answer', 'splade_ret_docs', 'passages_with_ids', 'groundtruth_with_ids']\n",
    "mpnet_keep  = ['mpnet_ret_docs', 'passages_with_ids', 'groundtruth_with_ids']\n",
    "\n",
    "# Sanity checks\n",
    "missing_splade = set(splade_keep) - set(splade_df.columns)\n",
    "missing_mpnet  = set(mpnet_keep) - set(mpnet_df.columns)\n",
    "if missing_splade:\n",
    "    raise ValueError(f\"Missing columns in splade_df: {missing_splade}\")\n",
    "if missing_mpnet:\n",
    "    raise ValueError(f\"Missing columns in mpnet_df: {missing_mpnet}\")\n",
    "\n",
    "splade_df = splade_df[splade_keep].copy()\n",
    "mpnet_df  = mpnet_df[mpnet_keep].copy()\n",
    "\n",
    "# ------------- Merge -------------\n",
    "# If you expect exactly one mpnet row per key, validate='m:1' will catch duplicates on the right.\n",
    "merged = pd.merge(\n",
    "    splade_df,\n",
    "    mpnet_df,\n",
    "    on=['passages_with_ids', 'groundtruth_with_ids'],\n",
    "    how='inner',\n",
    "    validate='m:1'\n",
    ")\n",
    "\n",
    "# ------------- Final selection and save -------------\n",
    "final_cols = ['question', 'answer', 'passages_with_ids', 'groundtruth_with_ids', 'splade_ret_docs', 'mpnet_ret_docs']\n",
    "merged = merged[final_cols]\n",
    "\n",
    "out_path = 'hotpotqa_merged_splade_mpnet.csv'\n",
    "merged.to_csv(out_path, index=False)\n",
    "\n",
    "print('Merged shape:', merged.shape)\n",
    "print('Saved to:', out_path)\n",
    "print(merged.head(3).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid on HotpotQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 80044 rows to hotpotqa_hybrid_rrf_results.csv\n",
      "Averages:\n",
      "MAP: 0.285133\n",
      "NDCG: 0.365928\n",
      "MAP@3: 0.256652\n",
      "NDCG@3: 0.322585\n",
      "MAP@5: 0.273515\n",
      "NDCG@5: 0.342596\n",
      "MAP@10: 0.282458\n",
      "NDCG@10: 0.358922\n"
     ]
    }
   ],
   "source": [
    "# Hybrid + vanilla RRF fusion + pytrec_eval evaluation\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import pytrec_eval  # make sure available\n",
    "\n",
    "CUTS = (3, 5, 10)\n",
    "\n",
    "# ------------- Load + prep (same as before) -------------\n",
    "splade_df = pd.read_csv('./prior_results/hotpotqa_splade_modified_metrics.csv')\n",
    "mpnet_df = pd.read_csv('./prior_results/hotpotqa_mpnet_modified_metrics.csv')\n",
    "\n",
    "if 'all-mpnet-base-v2_ret_docs' in mpnet_df.columns:\n",
    "    mpnet_df = mpnet_df.rename(columns={'all-mpnet-base-v2_ret_docs': 'mpnet_ret_docs'})\n",
    "\n",
    "splade_keep = ['question', 'answer', 'splade_ret_docs', 'passages_with_ids', 'groundtruth_with_ids']\n",
    "mpnet_keep  = ['question', 'answer', 'mpnet_ret_docs', 'passages_with_ids', 'groundtruth_with_ids']\n",
    "\n",
    "splade_df = splade_df[splade_keep].copy()\n",
    "mpnet_df  = mpnet_df[mpnet_keep].copy()\n",
    "\n",
    "def lower_str(x):\n",
    "    if pd.isna(x):\n",
    "        return x\n",
    "    return str(x).lower()\n",
    "\n",
    "for col in ['question', 'answer', 'passages_with_ids', 'groundtruth_with_ids']:\n",
    "    if col in splade_df.columns:\n",
    "        splade_df[col] = splade_df[col].apply(lower_str)\n",
    "    if col in mpnet_df.columns:\n",
    "        mpnet_df[col] = mpnet_df[col].apply(lower_str)\n",
    "\n",
    "merged = pd.merge(\n",
    "    mpnet_df, splade_df,\n",
    "    on=['question', 'answer', 'passages_with_ids', 'groundtruth_with_ids'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# ------------- Parsing helpers (hardened) -------------\n",
    "def safe_load(x):\n",
    "    if isinstance(x, (list, dict)):\n",
    "        return x\n",
    "    if pd.isna(x):\n",
    "        return None\n",
    "    s = str(x)\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        try:\n",
    "            import ast\n",
    "            return ast.literal_eval(s)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "def normalize_doc_id(x):\n",
    "    return None if x is None else str(x).strip().lower()\n",
    "\n",
    "def to_ranked_dict(items):\n",
    "    d = {}\n",
    "    if not items:\n",
    "        return d\n",
    "    for i, it in enumerate(items):\n",
    "        doc_id, rank, score = None, i+1, None\n",
    "        if isinstance(it, (str, int)):\n",
    "            doc_id = normalize_doc_id(it)\n",
    "        elif isinstance(it, dict):\n",
    "            doc_id = it.get('doc_id') or it.get('id') or it.get('docid')\n",
    "            if doc_id is None and len(it) == 1:\n",
    "                k, v = next(iter(it.items()))\n",
    "                doc_id = k\n",
    "                score = v if isinstance(v, (int, float, str)) else None\n",
    "            else:\n",
    "                rank = it.get('rank', rank)\n",
    "                score = it.get('score', it.get('sim'))\n",
    "            doc_id = normalize_doc_id(doc_id)\n",
    "        if not doc_id:\n",
    "            continue\n",
    "        try:\n",
    "            rank = int(rank)\n",
    "        except Exception:\n",
    "            rank = i+1\n",
    "        try:\n",
    "            score = float(score) if score is not None else None\n",
    "        except Exception:\n",
    "            score = None\n",
    "        d[doc_id] = (rank, score)\n",
    "    return d\n",
    "\n",
    "def to_qrels_docids_only(gt_items):\n",
    "    rel = {}\n",
    "    if not gt_items:\n",
    "        return rel\n",
    "    for it in gt_items:\n",
    "        candidate = None\n",
    "        if isinstance(it, dict):\n",
    "            candidate = it.get('doc_id') or it.get('id') or it.get('docid')\n",
    "        elif isinstance(it, (str, int)):\n",
    "            candidate = it\n",
    "        did = normalize_doc_id(candidate)\n",
    "        if did:\n",
    "            rel[did] = 1\n",
    "    return rel\n",
    "\n",
    "def rrf_fusion(rank_dicts, k=60):\n",
    "    scores = defaultdict(float)\n",
    "    components = defaultdict(list)\n",
    "    for src_idx, d in enumerate(rank_dicts):\n",
    "        for doc_id, (rank, _score) in d.items():\n",
    "            if not doc_id:\n",
    "                continue\n",
    "            scores[doc_id] += 1.0 / (k + rank)\n",
    "            components[doc_id].append({'source': src_idx, 'rank': rank})\n",
    "    sorted_docs = sorted(scores.items(), key=lambda x: (-x[1], x[0]))\n",
    "    fused = []\n",
    "    for i, (doc_id, sc) in enumerate(sorted_docs, start=1):\n",
    "        fused.append({'doc_id': doc_id, 'score': sc, 'rank': i, 'components': components[doc_id]})\n",
    "    return fused\n",
    "\n",
    "# ------------- Build runs + qrels for pytrec_eval -------------\n",
    "results_by_qid = {}  # qid -> {doc_id: score}\n",
    "qrels_by_qid = {}    # qid -> {doc_id: relevance}\n",
    "rows_out = []\n",
    "\n",
    "for q_idx, row in merged.iterrows():\n",
    "    mpnet_items = safe_load(row['mpnet_ret_docs'])\n",
    "    splade_items = safe_load(row['splade_ret_docs'])\n",
    "    gt_items = safe_load(row['groundtruth_with_ids'])\n",
    "\n",
    "    mpnet_ranked = to_ranked_dict(mpnet_items)\n",
    "    splade_ranked = to_ranked_dict(splade_items)\n",
    "    fused = rrf_fusion([mpnet_ranked, splade_ranked], k=60)\n",
    "\n",
    "    for e in fused:\n",
    "        e['doc_id'] = normalize_doc_id(e['doc_id'])\n",
    "\n",
    "    qid = f\"q_{q_idx}\"\n",
    "\n",
    "    run_dict = {e['doc_id']: float(e['score']) for e in fused if e['doc_id'] is not None}\n",
    "    results_by_qid[qid] = run_dict\n",
    "\n",
    "    qrels = to_qrels_docids_only(gt_items)\n",
    "    qrels_by_qid[qid] = {doc_id: int(rel) for doc_id, rel in qrels.items()}\n",
    "\n",
    "    rows_out.append({\n",
    "        'qid': qid,\n",
    "        'question': row['question'],\n",
    "        'answer': row['answer'],\n",
    "        'mpnet_ret_docs': mpnet_items,\n",
    "        'splade_ret_docs': splade_items,\n",
    "        'hybrid_ret_docs': fused,\n",
    "        'passages_with_ids': row['passages_with_ids'],\n",
    "        'groundtruth_with_ids': gt_items\n",
    "    })\n",
    "\n",
    "# ------------- Evaluate with pytrec_eval (request all cuts!) -------------\n",
    "metric_keys = {\"map\", \"ndcg\"} | {f\"map_cut_{k}\" for k in CUTS} | {f\"ndcg_cut_{k}\" for k in CUTS}\n",
    "evaluator = pytrec_eval.RelevanceEvaluator(qrels_by_qid, metric_keys)\n",
    "eval_res = evaluator.evaluate(results_by_qid)\n",
    "\n",
    "def metric_at(pv, metric_base, k):\n",
    "    key = f\"{metric_base}_{k}\"  # e.g., 'map_cut_3'\n",
    "    return float(pv.get(key, 0.0))\n",
    "\n",
    "metrics_rows = []\n",
    "for r in rows_out:\n",
    "    qid = r['qid']\n",
    "    pv = eval_res.get(qid, {})\n",
    "    metrics = {\n",
    "        'MAP':    float(pv.get('map', 0.0)),\n",
    "        'NDCG':   float(pv.get('ndcg', 0.0)),\n",
    "        'MAP@3':  round(metric_at(pv, 'map_cut', 3), 6),\n",
    "        'NDCG@3': round(metric_at(pv, 'ndcg_cut', 3), 6),\n",
    "        'MAP@5':  round(metric_at(pv, 'map_cut', 5), 6),\n",
    "        'NDCG@5': round(metric_at(pv, 'ndcg_cut', 5), 6),\n",
    "        'MAP@10': round(metric_at(pv, 'map_cut', 10), 6),\n",
    "        'NDCG@10':round(metric_at(pv, 'ndcg_cut', 10), 6),\n",
    "    }\n",
    "    metrics_rows.append({**r, **metrics})\n",
    "\n",
    "hybrid_df = pd.DataFrame(metrics_rows)\n",
    "hybrid_df = hybrid_df[['question', 'answer', 'mpnet_ret_docs', 'splade_ret_docs',\n",
    "                       'hybrid_ret_docs', 'passages_with_ids', 'groundtruth_with_ids',\n",
    "                       'MAP', 'NDCG', 'MAP@3', 'NDCG@3', 'MAP@5', 'NDCG@5', 'MAP@10', 'NDCG@10']]\n",
    "\n",
    "hybrid_df.to_csv('hotpotqa_hybrid_rrf_results.csv', index=False)\n",
    "print(f\"Saved {len(hybrid_df)} rows to hotpotqa_hybrid_rrf_results.csv\")\n",
    "\n",
    "# Print averages\n",
    "avg_metrics = hybrid_df[['MAP','NDCG','MAP@3','NDCG@3','MAP@5','NDCG@5','MAP@10','NDCG@10']].mean().round(6)\n",
    "print('Averages:')\n",
    "for m in ['MAP','NDCG','MAP@3','NDCG@3','MAP@5','NDCG@5','MAP@10','NDCG@10']:\n",
    "    print(f\"{m}: {avg_metrics[m]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged shape: (90447, 6)\n",
      "Saved 90447 rows to hotpotqa_hybrid_linear_results_sparse_0.7.csv\n",
      "Averages (alpha=0.30):\n",
      "MAP: 0.304735\n",
      "NDCG: 0.382653\n",
      "MAP@3: 0.282216\n",
      "NDCG@3: 0.3501\n",
      "MAP@5: 0.294928\n",
      "NDCG@5: 0.362942\n",
      "MAP@10: 0.302289\n",
      "NDCG@10: 0.376265\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Hybrid(Splade + Mpnet) with Linear interpolation (LI) updated\n",
    "\n",
    "the fusion step: we normalize scores per source list and combine them as\n",
    "fused_score = alpha * mpnet_score + (1 - alpha) * splade_score\n",
    "\n",
    "Adds: full_text field to each item in hybrid_ret_docs by building a doc_id -> text map\n",
    "(prefer passages_with_ids; fallback to mpnet/splade lists).\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import pytrec_eval  # make sure available\n",
    "\n",
    "CUTS = (3, 5, 10)\n",
    "ALPHA = 0.3  # weight for mpnet; (1-ALPHA) is weight for splade\n",
    "\n",
    "# ------------- Load (as provided) -------------\n",
    "splade_path = './prior_results/hotpotqa_splade_modified_metrics.csv'\n",
    "mpnet_path  = './prior_results/hotpotqa_mpnet_modified_metrics.csv'\n",
    "\n",
    "splade_df = pd.read_csv(splade_path)\n",
    "mpnet_df  = pd.read_csv(mpnet_path)\n",
    "\n",
    "# Normalize column name if needed\n",
    "if 'all-mpnet-base-v2_ret_docs' in mpnet_df.columns:\n",
    "    mpnet_df = mpnet_df.rename(columns={'all-mpnet-base-v2_ret_docs': 'mpnet_ret_docs'})\n",
    "\n",
    "# ------------- Keep only necessary columns (as provided) -------------\n",
    "splade_keep = ['question', 'answer', 'splade_ret_docs', 'passages_with_ids', 'groundtruth_with_ids']\n",
    "mpnet_keep  = ['mpnet_ret_docs', 'passages_with_ids', 'groundtruth_with_ids']\n",
    "\n",
    "# Sanity checks\n",
    "missing_splade = set(splade_keep) - set(splade_df.columns)\n",
    "missing_mpnet  = set(mpnet_keep) - set(mpnet_df.columns)\n",
    "if missing_splade:\n",
    "    raise ValueError(f\"Missing columns in splade_df: {missing_splade}\")\n",
    "if missing_mpnet:\n",
    "    raise ValueError(f\"Missing columns in mpnet_df: {missing_mpnet}\")\n",
    "\n",
    "splade_df = splade_df[splade_keep].copy()\n",
    "mpnet_df  = mpnet_df[mpnet_keep].copy()\n",
    "\n",
    "# ------------- Merge (as provided) -------------\n",
    "# If you expect exactly one mpnet row per key, validate='m:1' will catch duplicates on the right.\n",
    "merged = pd.merge(\n",
    "    splade_df,\n",
    "    mpnet_df,\n",
    "    on=['passages_with_ids', 'groundtruth_with_ids'],\n",
    "    how='inner',\n",
    "    validate='m:1'\n",
    ")\n",
    "\n",
    "# ------------- Final selection (as provided) -------------\n",
    "final_cols = ['question', 'answer', 'passages_with_ids', 'groundtruth_with_ids', 'splade_ret_docs', 'mpnet_ret_docs']\n",
    "merged = merged[final_cols]\n",
    "print('Merged shape:', merged.shape)\n",
    "\n",
    "# ------------- Parsing helpers (same as before) -------------\n",
    "def safe_load(x):\n",
    "    if isinstance(x, (list, dict)):\n",
    "        return x\n",
    "    if pd.isna(x):\n",
    "        return None\n",
    "    s = str(x)\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        try:\n",
    "            import ast\n",
    "            return ast.literal_eval(s)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "def normalize_doc_id(x):\n",
    "    return None if x is None else str(x).strip().lower()\n",
    "\n",
    "def to_score_dict(items):\n",
    "    \"\"\"\n",
    "    Convert retrieval list into dict: doc_id -> score (float).\n",
    "    Accepts dicts containing {'doc_id', 'score'}; ignores items without both.\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    if not items:\n",
    "        return d\n",
    "    for it in items:\n",
    "        if not isinstance(it, dict):\n",
    "            continue\n",
    "        did = normalize_doc_id(it.get('doc_id') or it.get('id') or it.get('docid'))\n",
    "        if not did:\n",
    "            continue\n",
    "        score = it.get('score', it.get('sim'))\n",
    "        try:\n",
    "            score = float(score)\n",
    "        except Exception:\n",
    "            continue\n",
    "        d[did] = score\n",
    "    return d\n",
    "\n",
    "def min_max_norm(d):\n",
    "    \"\"\"\n",
    "    Min-max normalize a dict of {doc_id: score} per query.\n",
    "    If all scores equal or empty, returns zeros.\n",
    "    \"\"\"\n",
    "    if not d:\n",
    "        return {}\n",
    "    vals = list(d.values())\n",
    "    vmin, vmax = min(vals), max(vals)\n",
    "    if vmax == vmin:\n",
    "        # return zeros so that fusion still works deterministically\n",
    "        return {k: 0.0 for k in d}\n",
    "    return {k: (v - vmin) / (vmax - vmin) for k, v in d.items()}\n",
    "\n",
    "def fuse_linear(mpnet_scores, splade_scores, alpha=0.5, normalize=True):\n",
    "    \"\"\"\n",
    "    Linear interpolation fusion:\n",
    "      fused = alpha * mpnet + (1 - alpha) * splade\n",
    "    - If normalize, scores are min-max normalized per source before fusion.\n",
    "    - Missing docs in one list get score 0 from that source.\n",
    "    Returns list of dicts: {'doc_id', 'score'} sorted by score desc with rank.\n",
    "    \"\"\"\n",
    "    m = mpnet_scores or {}\n",
    "    s = splade_scores or {}\n",
    "    if normalize:\n",
    "        m = min_max_norm(m)\n",
    "        s = min_max_norm(s)\n",
    "    all_ids = set(m.keys()) | set(s.keys())\n",
    "    fused = {}\n",
    "    for did in all_ids:\n",
    "        ms = m.get(did, 0.0)\n",
    "        ss = s.get(did, 0.0)\n",
    "        fused[did] = alpha * ms + (1.0 - alpha) * ss\n",
    "    # sort by fused score desc, then doc_id for stability\n",
    "    sorted_items = sorted(fused.items(), key=lambda x: (-x[1], x[0]))\n",
    "    out = []\n",
    "    for i, (did, sc) in enumerate(sorted_items, start=1):\n",
    "        out.append({'doc_id': did, 'score': float(sc), 'rank': i})\n",
    "    return out\n",
    "\n",
    "def to_qrels_docids_only(gt_items):\n",
    "    rel = {}\n",
    "    if not gt_items:\n",
    "        return rel\n",
    "    for it in gt_items:\n",
    "        candidate = None\n",
    "        if isinstance(it, dict):\n",
    "            candidate = it.get('doc_id') or it.get('id') or it.get('docid')\n",
    "        elif isinstance(it, (str, int)):\n",
    "            candidate = it\n",
    "        did = normalize_doc_id(candidate)\n",
    "        if did:\n",
    "            rel[did] = 1\n",
    "    return rel\n",
    "\n",
    "# -------- NEW: Build a doc_id -> full_text map, preferring passages, then retriever lists --------\n",
    "def build_doc_text_map(passages_items):\n",
    "    \"\"\"\n",
    "    Build doc_id -> text map from passages_with_ids first (prefer 'full_text', then 'text'/'content').\n",
    "    \"\"\"\n",
    "    mp = {}\n",
    "    items = passages_items or []\n",
    "    for it in items:\n",
    "        if not isinstance(it, dict):\n",
    "            continue\n",
    "        did = normalize_doc_id(it.get('doc_id') or it.get('id') or it.get('docid'))\n",
    "        if not did:\n",
    "            continue\n",
    "        txt = it.get('full_text') or it.get('text') or it.get('content')\n",
    "        if isinstance(txt, str) and txt.strip():\n",
    "            mp[did] = txt\n",
    "    return mp\n",
    "\n",
    "def extend_doc_text_map(mp, retriever_items):\n",
    "    \"\"\"\n",
    "    Enrich the map using mpnet/splade lists (prefer 'full_text', fallback to 'snippet'/'preview_snippet'/'text').\n",
    "    \"\"\"\n",
    "    items = retriever_items or []\n",
    "    for it in items:\n",
    "        if not isinstance(it, dict):\n",
    "            continue\n",
    "        did = normalize_doc_id(it.get('doc_id') or it.get('id') or it.get('docid'))\n",
    "        if not did or did in mp:\n",
    "            continue\n",
    "        txt = it.get('full_text') or it.get('snippet') or it.get('preview_snippet') or it.get('text')\n",
    "        if isinstance(txt, str) and txt.strip():\n",
    "            mp[did] = txt\n",
    "    return mp\n",
    "\n",
    "# ------------- Build runs + qrels for pytrec_eval (with linear fusion) -------------\n",
    "results_by_qid = {}  # qid -> {doc_id: score}\n",
    "qrels_by_qid = {}    # qid -> {doc_id: relevance}\n",
    "rows_out = []\n",
    "\n",
    "for q_idx, row in merged.iterrows():\n",
    "    mpnet_items = safe_load(row['mpnet_ret_docs'])\n",
    "    splade_items = safe_load(row['splade_ret_docs'])\n",
    "    gt_items = safe_load(row['groundtruth_with_ids'])\n",
    "    passages_items = safe_load(row['passages_with_ids'])\n",
    "\n",
    "    mpnet_scores = to_score_dict(mpnet_items)\n",
    "    splade_scores = to_score_dict(splade_items)\n",
    "\n",
    "    fused = fuse_linear(mpnet_scores, splade_scores, alpha=ALPHA, normalize=True)\n",
    "\n",
    "    # normalize doc_ids (ensure lowercased/trimmed)\n",
    "    for e in fused:\n",
    "        e['doc_id'] = normalize_doc_id(e['doc_id'])\n",
    "\n",
    "    qid = f\"q_{q_idx}\"\n",
    "\n",
    "    # Build doc text map with priority: passages -> mpnet -> splade\n",
    "    doc_text_map = build_doc_text_map(passages_items)\n",
    "    doc_text_map = extend_doc_text_map(doc_text_map, mpnet_items)\n",
    "    doc_text_map = extend_doc_text_map(doc_text_map, splade_items)\n",
    "\n",
    "    # Attach full_text into hybrid_ret_docs entries\n",
    "    hybrid_with_text = []\n",
    "    for e in fused:\n",
    "        did = e['doc_id']\n",
    "        hybrid_with_text.append({\n",
    "            'doc_id': did,\n",
    "            'score': e['score'],\n",
    "            'rank': e['rank'],\n",
    "            'full_text': doc_text_map.get(did)  # may be None if not available anywhere\n",
    "        })\n",
    "\n",
    "    # run dict: doc_id -> fused score\n",
    "    run_dict = {e['doc_id']: float(e['score']) for e in fused if e['doc_id'] is not None}\n",
    "    results_by_qid[qid] = run_dict\n",
    "\n",
    "    # qrels from groundtruth_with_ids\n",
    "    qrels = to_qrels_docids_only(gt_items)\n",
    "    qrels_by_qid[qid] = {doc_id: int(rel) for doc_id, rel in qrels.items()}\n",
    "\n",
    "    rows_out.append({\n",
    "        'qid': qid,\n",
    "        'question': row['question'],\n",
    "        'answer': row['answer'],\n",
    "        'mpnet_ret_docs': mpnet_items,\n",
    "        'splade_ret_docs': splade_items,\n",
    "        'hybrid_ret_docs': hybrid_with_text,  # now includes full_text\n",
    "        'passages_with_ids': row['passages_with_ids'],\n",
    "        'groundtruth_with_ids': gt_items\n",
    "    })\n",
    "\n",
    "# ------------- Evaluate with pytrec_eval (request normal + cuts) -------------\n",
    "metric_keys = {\"map\", \"ndcg\"} | {f\"map_cut_{k}\" for k in CUTS} | {f\"ndcg_cut_{k}\" for k in CUTS}\n",
    "evaluator = pytrec_eval.RelevanceEvaluator(qrels_by_qid, metric_keys)\n",
    "eval_res = evaluator.evaluate(results_by_qid)\n",
    "\n",
    "def metric_at(pv, metric_base, k):\n",
    "    key = f\"{metric_base}_{k}\"  # e.g., 'map_cut_3'\n",
    "    return float(pv.get(key, 0.0))\n",
    "\n",
    "metrics_rows = []\n",
    "for r in rows_out:\n",
    "    qid = r['qid']\n",
    "    pv = eval_res.get(qid, {})\n",
    "    metrics = {\n",
    "        'MAP':    float(pv.get('map', 0.0)),\n",
    "        'NDCG':   float(pv.get('ndcg', 0.0)),\n",
    "        'MAP@3':  round(metric_at(pv, 'map_cut', 3), 6),\n",
    "        'NDCG@3': round(metric_at(pv, 'ndcg_cut', 3), 6),\n",
    "        'MAP@5':  round(metric_at(pv, 'map_cut', 5), 6),\n",
    "        'NDCG@5': round(metric_at(pv, 'ndcg_cut', 5), 6),\n",
    "        'MAP@10': round(metric_at(pv, 'map_cut', 10), 6),\n",
    "        'NDCG@10':round(metric_at(pv, 'ndcg_cut', 10), 6),\n",
    "    }\n",
    "    metrics_rows.append({**r, **metrics})\n",
    "\n",
    "hybrid_df = pd.DataFrame(metrics_rows)\n",
    "hybrid_df = hybrid_df[['question', 'answer', 'mpnet_ret_docs', 'splade_ret_docs',\n",
    "                       'hybrid_ret_docs', 'passages_with_ids', 'groundtruth_with_ids',\n",
    "                       'MAP', 'NDCG', 'MAP@3', 'NDCG@3', 'MAP@5', 'NDCG@5', 'MAP@10', 'NDCG@10']]\n",
    "\n",
    "# ------------- Save -------------\n",
    "out_csv = 'hotpotqa_hybrid_linear_results_sparse_0.7.csv'\n",
    "hybrid_df.to_csv(out_csv, index=False)\n",
    "print(f\"Saved {len(hybrid_df)} rows to {out_csv}\")\n",
    "\n",
    "# Print averages\n",
    "avg_metrics = hybrid_df[['MAP','NDCG','MAP@3','NDCG@3','MAP@5','NDCG@5','MAP@10','NDCG@10']].mean().round(6)\n",
    "print('Averages (alpha=%.2f):' % ALPHA)\n",
    "for m in ['MAP','NDCG','MAP@3','NDCG@3','MAP@5','NDCG@5','MAP@10','NDCG@10']:\n",
    "    print(f\"{m}: {avg_metrics[m]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merged shape: (90447, 6)\n",
    "Saved 90447 rows to hotpotqa_hybrid_linear_results.csv\n",
    "Averages (alpha=0.40):\n",
    "MAP: 0.301074\n",
    "NDCG: 0.380012\n",
    "MAP@3: 0.277347\n",
    "NDCG@3: 0.345199\n",
    "MAP@5: 0.290613\n",
    "NDCG@5: 0.358912\n",
    "MAP@10: 0.298428\n",
    "NDCG@10: 0.373107\n",
    "\n",
    "\n",
    "Merged shape: (90447, 6)\n",
    "Saved 90447 rows to hotpotqa_hybrid_linear_results.csv\n",
    "Averages (alpha=0.30):\n",
    "MAP: 0.304735\n",
    "NDCG: 0.382653\n",
    "MAP@3: 0.282216\n",
    "NDCG@3: 0.3501\n",
    "MAP@5: 0.294928\n",
    "NDCG@5: 0.362942\n",
    "MAP@10: 0.302289\n",
    "NDCG@10: 0.376265"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged shape: (90447, 6)\n",
      "Saved 90447 rows to hotpotqa_hybrid_linear_with_rerank_minilm_alpha_0.3_beta_0.85.csv\n",
      "Averages (alpha=0.30) - FUSED:\n",
      "FUSED_MAP: 0.304735\n",
      "FUSED_NDCG: 0.382653\n",
      "FUSED_MAP@3: 0.282216\n",
      "FUSED_NDCG@3: 0.3501\n",
      "FUSED_MAP@5: 0.294928\n",
      "FUSED_NDCG@5: 0.362942\n",
      "FUSED_MAP@10: 0.302289\n",
      "FUSED_NDCG@10: 0.376265\n",
      "Averages (alpha=0.30, beta=0.85) - RERANK:\n",
      "RERANK_MAP: 0.315942\n",
      "RERANK_NDCG: 0.391368\n",
      "RERANK_MAP@3: 0.295414\n",
      "RERANK_NDCG@3: 0.362896\n",
      "RERANK_MAP@5: 0.307314\n",
      "RERANK_NDCG@5: 0.374353\n",
      "RERANK_MAP@10: 0.313973\n",
      "RERANK_NDCG@10: 0.38639\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Hybrid (SPLADE + MPNet) with Linear Interpolation + Cross-Encoder Reranking updated\n",
    "Reranker model: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
    "\n",
    "Adds: 'full_text' to each item in both hybrid_ret_docs (fused) and hybrid_reranked_docs.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import pytrec_eval\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# ----------------- Config -----------------\n",
    "CUTS = (3, 5, 10)\n",
    "\n",
    "# Linear interpolation weight for MPNet vs SPLADE\n",
    "ALPHA = 0.3   # fused = ALPHA * mpnet + (1 - ALPHA) * splade\n",
    "\n",
    "# Reranking weight (beta is weight for reranker)\n",
    "BETA = 0.85   # final = BETA * rerank_score + (1 - BETA) * fused_score\n",
    "\n",
    "# Reranker pool and output serialization size\n",
    "MAX_CANDIDATES_FOR_RERANK = 200\n",
    "TOPK_SAVE = 100  # how many reranked docs to store per row\n",
    "\n",
    "# Inputs\n",
    "splade_path = './prior_results/hotpotqa_splade_modified_metrics.csv'\n",
    "mpnet_path  = './prior_results/hotpotqa_mpnet_modified_metrics.csv'\n",
    "\n",
    "# Outputs\n",
    "out_path = f\"hotpotqa_hybrid_linear_with_rerank_minilm_alpha_{ALPHA}_beta_{BETA}.csv\"\n",
    "\n",
    "# Reranker model\n",
    "RERANKER_MODEL_NAME = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "RERANKER_MAX_LENGTH = 512\n",
    "RERANKER_BATCH_SIZE = 32\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE = torch.float32  # the model is small; precision 32-bit is fine\n",
    "\n",
    "# ----------------- Load -----------------\n",
    "splade_df = pd.read_csv(splade_path)\n",
    "mpnet_df  = pd.read_csv(mpnet_path)\n",
    "\n",
    "# Normalize column name if needed\n",
    "if 'all-mpnet-base-v2_ret_docs' in mpnet_df.columns:\n",
    "    mpnet_df = mpnet_df.rename(columns={'all-mpnet-base-v2_ret_docs': 'mpnet_ret_docs'})\n",
    "\n",
    "# Keep only necessary columns\n",
    "splade_keep = ['question', 'answer', 'splade_ret_docs', 'passages_with_ids', 'groundtruth_with_ids']\n",
    "mpnet_keep  = ['mpnet_ret_docs', 'passages_with_ids', 'groundtruth_with_ids']\n",
    "\n",
    "missing_splade = set(splade_keep) - set(splade_df.columns)\n",
    "missing_mpnet  = set(mpnet_keep) - set(mpnet_df.columns)\n",
    "if missing_splade:\n",
    "    raise ValueError(f\"Missing columns in splade_df: {missing_splade}\")\n",
    "if missing_mpnet:\n",
    "    raise ValueError(f\"Missing columns in mpnet_df: {missing_mpnet}\")\n",
    "\n",
    "splade_df = splade_df[splade_keep].copy()\n",
    "mpnet_df  = mpnet_df[mpnet_keep].copy()\n",
    "\n",
    "# Merge on passages_with_ids and groundtruth_with_ids\n",
    "merged = pd.merge(\n",
    "    splade_df,\n",
    "    mpnet_df,\n",
    "    on=['passages_with_ids', 'groundtruth_with_ids'],\n",
    "    how='inner',\n",
    "    validate='m:1'\n",
    ")\n",
    "\n",
    "final_cols = ['question', 'answer', 'passages_with_ids', 'groundtruth_with_ids', 'splade_ret_docs', 'mpnet_ret_docs']\n",
    "merged = merged[final_cols]\n",
    "print('Merged shape:', merged.shape)\n",
    "\n",
    "# ----------------- Helpers -----------------\n",
    "def safe_load(x):\n",
    "    if isinstance(x, (list, dict)):\n",
    "        return x\n",
    "    if pd.isna(x):\n",
    "        return None\n",
    "    s = str(x)\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        try:\n",
    "            import ast\n",
    "            return ast.literal_eval(s)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "def normalize_doc_id(x):\n",
    "    return None if x is None else str(x).strip().lower()\n",
    "\n",
    "def to_score_dict(items):\n",
    "    \"\"\"\n",
    "    Convert retrieval list into dict: doc_id -> score (float).\n",
    "    Accepts dicts containing {'doc_id', 'score'}; ignores items without both.\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    if not items:\n",
    "        return d\n",
    "    for it in items:\n",
    "        if not isinstance(it, dict):\n",
    "            continue\n",
    "        did = normalize_doc_id(it.get('doc_id') or it.get('id') or it.get('docid'))\n",
    "        if not did:\n",
    "            continue\n",
    "        score = it.get('score', it.get('sim'))\n",
    "        try:\n",
    "            score = float(score)\n",
    "        except Exception:\n",
    "            continue\n",
    "        d[did] = score\n",
    "    return d\n",
    "\n",
    "def min_max_norm(d):\n",
    "    \"\"\"\n",
    "    Min–max normalize {doc_id: score}.\n",
    "    If all scores equal or empty, returns zeros.\n",
    "    \"\"\"\n",
    "    if not d:\n",
    "        return {}\n",
    "    vals = list(d.values())\n",
    "    vmin, vmax = min(vals), max(vals)\n",
    "    if vmax == vmin:\n",
    "        return {k: 0.0 for k in d}\n",
    "    return {k: (v - vmin) / (vmax - vmin) for k, v in d.items()}\n",
    "\n",
    "def fuse_linear(mpnet_scores, splade_scores, alpha=0.5, normalize=True):\n",
    "    \"\"\"\n",
    "    Linear interpolation fusion:\n",
    "      fused = alpha * mpnet + (1 - alpha) * splade\n",
    "    - If normalize, scores are min–max normalized per source before fusion.\n",
    "    - Missing docs in one list get score 0 from that source.\n",
    "    Returns list of dicts: {'doc_id','score','rank'} sorted by score desc with rank.\n",
    "    \"\"\"\n",
    "    m = mpnet_scores or {}\n",
    "    s = splade_scores or {}\n",
    "    if normalize:\n",
    "        m = min_max_norm(m)\n",
    "        s = min_max_norm(s)\n",
    "    all_ids = set(m.keys()) | set(s.keys())\n",
    "    fused = {}\n",
    "    for did in all_ids:\n",
    "        ms = m.get(did, 0.0)\n",
    "        ss = s.get(did, 0.0)\n",
    "        fused[did] = alpha * ms + (1.0 - alpha) * ss\n",
    "    sorted_items = sorted(fused.items(), key=lambda x: (-x[1], x[0]))\n",
    "    out = []\n",
    "    for i, (did, sc) in enumerate(sorted_items, start=1):\n",
    "        out.append({'doc_id': did, 'score': float(sc), 'rank': i})\n",
    "    return out\n",
    "\n",
    "def to_qrels_docids_only(gt_items):\n",
    "    rel = {}\n",
    "    if not gt_items:\n",
    "        return rel\n",
    "    for it in gt_items:\n",
    "        candidate = None\n",
    "        if isinstance(it, dict):\n",
    "            candidate = it.get('doc_id') or it.get('id') or it.get('docid')\n",
    "        elif isinstance(it, (str, int)):\n",
    "            candidate = it\n",
    "        did = normalize_doc_id(candidate)\n",
    "        if did:\n",
    "            rel[did] = 1\n",
    "    return rel\n",
    "\n",
    "# Build a doc_id -> full_text map\n",
    "def build_doc_text_map(passages_items) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Prefer passages_with_ids['full_text'|'text'|'content'].\n",
    "    \"\"\"\n",
    "    mp = {}\n",
    "    items = passages_items or []\n",
    "    for it in items:\n",
    "        if not isinstance(it, dict):\n",
    "            continue\n",
    "        did = normalize_doc_id(it.get('doc_id') or it.get('id') or it.get('docid'))\n",
    "        txt = it.get('full_text') or it.get('text') or it.get('content')\n",
    "        if did and isinstance(txt, str) and txt.strip():\n",
    "            mp[did] = txt\n",
    "    return mp\n",
    "\n",
    "def extend_doc_text_map(mp, retriever_items):\n",
    "    \"\"\"\n",
    "    Enrich map with text from mpnet/splade lists (prefer 'full_text', fallback to 'snippet'/'preview_snippet'/'text').\n",
    "    \"\"\"\n",
    "    items = retriever_items or []\n",
    "    for it in items:\n",
    "        if not isinstance(it, dict):\n",
    "            continue\n",
    "        did = normalize_doc_id(it.get('doc_id') or it.get('id') or it.get('docid'))\n",
    "        if not did or did in mp:\n",
    "            continue\n",
    "        txt = it.get('full_text') or it.get('snippet') or it.get('preview_snippet') or it.get('text')\n",
    "        if isinstance(txt, str) and txt.strip():\n",
    "            mp[did] = txt\n",
    "    return mp\n",
    "\n",
    "# ----------------- Reranker: cross-encoder/ms-marco-MiniLM-L-6-v2 -----------------\n",
    "class MiniLMReranker:\n",
    "    \"\"\"\n",
    "    Cross-encoder reranker using 'cross-encoder/ms-marco-MiniLM-L-6-v2'.\n",
    "    Returns a float score per (query, passage), where higher indicates higher relevance.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str = RERANKER_MODEL_NAME, device: str = DEVICE, dtype=DTYPE, max_length: int = RERANKER_MAX_LENGTH):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        self.max_length = max_length\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def score(self, pairs: List[Tuple[str, str]], batch_size: int = RERANKER_BATCH_SIZE) -> List[float]:\n",
    "        scores: List[float] = []\n",
    "        for i in range(0, len(pairs), batch_size):\n",
    "            batch = pairs[i:i+batch_size]\n",
    "            queries = [b[0] for b in batch]\n",
    "            docs    = [b[1] for b in batch]\n",
    "            enc = self.tokenizer(\n",
    "                queries,\n",
    "                docs,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            enc = {k: v.to(self.device) for k, v in enc.items()}\n",
    "            logits = self.model(**enc).logits  # shape (B, 1) typically\n",
    "            if logits.dim() == 2 and logits.size(1) == 1:\n",
    "                batch_scores = logits.squeeze(1).detach().float().cpu().tolist()\n",
    "            else:\n",
    "                batch_scores = logits.view(-1).detach().float().cpu().tolist()\n",
    "            scores.extend(batch_scores)\n",
    "        return scores\n",
    "\n",
    "reranker = MiniLMReranker()\n",
    "\n",
    "def apply_reranker_on_fused_list(\n",
    "    query: str,\n",
    "    fused_list: List[Dict],\n",
    "    doc_text_map: Dict[str, str],\n",
    "    beta: float = BETA,\n",
    "    max_candidates: int = MAX_CANDIDATES_FOR_RERANK,\n",
    "    batch_size: int = RERANKER_BATCH_SIZE\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Input fused_list: [{'doc_id','score','rank'}]\n",
    "    Output: [{'doc_id','fused_score','rerank_score','final_score','rank','full_text'}], sorted by final_score desc.\n",
    "    \"\"\"\n",
    "    # Trim candidate pool\n",
    "    pool = fused_list[:max_candidates]\n",
    "\n",
    "    # Prepare (query, text) pairs; also track text to attach later\n",
    "    pairs = []\n",
    "    valid_idxs = []\n",
    "    cached_texts = {}\n",
    "    for i, e in enumerate(pool):\n",
    "        did = e['doc_id']\n",
    "        txt = doc_text_map.get(did)\n",
    "        if isinstance(txt, str) and txt.strip():\n",
    "            pairs.append((query, txt))\n",
    "            valid_idxs.append(i)\n",
    "            cached_texts[i] = txt  # store for output\n",
    "\n",
    "    # If nothing to score, pass-through and attach whatever text we have (may be None)\n",
    "    if not pairs:\n",
    "        out = []\n",
    "        for i, e in enumerate(pool, start=1):\n",
    "            did = e['doc_id']\n",
    "            out.append({\n",
    "                'doc_id': did,\n",
    "                'fused_score': float(e['score']),\n",
    "                'rerank_score': 0.0,\n",
    "                'final_score': float(e['score']),\n",
    "                'rank': i,\n",
    "                'full_text': doc_text_map.get(did)  # might be None\n",
    "            })\n",
    "        return out\n",
    "\n",
    "    # Reranker scores\n",
    "    rr_scores = reranker.score(pairs, batch_size=batch_size)\n",
    "\n",
    "    # Attach scores and compute final\n",
    "    for idx_local, s in zip(valid_idxs, rr_scores):\n",
    "        pool[idx_local]['rerank_score'] = float(s)\n",
    "    for e in pool:\n",
    "        e['rerank_score'] = float(e.get('rerank_score', 0.0))\n",
    "        e['fused_score'] = float(e.get('score', 0.0))\n",
    "        e['final_score'] = beta * e['rerank_score'] + (1.0 - beta) * e['fused_score']\n",
    "\n",
    "    # Sort and re-rank\n",
    "    ranked = sorted(pool, key=lambda x: (-x['final_score'], x['doc_id']))\n",
    "\n",
    "    # Build output with full_text\n",
    "    out = []\n",
    "    for new_rank, e in enumerate(ranked, start=1):\n",
    "        did = e['doc_id']\n",
    "        out.append({\n",
    "            'doc_id': did,\n",
    "            'fused_score': e['fused_score'],\n",
    "            'rerank_score': e['rerank_score'],\n",
    "            'final_score': e['final_score'],\n",
    "            'rank': new_rank,\n",
    "            'full_text': doc_text_map.get(did)  # attach document text\n",
    "        })\n",
    "    return out\n",
    "\n",
    "# ----------------- Build Runs + Evaluate (Fusion + Rerank) -----------------\n",
    "results_by_qid_fused = {}     # qid -> {doc_id: fused score}\n",
    "results_by_qid_reranked = {}  # qid -> {doc_id: final score}\n",
    "qrels_by_qid = {}             # qid -> {doc_id: relevance}\n",
    "rows_out = []\n",
    "\n",
    "for q_idx, row in merged.iterrows():\n",
    "    mpnet_items = safe_load(row['mpnet_ret_docs'])\n",
    "    splade_items = safe_load(row['splade_ret_docs'])\n",
    "    gt_items = safe_load(row['groundtruth_with_ids'])\n",
    "    passages_items = safe_load(row['passages_with_ids'])\n",
    "\n",
    "    mpnet_scores = to_score_dict(mpnet_items)\n",
    "    splade_scores = to_score_dict(splade_items)\n",
    "\n",
    "    fused = fuse_linear(mpnet_scores, splade_scores, alpha=ALPHA, normalize=True)\n",
    "    for e in fused:\n",
    "        e['doc_id'] = normalize_doc_id(e['doc_id'])\n",
    "\n",
    "    qid = f\"q_{q_idx}\"\n",
    "\n",
    "    # Build doc_id -> text map, using passages first, then enrich from retriever lists\n",
    "    doc_text_map = build_doc_text_map(passages_items)\n",
    "    doc_text_map = extend_doc_text_map(doc_text_map, mpnet_items)\n",
    "    doc_text_map = extend_doc_text_map(doc_text_map, splade_items)\n",
    "\n",
    "    # Apply reranker over fused list\n",
    "    reranked = apply_reranker_on_fused_list(\n",
    "        query=row['question'],\n",
    "        fused_list=fused,\n",
    "        doc_text_map=doc_text_map,\n",
    "        beta=BETA,\n",
    "        max_candidates=MAX_CANDIDATES_FOR_RERANK,\n",
    "        batch_size=RERANKER_BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    # Also attach full_text to fused list for serialization\n",
    "    fused_with_text = []\n",
    "    for e in fused:\n",
    "        did = e['doc_id']\n",
    "        fused_with_text.append({\n",
    "            'doc_id': did,\n",
    "            'score': e['score'],\n",
    "            'rank': e['rank'],\n",
    "            'full_text': doc_text_map.get(did)\n",
    "        })\n",
    "\n",
    "    # Build run dicts\n",
    "    run_fused = {e['doc_id']: float(e['score']) for e in fused if e['doc_id'] is not None}\n",
    "    run_reranked = {e['doc_id']: float(e['final_score']) for e in reranked if e['doc_id'] is not None}\n",
    "\n",
    "    results_by_qid_fused[qid] = run_fused\n",
    "    results_by_qid_reranked[qid] = run_reranked\n",
    "\n",
    "    # Qrels\n",
    "    qrels = to_qrels_docids_only(gt_items)\n",
    "    qrels_by_qid[qid] = {doc_id: int(rel) for doc_id, rel in qrels.items()}\n",
    "\n",
    "    # Row serialization\n",
    "    rows_out.append({\n",
    "        'qid': qid,\n",
    "        'question': row['question'],\n",
    "        'answer': row['answer'],\n",
    "        'mpnet_ret_docs': mpnet_items,\n",
    "        'splade_ret_docs': splade_items,\n",
    "        'hybrid_ret_docs': fused_with_text,                 # before rerank, now with full_text\n",
    "        'hybrid_reranked_docs': reranked[:TOPK_SAVE],       # after rerank + blending, with full_text\n",
    "        'passages_with_ids': passages_items,\n",
    "        'groundtruth_with_ids': gt_items\n",
    "    })\n",
    "\n",
    "# ----------------- Evaluation (pytrec_eval) -----------------\n",
    "metric_keys = {\"map\", \"ndcg\"} | {f\"map_cut_{k}\" for k in CUTS} | {f\"ndcg_cut_{k}\" for k in CUTS}\n",
    "evaluator_fused = pytrec_eval.RelevanceEvaluator(qrels_by_qid, metric_keys)\n",
    "evaluator_reranked = pytrec_eval.RelevanceEvaluator(qrels_by_qid, metric_keys)\n",
    "\n",
    "eval_fused = evaluator_fused.evaluate(results_by_qid_fused)\n",
    "eval_reranked = evaluator_reranked.evaluate(results_by_qid_reranked)\n",
    "\n",
    "def extract_metrics(pv):\n",
    "    return {\n",
    "        'MAP': float(pv.get('map', 0.0)),\n",
    "        'NDCG': float(pv.get('ndcg', 0.0)),\n",
    "        'MAP@3': round(float(pv.get('map_cut_3', 0.0)), 6),\n",
    "        'NDCG@3': round(float(pv.get('ndcg_cut_3', 0.0)), 6),\n",
    "        'MAP@5': round(float(pv.get('map_cut_5', 0.0)), 6),\n",
    "        'NDCG@5': round(float(pv.get('ndcg_cut_5', 0.0)), 6),\n",
    "        'MAP@10': round(float(pv.get('map_cut_10', 0.0)), 6),\n",
    "        'NDCG@10': round(float(pv.get('ndcg_cut_10', 0.0)), 6),\n",
    "    }\n",
    "\n",
    "metrics_rows = []\n",
    "for r in rows_out:\n",
    "    qid = r['qid']\n",
    "    fused_metrics = extract_metrics(eval_fused.get(qid, {}))\n",
    "    rerank_metrics = extract_metrics(eval_reranked.get(qid, {}))\n",
    "    metrics_rows.append({\n",
    "        **r,\n",
    "        **{f\"FUSED_{k}\": v for k, v in fused_metrics.items()},\n",
    "        **{f\"RERANK_{k}\": v for k, v in rerank_metrics.items()},\n",
    "    })\n",
    "\n",
    "hybrid_df = pd.DataFrame(metrics_rows)\n",
    "cols = ['question', 'answer', 'mpnet_ret_docs', 'splade_ret_docs',\n",
    "        'hybrid_ret_docs', 'hybrid_reranked_docs', 'passages_with_ids',\n",
    "        'groundtruth_with_ids',\n",
    "        'FUSED_MAP','FUSED_NDCG','FUSED_MAP@3','FUSED_NDCG@3','FUSED_MAP@5','FUSED_NDCG@5','FUSED_MAP@10','FUSED_NDCG@10',\n",
    "        'RERANK_MAP','RERANK_NDCG','RERANK_MAP@3','RERANK_NDCG@3','RERANK_MAP@5','RERANK_NDCG@5','RERANK_MAP@10','RERANK_NDCG@10']\n",
    "hybrid_df = hybrid_df[cols]\n",
    "\n",
    "# Save\n",
    "hybrid_df.to_csv(out_path, index=False)\n",
    "print(f\"Saved {len(hybrid_df)} rows to {out_path}\")\n",
    "\n",
    "# Print averages\n",
    "avg_fused = hybrid_df[['FUSED_MAP','FUSED_NDCG','FUSED_MAP@3','FUSED_NDCG@3','FUSED_MAP@5','FUSED_NDCG@5','FUSED_MAP@10','FUSED_NDCG@10']].mean().round(6)\n",
    "avg_rerank = hybrid_df[['RERANK_MAP','RERANK_NDCG','RERANK_MAP@3','RERANK_NDCG@3','RERANK_MAP@5','RERANK_NDCG@5','RERANK_MAP@10','RERANK_NDCG@10']].mean().round(6)\n",
    "\n",
    "print(f\"Averages (alpha={ALPHA:.2f}) - FUSED:\")\n",
    "for m in ['FUSED_MAP','FUSED_NDCG','FUSED_MAP@3','FUSED_NDCG@3','FUSED_MAP@5','FUSED_NDCG@5','FUSED_MAP@10','FUSED_NDCG@10']:\n",
    "    print(f\"{m}: {avg_fused[m]}\")\n",
    "\n",
    "print(f\"Averages (alpha={ALPHA:.2f}, beta={BETA:.2f}) - RERANK:\")\n",
    "for m in ['RERANK_MAP','RERANK_NDCG','RERANK_MAP@3','RERANK_NDCG@3','RERANK_MAP@5','RERANK_NDCG@5','RERANK_MAP@10','RERANK_NDCG@10']:\n",
    "    print(f\"{m}: {avg_rerank[m]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merged shape: (90447, 6)\n",
    "Saved 90447 rows to hotpotqa_hybrid_linear_with_rerank_minilm_alpha_0.3_beta_0.85.csv\n",
    "Averages (alpha=0.30) - FUSED:\n",
    "FUSED_MAP: 0.304735\n",
    "FUSED_NDCG: 0.382653\n",
    "FUSED_MAP@3: 0.282216\n",
    "FUSED_NDCG@3: 0.3501\n",
    "FUSED_MAP@5: 0.294928\n",
    "FUSED_NDCG@5: 0.362942\n",
    "FUSED_MAP@10: 0.302289\n",
    "FUSED_NDCG@10: 0.376265\n",
    "Averages (alpha=0.30, beta=0.85) - RERANK:\n",
    "RERANK_MAP: 0.315942\n",
    "RERANK_NDCG: 0.391368\n",
    "RERANK_MAP@3: 0.295414\n",
    "RERANK_NDCG@3: 0.362896\n",
    "RERANK_MAP@5: 0.307314\n",
    "RERANK_NDCG@5: 0.374353\n",
    "RERANK_MAP@10: 0.313973\n",
    "RERANK_NDCG@10: 0.38639"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 80044 rows to hotpotqa_hybrid_combmnz_results.csv\n",
      "Averages (CombMNZ):\n",
      "MAP: 0.287889\n",
      "NDCG: 0.368214\n",
      "MAP@3: 0.261554\n",
      "NDCG@3: 0.328154\n",
      "MAP@5: 0.276335\n",
      "NDCG@5: 0.344823\n",
      "MAP@10: 0.285001\n",
      "NDCG@10: 0.36068\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "CombMNZ works as:\n",
    "\n",
    "Normalize scores per list (recommended) to make scales comparable.\n",
    "For each doc, sum the normalized scores across systems (CombSUM) and multiply by the number of systems that retrieved the doc (NZ = non-zero count).\n",
    "fused_score = (sum of normalized scores) * (number of non-zero contributors)\n",
    "\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import pytrec_eval  # make sure available\n",
    "\n",
    "CUTS = (3, 5, 10)\n",
    "\n",
    "# ------------- Load + prep (same as before) -------------\n",
    "splade_df = pd.read_csv('./prior_results/hotpotqa_splade_modified_metrics.csv')\n",
    "mpnet_df = pd.read_csv('./prior_results/hotpotqa_mpnet_modified_metrics.csv')\n",
    "\n",
    "if 'all-mpnet-base-v2_ret_docs' in mpnet_df.columns:\n",
    "    mpnet_df = mpnet_df.rename(columns={'all-mpnet-base-v2_ret_docs': 'mpnet_ret_docs'})\n",
    "\n",
    "splade_keep = ['question', 'answer', 'splade_ret_docs', 'passages_with_ids', 'groundtruth_with_ids']\n",
    "mpnet_keep  = ['question', 'answer', 'mpnet_ret_docs', 'passages_with_ids', 'groundtruth_with_ids']\n",
    "\n",
    "splade_df = splade_df[splade_keep].copy()\n",
    "mpnet_df  = mpnet_df[mpnet_keep].copy()\n",
    "\n",
    "def lower_str(x):\n",
    "    if pd.isna(x):\n",
    "        return x\n",
    "    return str(x).lower()\n",
    "\n",
    "for col in ['question', 'answer', 'passages_with_ids', 'groundtruth_with_ids']:\n",
    "    if col in splade_df.columns:\n",
    "        splade_df[col] = splade_df[col].apply(lower_str)\n",
    "    if col in mpnet_df.columns:\n",
    "        mpnet_df[col] = mpnet_df[col].apply(lower_str)\n",
    "\n",
    "merged = pd.merge(\n",
    "    mpnet_df, splade_df,\n",
    "    on=['question', 'answer', 'passages_with_ids', 'groundtruth_with_ids'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# ------------- Parsing helpers (hardened) -------------\n",
    "def safe_load(x):\n",
    "    if isinstance(x, (list, dict)):\n",
    "        return x\n",
    "    if pd.isna(x):\n",
    "        return None\n",
    "    s = str(x)\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        try:\n",
    "            import ast\n",
    "            return ast.literal_eval(s)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "def normalize_doc_id(x):\n",
    "    return None if x is None else str(x).strip().lower()\n",
    "\n",
    "def to_score_dict(items):\n",
    "    \"\"\"\n",
    "    Convert retrieval list into dict: doc_id -> score (float).\n",
    "    Accepts dicts containing {'doc_id', 'score'}; ignores items without both.\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    if not items:\n",
    "        return d\n",
    "    for it in items:\n",
    "        if not isinstance(it, dict):\n",
    "            continue\n",
    "        did = normalize_doc_id(it.get('doc_id') or it.get('id') or it.get('docid'))\n",
    "        if not did:\n",
    "            continue\n",
    "        score = it.get('score', it.get('sim'))\n",
    "        try:\n",
    "            score = float(score)\n",
    "        except Exception:\n",
    "            continue\n",
    "        d[did] = score\n",
    "    return d\n",
    "\n",
    "def min_max_norm(d):\n",
    "    \"\"\"\n",
    "    Min-max normalize a dict of {doc_id: score} per query.\n",
    "    If all scores equal or empty, returns zeros.\n",
    "    \"\"\"\n",
    "    if not d:\n",
    "        return {}\n",
    "    vals = list(d.values())\n",
    "    vmin, vmax = min(vals), max(vals)\n",
    "    if vmax == vmin:\n",
    "        return {k: 0.0 for k in d}\n",
    "    return {k: (v - vmin) / (vmax - vmin) for k, v in d.items()}\n",
    "\n",
    "def fuse_combmnz(mpnet_scores, splade_scores, normalize=True):\n",
    "    \"\"\"\n",
    "    CombMNZ fusion:\n",
    "      For each doc:\n",
    "        s_norm = (normalize scores per list if requested)\n",
    "        sum_scores = s_m + s_s\n",
    "        nz = count of non-zero source scores contributing (>= 2 if in both lists and >0 after normalization)\n",
    "        fused = sum_scores * nz\n",
    "    Missing docs in a source treated as 0.\n",
    "    Returns list of dicts: {'doc_id', 'score'} sorted by fused score desc with rank.\n",
    "    \"\"\"\n",
    "    m = mpnet_scores or {}\n",
    "    s = splade_scores or {}\n",
    "    if normalize:\n",
    "        m = min_max_norm(m)\n",
    "        s = min_max_norm(s)\n",
    "\n",
    "    all_ids = set(m.keys()) | set(s.keys())\n",
    "    fused = {}\n",
    "    for did in all_ids:\n",
    "        ms = m.get(did, 0.0)\n",
    "        ss = s.get(did, 0.0)\n",
    "        sum_scores = ms + ss\n",
    "        nz = (1 if ms > 0.0 else 0) + (1 if ss > 0.0 else 0)\n",
    "        # If both are zero (rare with normalization unless singletons), nz stays 0; fused should be 0\n",
    "        fused_score = sum_scores * nz if nz > 0 else 0.0\n",
    "        fused[did] = fused_score\n",
    "\n",
    "    sorted_items = sorted(fused.items(), key=lambda x: (-x[1], x[0]))\n",
    "    out = []\n",
    "    for i, (did, sc) in enumerate(sorted_items, start=1):\n",
    "        out.append({'doc_id': did, 'score': float(sc), 'rank': i})\n",
    "    return out\n",
    "\n",
    "def to_qrels_docids_only(gt_items):\n",
    "    rel = {}\n",
    "    if not gt_items:\n",
    "        return rel\n",
    "    for it in gt_items:\n",
    "        candidate = None\n",
    "        if isinstance(it, dict):\n",
    "            candidate = it.get('doc_id') or it.get('id') or it.get('docid')\n",
    "        elif isinstance(it, (str, int)):\n",
    "            candidate = it\n",
    "        did = normalize_doc_id(candidate)\n",
    "        if did:\n",
    "            rel[did] = 1\n",
    "    return rel\n",
    "\n",
    "# ------------- Build runs + qrels for pytrec_eval (with CombMNZ fusion) -------------\n",
    "results_by_qid = {}  # qid -> {doc_id: score}\n",
    "qrels_by_qid = {}    # qid -> {doc_id: relevance}\n",
    "rows_out = []\n",
    "\n",
    "for q_idx, row in merged.iterrows():\n",
    "    mpnet_items = safe_load(row['mpnet_ret_docs'])\n",
    "    splade_items = safe_load(row['splade_ret_docs'])\n",
    "    gt_items = safe_load(row['groundtruth_with_ids'])\n",
    "\n",
    "    mpnet_scores = to_score_dict(mpnet_items)\n",
    "    splade_scores = to_score_dict(splade_items)\n",
    "\n",
    "    fused = fuse_combmnz(mpnet_scores, splade_scores, normalize=True)\n",
    "\n",
    "    # ensure normalized doc_ids\n",
    "    for e in fused:\n",
    "        e['doc_id'] = normalize_doc_id(e['doc_id'])\n",
    "\n",
    "    qid = f\"q_{q_idx}\"\n",
    "\n",
    "    # run dict: doc_id -> fused score\n",
    "    run_dict = {e['doc_id']: float(e['score']) for e in fused if e['doc_id'] is not None}\n",
    "    results_by_qid[qid] = run_dict\n",
    "\n",
    "    # qrels from groundtruth_with_ids\n",
    "    qrels = to_qrels_docids_only(gt_items)\n",
    "    qrels_by_qid[qid] = {doc_id: int(rel) for doc_id, rel in qrels.items()}\n",
    "\n",
    "    rows_out.append({\n",
    "        'qid': qid,\n",
    "        'question': row['question'],\n",
    "        'answer': row['answer'],\n",
    "        'mpnet_ret_docs': mpnet_items,\n",
    "        'splade_ret_docs': splade_items,\n",
    "        'hybrid_ret_docs': fused,  # CombMNZ fused list with scores and ranks\n",
    "        'passages_with_ids': row['passages_with_ids'],\n",
    "        'groundtruth_with_ids': gt_items\n",
    "    })\n",
    "\n",
    "# ------------- Evaluate with pytrec_eval (request normal + cuts) -------------\n",
    "metric_keys = {\"map\", \"ndcg\"} | {f\"map_cut_{k}\" for k in CUTS} | {f\"ndcg_cut_{k}\" for k in CUTS}\n",
    "evaluator = pytrec_eval.RelevanceEvaluator(qrels_by_qid, metric_keys)\n",
    "eval_res = evaluator.evaluate(results_by_qid)\n",
    "\n",
    "def metric_at(pv, metric_base, k):\n",
    "    key = f\"{metric_base}_{k}\"  # e.g., 'map_cut_3'\n",
    "    return float(pv.get(key, 0.0))\n",
    "\n",
    "metrics_rows = []\n",
    "for r in rows_out:\n",
    "    qid = r['qid']\n",
    "    pv = eval_res.get(qid, {})\n",
    "    metrics = {\n",
    "        'MAP':    float(pv.get('map', 0.0)),\n",
    "        'NDCG':   float(pv.get('ndcg', 0.0)),\n",
    "        'MAP@3':  round(metric_at(pv, 'map_cut', 3), 6),\n",
    "        'NDCG@3': round(metric_at(pv, 'ndcg_cut', 3), 6),\n",
    "        'MAP@5':  round(metric_at(pv, 'map_cut', 5), 6),\n",
    "        'NDCG@5': round(metric_at(pv, 'ndcg_cut', 5), 6),\n",
    "        'MAP@10': round(metric_at(pv, 'map_cut', 10), 6),\n",
    "        'NDCG@10':round(metric_at(pv, 'ndcg_cut', 10), 6),\n",
    "    }\n",
    "    metrics_rows.append({**r, **metrics})\n",
    "\n",
    "hybrid_df = pd.DataFrame(metrics_rows)\n",
    "hybrid_df = hybrid_df[['question', 'answer', 'mpnet_ret_docs', 'splade_ret_docs',\n",
    "                       'hybrid_ret_docs', 'passages_with_ids', 'groundtruth_with_ids',\n",
    "                       'MAP', 'NDCG', 'MAP@3', 'NDCG@3', 'MAP@5', 'NDCG@5', 'MAP@10', 'NDCG@10']]\n",
    "\n",
    "# ------------- Save -------------\n",
    "hybrid_df.to_csv('hotpotqa_hybrid_combmnz_results.csv', index=False)\n",
    "print(f\"Saved {len(hybrid_df)} rows to hotpotqa_hybrid_combmnz_results.csv\")\n",
    "\n",
    "# Print averages\n",
    "avg_metrics = hybrid_df[['MAP','NDCG','MAP@3','NDCG@3','MAP@5','NDCG@5','MAP@10','NDCG@10']].mean().round(6)\n",
    "print('Averages (CombMNZ):')\n",
    "for m in ['MAP','NDCG','MAP@3','NDCG@3','MAP@5','NDCG@5','MAP@10','NDCG@10']:\n",
    "    print(f\"{m}: {avg_metrics[m]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csmala/miniconda3/envs/halu_rag/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 80044 rows to hotpotqa_hybrid_adaptive_results.csv\n",
      "Averages (Adaptive weights via classifier):\n",
      "MAP: 0.245609\n",
      "NDCG: 0.331681\n",
      "MAP@3: 0.214106\n",
      "NDCG@3: 0.271198\n",
      "MAP@5: 0.225806\n",
      "NDCG@5: 0.285706\n",
      "MAP@10: 0.240078\n",
      "NDCG@10: 0.317363\n"
     ]
    }
   ],
   "source": [
    "# Adaptive QC fusion using a BERT-based classifier to get per-query weights\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import pytrec_eval\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "CUTS = (3, 5, 10)\n",
    "\n",
    "# ---------- Load adaptive weight model ----------\n",
    "FOLDER_PATH = \"bert_model_QC_finetuned\"\n",
    "TEMPERATURE = 1.17  # tune as needed\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(FOLDER_PATH)\n",
    "model = BertForSequenceClassification.from_pretrained(FOLDER_PATH)\n",
    "model.eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_query_weights(query_text: str, temperature: float = TEMPERATURE):\n",
    "    \"\"\"\n",
    "    Returns (w_sparse, w_dense) from the classifier's softmax over logits/temperature.\n",
    "    Assumes class 0 = sparse, class 1 = dense.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(query_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits  # shape [1,2]\n",
    "    weights = torch.softmax(logits / temperature, dim=-1).squeeze(0)  # [2]\n",
    "    w_sparse = float(weights[0].item())\n",
    "    w_dense = float(weights[1].item())\n",
    "    return w_sparse, w_dense\n",
    "\n",
    "# ------------- Load + prep (same as before) -------------\n",
    "splade_df = pd.read_csv('./prior_results/hotpotqa_splade_modified_metrics.csv')\n",
    "mpnet_df = pd.read_csv('./prior_results/hotpotqa_mpnet_modified_metrics.csv')\n",
    "\n",
    "if 'all-mpnet-base-v2_ret_docs' in mpnet_df.columns:\n",
    "    mpnet_df = mpnet_df.rename(columns={'all-mpnet-base-v2_ret_docs': 'mpnet_ret_docs'})\n",
    "\n",
    "splade_keep = ['question', 'answer', 'splade_ret_docs', 'passages_with_ids', 'groundtruth_with_ids']\n",
    "mpnet_keep  = ['question', 'answer', 'mpnet_ret_docs', 'passages_with_ids', 'groundtruth_with_ids']\n",
    "\n",
    "splade_df = splade_df[splade_keep].copy()\n",
    "mpnet_df  = mpnet_df[mpnet_keep].copy()\n",
    "\n",
    "def lower_str(x):\n",
    "    if pd.isna(x):\n",
    "        return x\n",
    "    return str(x).lower()\n",
    "\n",
    "for col in ['question', 'answer', 'passages_with_ids', 'groundtruth_with_ids']:\n",
    "    if col in splade_df.columns:\n",
    "        splade_df[col] = splade_df[col].apply(lower_str)\n",
    "    if col in mpnet_df.columns:\n",
    "        mpnet_df[col] = mpnet_df[col].apply(lower_str)\n",
    "\n",
    "merged = pd.merge(\n",
    "    mpnet_df, splade_df,\n",
    "    on=['question', 'answer', 'passages_with_ids', 'groundtruth_with_ids'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# ------------- Parsing helpers -------------\n",
    "def safe_load(x):\n",
    "    if isinstance(x, (list, dict)):\n",
    "        return x\n",
    "    if pd.isna(x):\n",
    "        return None\n",
    "    s = str(x)\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        try:\n",
    "            import ast\n",
    "            return ast.literal_eval(s)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "def normalize_doc_id(x):\n",
    "    return None if x is None else str(x).strip().lower()\n",
    "\n",
    "def to_score_dict(items):\n",
    "    \"\"\"\n",
    "    Convert retrieval list into dict: doc_id -> score (float).\n",
    "    Accepts dicts containing {'doc_id', 'score'}; ignores items without both.\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    if not items:\n",
    "        return d\n",
    "    for it in items:\n",
    "        if not isinstance(it, dict):\n",
    "            continue\n",
    "        did = normalize_doc_id(it.get('doc_id') or it.get('id') or it.get('docid'))\n",
    "        if not did:\n",
    "            continue\n",
    "        score = it.get('score', it.get('sim'))\n",
    "        try:\n",
    "            score = float(score)\n",
    "        except Exception:\n",
    "            continue\n",
    "        d[did] = score\n",
    "    return d\n",
    "\n",
    "def min_max_norm(d):\n",
    "    \"\"\"\n",
    "    Min-max normalize a dict of {doc_id: score} per query.\n",
    "    If all scores equal or empty, returns zeros.\n",
    "    \"\"\"\n",
    "    if not d:\n",
    "        return {}\n",
    "    vals = list(d.values())\n",
    "    vmin, vmax = min(vals), max(vals)\n",
    "    if vmax == vmin:\n",
    "        return {k: 0.0 for k in d}\n",
    "    return {k: (v - vmin) / (vmax - vmin) for k, v in d.items()}\n",
    "\n",
    "def fuse_adaptive(mpnet_scores, splade_scores, w_sparse: float, w_dense: float, normalize=True):\n",
    "    \"\"\"\n",
    "    Adaptive score-based fusion using classifier weights per query:\n",
    "      fused = w_dense * dense_norm + w_sparse * sparse_norm\n",
    "    Missing docs in a source get score 0 from that source.\n",
    "    Returns list of dicts: {'doc_id', 'score', 'rank'} sorted by score desc.\n",
    "    \"\"\"\n",
    "    dense = mpnet_scores or {}\n",
    "    sparse = splade_scores or {}\n",
    "    if normalize:\n",
    "        dense = min_max_norm(dense)\n",
    "        sparse = min_max_norm(sparse)\n",
    "    all_ids = set(dense.keys()) | set(sparse.keys())\n",
    "    fused = {}\n",
    "    for did in all_ids:\n",
    "        ds = dense.get(did, 0.0)\n",
    "        ss = sparse.get(did, 0.0)\n",
    "        fused[did] = w_dense * ds + w_sparse * ss\n",
    "    sorted_items = sorted(fused.items(), key=lambda x: (-x[1], x[0]))\n",
    "    out = []\n",
    "    for i, (did, sc) in enumerate(sorted_items, start=1):\n",
    "        out.append({'doc_id': did, 'score': float(sc), 'rank': i})\n",
    "    return out\n",
    "\n",
    "def to_qrels_docids_only(gt_items):\n",
    "    rel = {}\n",
    "    if not gt_items:\n",
    "        return rel\n",
    "    for it in gt_items:\n",
    "        candidate = None\n",
    "        if isinstance(it, dict):\n",
    "            candidate = it.get('doc_id') or it.get('id') or it.get('docid')\n",
    "        elif isinstance(it, (str, int)):\n",
    "            candidate = it\n",
    "        did = normalize_doc_id(candidate)\n",
    "        if did:\n",
    "            rel[did] = 1\n",
    "    return rel\n",
    "\n",
    "# ------------- Build runs + qrels with adaptive fusion -------------\n",
    "results_by_qid = {}  # qid -> {doc_id: score}\n",
    "qrels_by_qid = {}    # qid -> {doc_id: relevance}\n",
    "rows_out = []\n",
    "\n",
    "for q_idx, row in merged.iterrows():\n",
    "    question_text = row['question']  # use original text for classifier\n",
    "    mpnet_items = safe_load(row['mpnet_ret_docs'])\n",
    "    splade_items = safe_load(row['splade_ret_docs'])\n",
    "    gt_items = safe_load(row['groundtruth_with_ids'])\n",
    "\n",
    "    mpnet_scores = to_score_dict(mpnet_items)\n",
    "    splade_scores = to_score_dict(splade_items)\n",
    "\n",
    "    # Get adaptive weights per query: (w_sparse, w_dense)\n",
    "    w_sparse, w_dense = get_query_weights(question_text)\n",
    "\n",
    "    fused = fuse_adaptive(mpnet_scores, splade_scores, w_sparse=w_sparse, w_dense=w_dense, normalize=True)\n",
    "\n",
    "    # ensure normalized doc_ids\n",
    "    for e in fused:\n",
    "        e['doc_id'] = normalize_doc_id(e['doc_id'])\n",
    "\n",
    "    qid = f\"q_{q_idx}\"\n",
    "\n",
    "    # run dict: doc_id -> fused score\n",
    "    run_dict = {e['doc_id']: float(e['score']) for e in fused if e['doc_id'] is not None}\n",
    "    results_by_qid[qid] = run_dict\n",
    "\n",
    "    # qrels from groundtruth_with_ids\n",
    "    qrels = to_qrels_docids_only(gt_items)\n",
    "    qrels_by_qid[qid] = {doc_id: int(rel) for doc_id, rel in qrels.items()}\n",
    "\n",
    "    rows_out.append({\n",
    "        'qid': qid,\n",
    "        'question': row['question'],\n",
    "        'answer': row['answer'],\n",
    "        'mpnet_ret_docs': mpnet_items,\n",
    "        'splade_ret_docs': splade_items,\n",
    "        'hybrid_ret_docs': fused,  # adaptive fused list\n",
    "        'weights': {'sparse': w_sparse, 'dense': w_dense, 'temperature': TEMPERATURE},\n",
    "        'passages_with_ids': row['passages_with_ids'],\n",
    "        'groundtruth_with_ids': gt_items\n",
    "    })\n",
    "\n",
    "# ------------- Evaluate with pytrec_eval (normal + cuts) -------------\n",
    "metric_keys = {\"map\", \"ndcg\"} | {f\"map_cut_{k}\" for k in CUTS} | {f\"ndcg_cut_{k}\" for k in CUTS}\n",
    "evaluator = pytrec_eval.RelevanceEvaluator(qrels_by_qid, metric_keys)\n",
    "eval_res = evaluator.evaluate(results_by_qid)\n",
    "\n",
    "def metric_at(pv, metric_base, k):\n",
    "    key = f\"{metric_base}_{k}\"  # e.g., 'map_cut_3'\n",
    "    return float(pv.get(key, 0.0))\n",
    "\n",
    "metrics_rows = []\n",
    "for r in rows_out:\n",
    "    qid = r['qid']\n",
    "    pv = eval_res.get(qid, {})\n",
    "    metrics = {\n",
    "        'MAP':    float(pv.get('map', 0.0)),\n",
    "        'NDCG':   float(pv.get('ndcg', 0.0)),\n",
    "        'MAP@3':  round(metric_at(pv, 'map_cut', 3), 6),\n",
    "        'NDCG@3': round(metric_at(pv, 'ndcg_cut', 3), 6),\n",
    "        'MAP@5':  round(metric_at(pv, 'map_cut', 5), 6),\n",
    "        'NDCG@5': round(metric_at(pv, 'ndcg_cut', 5), 6),\n",
    "        'MAP@10': round(metric_at(pv, 'map_cut', 10), 6),\n",
    "        'NDCG@10':round(metric_at(pv, 'ndcg_cut', 10), 6),\n",
    "    }\n",
    "    metrics_rows.append({**r, **metrics})\n",
    "\n",
    "hybrid_df = pd.DataFrame(metrics_rows)\n",
    "hybrid_df = hybrid_df[['question', 'answer', 'mpnet_ret_docs', 'splade_ret_docs',\n",
    "                       'hybrid_ret_docs', 'weights', 'passages_with_ids', 'groundtruth_with_ids',\n",
    "                       'MAP', 'NDCG', 'MAP@3', 'NDCG@3', 'MAP@5', 'NDCG@5', 'MAP@10', 'NDCG@10']]\n",
    "\n",
    "# ------------- Save -------------\n",
    "hybrid_df.to_csv('hotpotqa_hybrid_adaptive_results.csv', index=False)\n",
    "print(f\"Saved {len(hybrid_df)} rows to hotpotqa_hybrid_adaptive_results.csv\")\n",
    "\n",
    "# Print averages\n",
    "avg_metrics = hybrid_df[['MAP','NDCG','MAP@3','NDCG@3','MAP@5','NDCG@5','MAP@10','NDCG@10']].mean().round(6)\n",
    "print('Averages (Adaptive weights via classifier):')\n",
    "for m in ['MAP','NDCG','MAP@3','NDCG@3','MAP@5','NDCG@5','MAP@10','NDCG@10']:\n",
    "    print(f\"{m}: {avg_metrics[m]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid on Halubench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['question', 'answer', 'passage', 'splade_ret_docs', 'passages_with_ids',\n",
       "       'groundtruth_with_ids', 'MAP@3', 'NDCG@3', 'MAP@5', 'NDCG@5', 'MAP@10',\n",
       "       'NDCG@10'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./prior_results/halubench_splade_modified_metrics.csv')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['question', 'answer', 'passage', 'mpnet_ret_docs', 'passages_with_ids',\n",
       "       'groundtruth_with_ids', 'MAP@3', 'NDCG@3', 'MAP@5', 'NDCG@5', 'MAP@10',\n",
       "       'NDCG@10'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./prior_results/halubench_mpnet_modified_metrics.csv')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged shape: (14900, 7)\n",
      "Saved 14900 rows to hybrid_results/halubench_hybrid_linear_results_splade_0.3_new.csv\n",
      "Averages (alpha=0.30):\n",
      "MAP: 0.844085\n",
      "NDCG: 0.864219\n",
      "MAP@3: 0.834239\n",
      "NDCG@3: 0.841771\n",
      "MAP@5: 0.838806\n",
      "NDCG@5: 0.850033\n",
      "MAP@10: 0.842712\n",
      "NDCG@10: 0.859471\n"
     ]
    }
   ],
   "source": [
    "# Hybrid + Linear interpolation\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import pytrec_eval  # ensure installed\n",
    "\n",
    "CUTS = (3, 5, 10)\n",
    "ALPHA = 0.3  # weight for mpnet; (1-ALPHA) is weight for splade\n",
    "\n",
    "# ------------- Load paths -------------\n",
    "splade_path = './prior_results/halubench_splade_modified_metrics.csv'\n",
    "mpnet_path  = './prior_results/halubench_mpnet_modified_metrics.csv'\n",
    "\n",
    "splade_df = pd.read_csv(splade_path)\n",
    "mpnet_df  = pd.read_csv(mpnet_path)\n",
    "\n",
    "# Normalize mpnet column name if needed\n",
    "if 'all-mpnet-base-v2_ret_docs' in mpnet_df.columns:\n",
    "    mpnet_df = mpnet_df.rename(columns={'all-mpnet-base-v2_ret_docs': 'mpnet_ret_docs'})\n",
    "\n",
    "# ------------- Keep only necessary columns -------------\n",
    "# From Splade we keep: question, answer, passage, splade_ret_docs, passages_with_ids, groundtruth_with_ids\n",
    "splade_keep = ['question', 'answer', 'passage', 'splade_ret_docs', 'passages_with_ids', 'groundtruth_with_ids']\n",
    "missing_splade = set(splade_keep) - set(splade_df.columns)\n",
    "if missing_splade:\n",
    "    raise ValueError(f\"Missing columns in splade_df: {missing_splade}\")\n",
    "splade_df = splade_df[splade_keep].copy()\n",
    "\n",
    "# From MpNet we keep only mpnet_ret_docs (drop everything else)\n",
    "mpnet_keep = ['mpnet_ret_docs']\n",
    "missing_mpnet = set(mpnet_keep) - set(mpnet_df.columns)\n",
    "if missing_mpnet:\n",
    "    raise ValueError(f\"Missing columns in mpnet_df: {missing_mpnet}\")\n",
    "mpnet_df = mpnet_df[mpnet_keep].copy()\n",
    "\n",
    "# ------------- Align by row index and concatenate mpnet_ret_docs -------------\n",
    "if len(splade_df) != len(mpnet_df):\n",
    "    raise ValueError(f\"Row count mismatch: Splade={len(splade_df)}, MpNet={len(mpnet_df)}. \"\n",
    "                     \"Row-wise concatenation requires equal lengths.\")\n",
    "\n",
    "# Reset index to ensure alignment (optional but safe)\n",
    "splade_df = splade_df.reset_index(drop=True)\n",
    "mpnet_df  = mpnet_df.reset_index(drop=True)\n",
    "\n",
    "# Concatenate the single column mpnet_ret_docs to splade_df\n",
    "merged = pd.concat([splade_df, mpnet_df['mpnet_ret_docs']], axis=1)\n",
    "print('Merged shape:', merged.shape)\n",
    "\n",
    "# ------------- Parsing + fusion helpers -------------\n",
    "def safe_load(x):\n",
    "    if isinstance(x, (list, dict)):\n",
    "        return x\n",
    "    if pd.isna(x):\n",
    "        return None\n",
    "    s = str(x)\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        try:\n",
    "            import ast\n",
    "            return ast.literal_eval(s)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "def normalize_doc_id(x):\n",
    "    return None if x is None else str(x).strip().lower()\n",
    "\n",
    "def to_score_dict(items):\n",
    "    \"\"\"\n",
    "    Convert retrieval list into dict: doc_id -> score (float).\n",
    "    Accepts dicts containing {'doc_id', 'score'} or {'id'/'docid', 'sim'}.\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    if not items:\n",
    "        return d\n",
    "    for it in items:\n",
    "        if not isinstance(it, dict):\n",
    "            continue\n",
    "        did = normalize_doc_id(it.get('doc_id') or it.get('id') or it.get('docid'))\n",
    "        if not did:\n",
    "            continue\n",
    "        score = it.get('score', it.get('sim'))\n",
    "        try:\n",
    "            score = float(score)\n",
    "        except Exception:\n",
    "            continue\n",
    "        d[did] = score\n",
    "    return d\n",
    "\n",
    "def min_max_norm(d):\n",
    "    \"\"\"\n",
    "    Min-max normalize a dict of {doc_id: score} per query to [0, 1].\n",
    "    If empty or all-equal, returns zeros.\n",
    "    \"\"\"\n",
    "    if not d:\n",
    "        return {}\n",
    "    vals = list(d.values())\n",
    "    vmin, vmax = min(vals), max(vals)\n",
    "    if vmax == vmin:\n",
    "        return {k: 0.0 for k in d}\n",
    "    return {k: (v - vmin) / (vmax - vmin) for k, v in d.items()}\n",
    "\n",
    "def fuse_linear(mpnet_scores, splade_scores, alpha=0.5, normalize=True):\n",
    "    \"\"\"\n",
    "    Linear interpolation fusion:\n",
    "      fused = alpha * mpnet + (1 - alpha) * splade\n",
    "    - If normalize, applies per-query min-max per source first.\n",
    "    - Missing docs get 0 from the source where absent.\n",
    "    Returns list of dicts: {'doc_id', 'score', 'rank'} sorted by score desc.\n",
    "    \"\"\"\n",
    "    m = mpnet_scores or {}\n",
    "    s = splade_scores or {}\n",
    "    if normalize:\n",
    "        m = min_max_norm(m)\n",
    "        s = min_max_norm(s)\n",
    "    all_ids = set(m.keys()) | set(s.keys())\n",
    "    fused = {}\n",
    "    for did in all_ids:\n",
    "        ms = m.get(did, 0.0)\n",
    "        ss = s.get(did, 0.0)\n",
    "        fused[did] = alpha * ms + (1.0 - alpha) * ss\n",
    "    sorted_items = sorted(fused.items(), key=lambda x: (-x[1], x[0]))\n",
    "    out = []\n",
    "    for i, (did, sc) in enumerate(sorted_items, start=1):\n",
    "        out.append({'doc_id': did, 'score': float(sc), 'rank': i})\n",
    "    return out\n",
    "\n",
    "def to_qrels_docids_only(gt_items):\n",
    "    \"\"\"\n",
    "    Build binary qrels: {doc_id: 1} from groundtruth_with_ids entries.\n",
    "    Accepts list of dicts or list of ids.\n",
    "    \"\"\"\n",
    "    rel = {}\n",
    "    if not gt_items:\n",
    "        return rel\n",
    "    for it in gt_items:\n",
    "        candidate = None\n",
    "        if isinstance(it, dict):\n",
    "            candidate = it.get('doc_id') or it.get('id') or it.get('docid')\n",
    "        elif isinstance(it, (str, int)):\n",
    "            candidate = it\n",
    "        did = normalize_doc_id(candidate)\n",
    "        if did:\n",
    "            rel[did] = 1\n",
    "    return rel\n",
    "\n",
    "# ---------- NEW: build doc_id -> full_text map (prefer passages, then fall back to retriever lists) ----------\n",
    "def build_doc_text_map(passages_items):\n",
    "    \"\"\"\n",
    "    Build doc_id -> text from passages_with_ids first (prefers 'full_text', falls back to 'text'/'content').\n",
    "    \"\"\"\n",
    "    mp = {}\n",
    "    items = passages_items or []\n",
    "    for it in items:\n",
    "        if not isinstance(it, dict):\n",
    "            continue\n",
    "        did = normalize_doc_id(it.get('doc_id') or it.get('id') or it.get('docid'))\n",
    "        if not did:\n",
    "            continue\n",
    "        txt = it.get('full_text') or it.get('text') or it.get('content')\n",
    "        if isinstance(txt, str) and txt.strip():\n",
    "            mp[did] = txt\n",
    "    return mp\n",
    "\n",
    "def extend_doc_text_map(mp, retriever_items):\n",
    "    \"\"\"\n",
    "    Enrich map from mpnet/splade lists (prefers 'full_text', falls back to 'snippet'/'preview_snippet'/'text').\n",
    "    \"\"\"\n",
    "    items = retriever_items or []\n",
    "    for it in items:\n",
    "        if not isinstance(it, dict):\n",
    "            continue\n",
    "        did = normalize_doc_id(it.get('doc_id') or it.get('id') or it.get('docid'))\n",
    "        if not did or did in mp:\n",
    "            continue\n",
    "        txt = it.get('full_text') or it.get('snippet') or it.get('preview_snippet') or it.get('text')\n",
    "        if isinstance(txt, str) and txt.strip():\n",
    "            mp[did] = txt\n",
    "    return mp\n",
    "\n",
    "# ------------- Build runs + qrels (linear fusion) -------------\n",
    "results_by_qid = {}  # qid -> {doc_id: score}\n",
    "qrels_by_qid = {}    # qid -> {doc_id: relevance}\n",
    "rows_out = []\n",
    "\n",
    "for q_idx, row in merged.iterrows():\n",
    "    mpnet_items = safe_load(row['mpnet_ret_docs'])\n",
    "    splade_items = safe_load(row['splade_ret_docs'])\n",
    "    gt_items     = safe_load(row['groundtruth_with_ids'])\n",
    "    passages_items = safe_load(row['passages_with_ids'])\n",
    "\n",
    "    mpnet_scores  = to_score_dict(mpnet_items)\n",
    "    splade_scores = to_score_dict(splade_items)\n",
    "    fused = fuse_linear(mpnet_scores, splade_scores, alpha=ALPHA, normalize=True)\n",
    "\n",
    "    # normalize doc_ids in fused output\n",
    "    for e in fused:\n",
    "        e['doc_id'] = normalize_doc_id(e['doc_id'])\n",
    "\n",
    "    # Build text map, preferring passages_with_ids, then enriching from the two retrieval lists\n",
    "    doc_text_map = build_doc_text_map(passages_items)\n",
    "    doc_text_map = extend_doc_text_map(doc_text_map, mpnet_items)\n",
    "    doc_text_map = extend_doc_text_map(doc_text_map, splade_items)\n",
    "\n",
    "    # Attach full_text to each fused entry (hybrid_ret_docs)\n",
    "    hybrid_with_text = []\n",
    "    for e in fused:\n",
    "        did = e['doc_id']\n",
    "        hybrid_with_text.append({\n",
    "            'doc_id': did,\n",
    "            'score': e['score'],\n",
    "            'rank': e['rank'],\n",
    "            'full_text': doc_text_map.get(did)  # may be None if unavailable\n",
    "        })\n",
    "\n",
    "    qid = f\"q_{q_idx}\"\n",
    "\n",
    "    # run dict for pytrec_eval: doc_id -> fused score\n",
    "    run_dict = {e['doc_id']: float(e['score']) for e in fused if e['doc_id'] is not None}\n",
    "    results_by_qid[qid] = run_dict\n",
    "\n",
    "    # qrels from groundtruth\n",
    "    qrels = to_qrels_docids_only(gt_items)\n",
    "    qrels_by_qid[qid] = {doc_id: int(rel) for doc_id, rel in qrels.items()}\n",
    "\n",
    "    rows_out.append({\n",
    "        'qid': qid,\n",
    "        'question': row['question'],\n",
    "        'answer': row['answer'],\n",
    "        'passage': row['passage'],\n",
    "        'mpnet_ret_docs': mpnet_items,\n",
    "        'splade_ret_docs': splade_items,\n",
    "        'hybrid_ret_docs': hybrid_with_text,  # now includes full_text\n",
    "        'passages_with_ids': row['passages_with_ids'],\n",
    "        'groundtruth_with_ids': gt_items\n",
    "    })\n",
    "\n",
    "# ------------- Evaluate (MAP/NDCG + cuts) -------------\n",
    "metric_keys = {\"map\", \"ndcg\"} | {f\"map_cut_{k}\" for k in CUTS} | {f\"ndcg_cut_{k}\" for k in CUTS}\n",
    "evaluator = pytrec_eval.RelevanceEvaluator(qrels_by_qid, metric_keys)\n",
    "eval_res = evaluator.evaluate(results_by_qid)\n",
    "\n",
    "def metric_at(pv, metric_base, k):\n",
    "    key = f\"{metric_base}_{k}\"  # e.g., 'map_cut_3'\n",
    "    return float(pv.get(key, 0.0))\n",
    "\n",
    "metrics_rows = []\n",
    "for r in rows_out:\n",
    "    qid = r['qid']\n",
    "    pv = eval_res.get(qid, {})\n",
    "    metrics = {\n",
    "        'MAP':    float(pv.get('map', 0.0)),\n",
    "        'NDCG':   float(pv.get('ndcg', 0.0)),\n",
    "        'MAP@3':  round(metric_at(pv, 'map_cut', 3), 6),\n",
    "        'NDCG@3': round(metric_at(pv, 'ndcg_cut', 3), 6),\n",
    "        'MAP@5':  round(metric_at(pv, 'map_cut', 5), 6),\n",
    "        'NDCG@5': round(metric_at(pv, 'ndcg_cut', 5), 6),\n",
    "        'MAP@10': round(metric_at(pv, 'map_cut', 10), 6),\n",
    "        'NDCG@10':round(metric_at(pv, 'ndcg_cut', 10), 6),\n",
    "    }\n",
    "    metrics_rows.append({**r, **metrics})\n",
    "\n",
    "hybrid_df = pd.DataFrame(metrics_rows)\n",
    "hybrid_df = hybrid_df[['question', 'answer', 'passage',\n",
    "                       'mpnet_ret_docs', 'splade_ret_docs', 'hybrid_ret_docs',\n",
    "                       'passages_with_ids', 'groundtruth_with_ids',\n",
    "                       'MAP', 'NDCG', 'MAP@3', 'NDCG@3', 'MAP@5', 'NDCG@5', 'MAP@10', 'NDCG@10']]\n",
    "\n",
    "# ------------- Save -------------\n",
    "out_dir = 'hybrid_results'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "out_csv = os.path.join(out_dir, f'halubench_hybrid_linear_results_splade_{ALPHA}_new.csv')\n",
    "\n",
    "hybrid_df.to_csv(out_csv, index=False)\n",
    "print(f\"Saved {len(hybrid_df)} rows to {out_csv}\")\n",
    "\n",
    "# Print averages\n",
    "avg_metrics = hybrid_df[['MAP','NDCG','MAP@3','NDCG@3','MAP@5','NDCG@5','MAP@10','NDCG@10']].mean().round(6)\n",
    "print('Averages (alpha=%.2f):' % ALPHA)\n",
    "for m in ['MAP','NDCG','MAP@3','NDCG@3','MAP@5','NDCG@5','MAP@10','NDCG@10']:\n",
    "    print(f\"{m}: {avg_metrics[m]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saved 14900 rows to hybrid_results/halubench_hybrid_linear_results_splade_0.5.csv\n",
    "Averages (alpha=0.50):\n",
    "MAP: 0.834822\n",
    "NDCG: 0.85738\n",
    "MAP@3: 0.82443\n",
    "NDCG@3: 0.834067\n",
    "MAP@5: 0.829473\n",
    "NDCG@5: 0.843159\n",
    "MAP@10: 0.833532\n",
    "NDCG@10: 0.852891\n",
    "\n",
    "Saved 14900 rows to hybrid_results/halubench_hybrid_linear_results_splade_0.6.csv\n",
    "Averages (alpha=0.40):\n",
    "MAP: 0.843462\n",
    "NDCG: 0.863791\n",
    "MAP@3: 0.833434\n",
    "NDCG@3: 0.841126\n",
    "MAP@5: 0.838273\n",
    "NDCG@5: 0.849883\n",
    "MAP@10: 0.842167\n",
    "NDCG@10: 0.859296\n",
    "\n",
    "Saved 14900 rows to hybrid_results/halubench_hybrid_linear_results_splade_0.7.csv\n",
    "Averages (alpha=0.30):\n",
    "MAP: 0.844085\n",
    "NDCG: 0.864219\n",
    "MAP@3: 0.834239\n",
    "NDCG@3: 0.841771\n",
    "MAP@5: 0.838806\n",
    "NDCG@5: 0.850033\n",
    "MAP@10: 0.842712\n",
    "NDCG@10: 0.859471\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged shape: (14900, 7)\n",
      "Saved 14900 rows to halubench_hybrid_linear_with_rerank_minilm_alpha_0.3_beta_0.85.csv\n",
      "Averages (alpha=0.30) - FUSED:\n",
      "FUSED_MAP: 0.844085\n",
      "FUSED_NDCG: 0.864219\n",
      "FUSED_MAP@3: 0.834239\n",
      "FUSED_NDCG@3: 0.841771\n",
      "FUSED_MAP@5: 0.838806\n",
      "FUSED_NDCG@5: 0.850033\n",
      "FUSED_MAP@10: 0.842712\n",
      "FUSED_NDCG@10: 0.859471\n",
      "Averages (alpha=0.30, beta=0.85) - RERANK:\n",
      "RERANK_MAP: 0.855388\n",
      "RERANK_NDCG: 0.872985\n",
      "RERANK_MAP@3: 0.846342\n",
      "RERANK_NDCG@3: 0.852801\n",
      "RERANK_MAP@5: 0.850752\n",
      "RERANK_NDCG@5: 0.860798\n",
      "RERANK_MAP@10: 0.854345\n",
      "RERANK_NDCG@10: 0.869451\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "HALUbench: SPLADE + MPNet Hybrid with Linear Interpolation + Cross-Encoder Reranking updated\n",
    "Reranker model: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
    "\n",
    "Adds: 'full_text' to each item in both hybrid_ret_docs (fused) and hybrid_reranked_docs.\n",
    "\n",
    "- Keeps HALUbench layout: row-wise concat of mpnet_ret_docs to the SPLADE dataframe.\n",
    "- Linear interpolation (min–max per-source per-query) with ALPHA=0.3.\n",
    "- Cross-encoder reranking atop the fused list, then blending with BETA.\n",
    "- Evaluates fused vs reranked with pytrec_eval and saves results.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import pytrec_eval\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# ----------------- Config -----------------\n",
    "CUTS = (3, 5, 10)\n",
    "\n",
    "# Linear interpolation weight for MPNet vs SPLADE (as in your HALUbench script)\n",
    "ALPHA = 0.3   # fused = ALPHA * mpnet + (1 - ALPHA) * splade\n",
    "\n",
    "# Reranking weight (beta weights the reranker vs fused score)\n",
    "BETA = 0.85   # final = BETA * rerank_score + (1 - BETA) * fused_score\n",
    "\n",
    "# Reranker pool and output serialization size\n",
    "MAX_CANDIDATES_FOR_RERANK = 200\n",
    "TOPK_SAVE = 100  # how many reranked docs to store per row\n",
    "\n",
    "# Inputs\n",
    "splade_path = './prior_results/halubench_splade_modified_metrics.csv'\n",
    "mpnet_path  = './prior_results/halubench_mpnet_modified_metrics.csv'\n",
    "\n",
    "# Outputs\n",
    "out_csv = f'halubench_hybrid_linear_with_rerank_minilm_alpha_{ALPHA}_beta_{BETA}.csv'\n",
    "\n",
    "# Reranker model settings\n",
    "RERANKER_MODEL_NAME = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "RERANKER_MAX_LENGTH = 512\n",
    "RERANKER_BATCH_SIZE = 32\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE = torch.float32\n",
    "\n",
    "\n",
    "# ----------------- Load -----------------\n",
    "splade_df = pd.read_csv(splade_path)\n",
    "mpnet_df  = pd.read_csv(mpnet_path)\n",
    "\n",
    "# Normalize mpnet column name if needed\n",
    "if 'all-mpnet-base-v2_ret_docs' in mpnet_df.columns:\n",
    "    mpnet_df = mpnet_df.rename(columns={'all-mpnet-base-v2_ret_docs': 'mpnet_ret_docs'})\n",
    "\n",
    "# Keep only necessary columns (HALUbench)\n",
    "splade_keep = ['question', 'answer', 'passage', 'splade_ret_docs', 'passages_with_ids', 'groundtruth_with_ids']\n",
    "missing_splade = set(splade_keep) - set(splade_df.columns)\n",
    "if missing_splade:\n",
    "    raise ValueError(f\"Missing columns in splade_df: {missing_splade}\")\n",
    "splade_df = splade_df[splade_keep].copy()\n",
    "\n",
    "mpnet_keep = ['mpnet_ret_docs']\n",
    "missing_mpnet = set(mpnet_keep) - set(mpnet_df.columns)\n",
    "if missing_mpnet:\n",
    "    raise ValueError(f\"Missing columns in mpnet_df: {missing_mpnet}\")\n",
    "mpnet_df = mpnet_df[mpnet_keep].copy()\n",
    "\n",
    "# Align by index and concatenate mpnet_ret_docs\n",
    "if len(splade_df) != len(mpnet_df):\n",
    "    raise ValueError(f\"Row count mismatch: Splade={len(splade_df)}, MpNet={len(mpnet_df)}. Row-wise concatenation requires equal lengths.\")\n",
    "\n",
    "splade_df = splade_df.reset_index(drop=True)\n",
    "mpnet_df  = mpnet_df.reset_index(drop=True)\n",
    "\n",
    "merged = pd.concat([splade_df, mpnet_df['mpnet_ret_docs']], axis=1)\n",
    "print('Merged shape:', merged.shape)\n",
    "\n",
    "\n",
    "# ----------------- Helpers -----------------\n",
    "def safe_load(x):\n",
    "    if isinstance(x, (list, dict)):\n",
    "        return x\n",
    "    if pd.isna(x):\n",
    "        return None\n",
    "    s = str(x)\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        try:\n",
    "            import ast\n",
    "            return ast.literal_eval(s)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "def normalize_doc_id(x):\n",
    "    return None if x is None else str(x).strip().lower()\n",
    "\n",
    "def to_score_dict(items):\n",
    "    \"\"\"\n",
    "    Convert retrieval list into dict: doc_id -> score (float).\n",
    "    Accepts dicts containing {'doc_id', 'score'} or {'id'/'docid', 'sim'}.\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    if not items:\n",
    "        return d\n",
    "    for it in items:\n",
    "        if not isinstance(it, dict):\n",
    "            continue\n",
    "        did = normalize_doc_id(it.get('doc_id') or it.get('id') or it.get('docid'))\n",
    "        if not did:\n",
    "            continue\n",
    "        score = it.get('score', it.get('sim'))\n",
    "        try:\n",
    "            score = float(score)\n",
    "        except Exception:\n",
    "            continue\n",
    "        d[did] = score\n",
    "    return d\n",
    "\n",
    "def min_max_norm(d):\n",
    "    \"\"\"\n",
    "    Min–max normalize {doc_id: score} to [0, 1] per query.\n",
    "    If empty or all-equal, returns zeros.\n",
    "    \"\"\"\n",
    "    if not d:\n",
    "        return {}\n",
    "    vals = list(d.values())\n",
    "    vmin, vmax = min(vals), max(vals)\n",
    "    if vmax == vmin:\n",
    "        return {k: 0.0 for k in d}\n",
    "    return {k: (v - vmin) / (vmax - vmin) for k, v in d.items()}\n",
    "\n",
    "def fuse_linear(mpnet_scores, splade_scores, alpha=0.5, normalize=True):\n",
    "    \"\"\"\n",
    "    Linear interpolation fusion:\n",
    "      fused = alpha * mpnet + (1 - alpha) * splade\n",
    "    - If normalize, applies per-query min-max per source first.\n",
    "    - Missing docs get 0 from the source where absent.\n",
    "    Returns list of dicts: {'doc_id', 'score', 'rank'} sorted by score desc.\n",
    "    \"\"\"\n",
    "    m = mpnet_scores or {}\n",
    "    s = splade_scores or {}\n",
    "    if normalize:\n",
    "        m = min_max_norm(m)\n",
    "        s = min_max_norm(s)\n",
    "    all_ids = set(m.keys()) | set(s.keys())\n",
    "    fused = {}\n",
    "    for did in all_ids:\n",
    "        ms = m.get(did, 0.0)\n",
    "        ss = s.get(did, 0.0)\n",
    "        fused[did] = alpha * ms + (1.0 - alpha) * ss\n",
    "    sorted_items = sorted(fused.items(), key=lambda x: (-x[1], x[0]))\n",
    "    out = []\n",
    "    for i, (did, sc) in enumerate(sorted_items, start=1):\n",
    "        out.append({'doc_id': did, 'score': float(sc), 'rank': i})\n",
    "    return out\n",
    "\n",
    "def to_qrels_docids_only(gt_items):\n",
    "    \"\"\"\n",
    "    Build binary qrels: {doc_id: 1} from groundtruth_with_ids entries.\n",
    "    Accepts list of dicts or list of ids.\n",
    "    \"\"\"\n",
    "    rel = {}\n",
    "    if not gt_items:\n",
    "        return rel\n",
    "    for it in gt_items:\n",
    "        candidate = None\n",
    "        if isinstance(it, dict):\n",
    "            candidate = it.get('doc_id') or it.get('id') or it.get('docid')\n",
    "        elif isinstance(it, (str, int)):\n",
    "            candidate = it\n",
    "        did = normalize_doc_id(candidate)\n",
    "        if did:\n",
    "            rel[did] = 1\n",
    "    return rel\n",
    "\n",
    "# ---- NEW: text mapping helpers ----\n",
    "def build_doc_text_map(passages_items) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Prefer passages_with_ids['full_text'|'text'|'content'].\n",
    "    \"\"\"\n",
    "    mp = {}\n",
    "    items = passages_items or []\n",
    "    for it in items:\n",
    "        if not isinstance(it, dict):\n",
    "            continue\n",
    "        did = normalize_doc_id(it.get('doc_id') or it.get('id') or it.get('docid'))\n",
    "        txt = it.get('full_text') or it.get('text') or it.get('content')\n",
    "        if did and isinstance(txt, str) and txt.strip():\n",
    "            mp[did] = txt\n",
    "    return mp\n",
    "\n",
    "def extend_doc_text_map(mp, retriever_items):\n",
    "    \"\"\"\n",
    "    Enrich with mpnet/splade text (prefer 'full_text', fallback to 'snippet'/'preview_snippet'/'text').\n",
    "    \"\"\"\n",
    "    items = retriever_items or []\n",
    "    for it in items:\n",
    "        if not isinstance(it, dict):\n",
    "            continue\n",
    "        did = normalize_doc_id(it.get('doc_id') or it.get('id') or it.get('docid'))\n",
    "        if not did or did in mp:\n",
    "            continue\n",
    "        txt = it.get('full_text') or it.get('snippet') or it.get('preview_snippet') or it.get('text')\n",
    "        if isinstance(txt, str) and txt.strip():\n",
    "            mp[did] = txt\n",
    "    return mp\n",
    "\n",
    "\n",
    "# ----------------- Reranker: cross-encoder/ms-marco-MiniLM-L-6-v2 -----------------\n",
    "class MiniLMReranker:\n",
    "    \"\"\"\n",
    "    Cross-encoder reranker using 'cross-encoder/ms-marco-MiniLM-L-6-v2'.\n",
    "    Returns a float score per (query, passage), where higher indicates higher relevance.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str = RERANKER_MODEL_NAME, device: str = DEVICE, dtype=DTYPE, max_length: int = RERANKER_MAX_LENGTH):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        self.max_length = max_length\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def score(self, pairs: List[Tuple[str, str]], batch_size: int = RERANKER_BATCH_SIZE) -> List[float]:\n",
    "        scores: List[float] = []\n",
    "        for i in range(0, len(pairs), batch_size):\n",
    "            batch = pairs[i:i+batch_size]\n",
    "            queries = [b[0] for b in batch]\n",
    "            docs    = [b[1] for b in batch]\n",
    "            enc = self.tokenizer(\n",
    "                queries,\n",
    "                docs,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            enc = {k: v.to(self.device) for k, v in enc.items()}\n",
    "            logits = self.model(**enc).logits  # (B, 1) typically\n",
    "            if logits.dim() == 2 and logits.size(1) == 1:\n",
    "                batch_scores = logits.squeeze(1).detach().float().cpu().tolist()\n",
    "            else:\n",
    "                batch_scores = logits.view(-1).detach().float().cpu().tolist()\n",
    "            scores.extend(batch_scores)\n",
    "        return scores\n",
    "\n",
    "reranker = MiniLMReranker()\n",
    "\n",
    "def apply_reranker_on_fused_list(\n",
    "    query: str,\n",
    "    fused_list: List[Dict],\n",
    "    doc_text_map: Dict[str, str],\n",
    "    beta: float = BETA,\n",
    "    max_candidates: int = MAX_CANDIDATES_FOR_RERANK,\n",
    "    batch_size: int = RERANKER_BATCH_SIZE\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Input fused_list: [{'doc_id','score','rank'}]\n",
    "    Output: [{'doc_id','fused_score','rerank_score','final_score','rank','full_text'}], sorted by final_score desc.\n",
    "    \"\"\"\n",
    "    # Trim candidate pool\n",
    "    pool = fused_list[:max_candidates]\n",
    "\n",
    "    # Prepare (query, text) pairs\n",
    "    pairs = []\n",
    "    valid_idxs = []\n",
    "    for i, e in enumerate(pool):\n",
    "        did = e['doc_id']\n",
    "        txt = doc_text_map.get(did)\n",
    "        if isinstance(txt, str) and txt.strip():\n",
    "            pairs.append((query, txt))\n",
    "            valid_idxs.append(i)\n",
    "\n",
    "    # If nothing to score, pass-through and still attach text if present\n",
    "    if not pairs:\n",
    "        out = []\n",
    "        for i, e in enumerate(pool, start=1):\n",
    "            did = e['doc_id']\n",
    "            out.append({\n",
    "                'doc_id': did,\n",
    "                'fused_score': float(e['score']),\n",
    "                'rerank_score': 0.0,\n",
    "                'final_score': float(e['score']),\n",
    "                'rank': i,\n",
    "                'full_text': doc_text_map.get(did)\n",
    "            })\n",
    "        return out\n",
    "\n",
    "    # Reranker scores\n",
    "    rr_scores = reranker.score(pairs, batch_size=batch_size)\n",
    "\n",
    "    # Attach scores, compute final\n",
    "    for idx_local, s in zip(valid_idxs, rr_scores):\n",
    "        pool[idx_local]['rerank_score'] = float(s)\n",
    "    for e in pool:\n",
    "        e['rerank_score'] = float(e.get('rerank_score', 0.0))\n",
    "        e['fused_score'] = float(e.get('score', 0.0))\n",
    "        e['final_score'] = beta * e['rerank_score'] + (1.0 - beta) * e['fused_score']\n",
    "\n",
    "    # Sort and re-rank\n",
    "    ranked = sorted(pool, key=lambda x: (-x['final_score'], x['doc_id']))\n",
    "\n",
    "    # Build output with full_text\n",
    "    out = []\n",
    "    for new_rank, e in enumerate(ranked, start=1):\n",
    "        did = e['doc_id']\n",
    "        out.append({\n",
    "            'doc_id': did,\n",
    "            'fused_score': e['fused_score'],\n",
    "            'rerank_score': e['rerank_score'],\n",
    "            'final_score': e['final_score'],\n",
    "            'rank': new_rank,\n",
    "            'full_text': doc_text_map.get(did)\n",
    "        })\n",
    "    return out\n",
    "\n",
    "\n",
    "# ----------------- Build runs + qrels (fusion + rerank) -----------------\n",
    "results_by_qid_fused = {}     # qid -> {doc_id: fused score}\n",
    "results_by_qid_reranked = {}  # qid -> {doc_id: final score}\n",
    "qrels_by_qid = {}             # qid -> {doc_id: relevance}\n",
    "rows_out = []\n",
    "\n",
    "for q_idx, row in merged.iterrows():\n",
    "    mpnet_items = safe_load(row['mpnet_ret_docs'])\n",
    "    splade_items = safe_load(row['splade_ret_docs'])\n",
    "    gt_items     = safe_load(row['groundtruth_with_ids'])\n",
    "    passages_items = safe_load(row['passages_with_ids'])\n",
    "\n",
    "    mpnet_scores  = to_score_dict(mpnet_items)\n",
    "    splade_scores = to_score_dict(splade_items)\n",
    "\n",
    "    fused = fuse_linear(mpnet_scores, splade_scores, alpha=ALPHA, normalize=True)\n",
    "\n",
    "    # normalize doc_ids in fused output\n",
    "    for e in fused:\n",
    "        e['doc_id'] = normalize_doc_id(e['doc_id'])\n",
    "\n",
    "    qid = f\"q_{q_idx}\"\n",
    "\n",
    "    # Build text map from passages, then enrich from retriever lists if needed\n",
    "    doc_text_map = build_doc_text_map(passages_items)\n",
    "    doc_text_map = extend_doc_text_map(doc_text_map, mpnet_items)\n",
    "    doc_text_map = extend_doc_text_map(doc_text_map, splade_items)\n",
    "\n",
    "    # Rerank on top of fused list\n",
    "    reranked = apply_reranker_on_fused_list(\n",
    "        query=row['question'],\n",
    "        fused_list=fused,\n",
    "        doc_text_map=doc_text_map,\n",
    "        beta=BETA,\n",
    "        max_candidates=MAX_CANDIDATES_FOR_RERANK,\n",
    "        batch_size=RERANKER_BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    # Attach full_text to fused list for serialization\n",
    "    fused_with_text = []\n",
    "    for e in fused:\n",
    "        did = e['doc_id']\n",
    "        fused_with_text.append({\n",
    "            'doc_id': did,\n",
    "            'score': e['score'],\n",
    "            'rank': e['rank'],\n",
    "            'full_text': doc_text_map.get(did)\n",
    "        })\n",
    "\n",
    "    # run dicts\n",
    "    run_fused = {e['doc_id']: float(e['score']) for e in fused if e['doc_id'] is not None}\n",
    "    run_reranked = {e['doc_id']: float(e['final_score']) for e in reranked if e['doc_id'] is not None}\n",
    "\n",
    "    results_by_qid_fused[qid] = run_fused\n",
    "    results_by_qid_reranked[qid] = run_reranked\n",
    "\n",
    "    # qrels\n",
    "    qrels = to_qrels_docids_only(gt_items)\n",
    "    qrels_by_qid[qid] = {doc_id: int(rel) for doc_id, rel in qrels.items()}\n",
    "\n",
    "    rows_out.append({\n",
    "        'qid': qid,\n",
    "        'question': row['question'],\n",
    "        'answer': row['answer'],\n",
    "        'passage': row['passage'],\n",
    "        'mpnet_ret_docs': mpnet_items,\n",
    "        'splade_ret_docs': splade_items,\n",
    "        'hybrid_ret_docs': fused_with_text,                 # with full_text\n",
    "        'hybrid_reranked_docs': reranked[:TOPK_SAVE],       # with full_text\n",
    "        'passages_with_ids': passages_items,\n",
    "        'groundtruth_with_ids': gt_items\n",
    "    })\n",
    "\n",
    "# ----------------- Evaluate (MAP/NDCG + cuts) -----------------\n",
    "metric_keys = {\"map\", \"ndcg\"} | {f\"map_cut_{k}\" for k in CUTS} | {f\"ndcg_cut_{k}\" for k in CUTS}\n",
    "evaluator_fused = pytrec_eval.RelevanceEvaluator(qrels_by_qid, metric_keys)\n",
    "evaluator_reranked = pytrec_eval.RelevanceEvaluator(qrels_by_qid, metric_keys)\n",
    "\n",
    "eval_fused = evaluator_fused.evaluate(results_by_qid_fused)\n",
    "eval_reranked = evaluator_reranked.evaluate(results_by_qid_reranked)\n",
    "\n",
    "def metric_at(pv, metric_base, k):\n",
    "    key = f\"{metric_base}_{k}\"\n",
    "    return float(pv.get(key, 0.0))\n",
    "\n",
    "def extract_metrics(pv):\n",
    "    return {\n",
    "        'MAP': float(pv.get('map', 0.0)),\n",
    "        'NDCG': float(pv.get('ndcg', 0.0)),\n",
    "        'MAP@3': round(metric_at(pv, 'map_cut', 3), 6),\n",
    "        'NDCG@3': round(metric_at(pv, 'ndcg_cut', 3), 6),\n",
    "        'MAP@5': round(metric_at(pv, 'map_cut', 5), 6),\n",
    "        'NDCG@5': round(metric_at(pv, 'ndcg_cut', 5), 6),\n",
    "        'MAP@10': round(metric_at(pv, 'map_cut', 10), 6),\n",
    "        'NDCG@10': round(metric_at(pv, 'ndcg_cut', 10), 6),\n",
    "    }\n",
    "\n",
    "metrics_rows = []\n",
    "for r in rows_out:\n",
    "    qid = r['qid']\n",
    "    fused_metrics = extract_metrics(eval_fused.get(qid, {}))\n",
    "    rerank_metrics = extract_metrics(eval_reranked.get(qid, {}))\n",
    "    metrics_rows.append({\n",
    "        **r,\n",
    "        **{f\"FUSED_{k}\": v for k, v in fused_metrics.items()},\n",
    "        **{f\"RERANK_{k}\": v for k, v in rerank_metrics.items()},\n",
    "    })\n",
    "\n",
    "hybrid_df = pd.DataFrame(metrics_rows)\n",
    "hybrid_df = hybrid_df[['question', 'answer', 'passage',\n",
    "                       'mpnet_ret_docs', 'splade_ret_docs',\n",
    "                       'hybrid_ret_docs', 'hybrid_reranked_docs',\n",
    "                       'passages_with_ids', 'groundtruth_with_ids',\n",
    "                       'FUSED_MAP', 'FUSED_NDCG', 'FUSED_MAP@3', 'FUSED_NDCG@3',\n",
    "                       'FUSED_MAP@5', 'FUSED_NDCG@5', 'FUSED_MAP@10', 'FUSED_NDCG@10',\n",
    "                       'RERANK_MAP', 'RERANK_NDCG', 'RERANK_MAP@3', 'RERANK_NDCG@3',\n",
    "                       'RERANK_MAP@5', 'RERANK_NDCG@5', 'RERANK_MAP@10', 'RERANK_NDCG@10']]\n",
    "\n",
    "# ------------- Save -------------\n",
    "# Safer save in case out_csv has no directory\n",
    "# dirpath = os.path.dirname(out_csv)\n",
    "# if dirpath:\n",
    "#     os.makedirs(dirpath, exist_ok=True)\n",
    "hybrid_df.to_csv(out_csv, index=False)\n",
    "print(f\"Saved {len(hybrid_df)} rows to {out_csv}\")\n",
    "\n",
    "# Print averages\n",
    "avg_fused = hybrid_df[['FUSED_MAP','FUSED_NDCG','FUSED_MAP@3','FUSED_NDCG@3','FUSED_MAP@5','FUSED_NDCG@5','FUSED_MAP@10','FUSED_NDCG@10']].mean().round(6)\n",
    "avg_rerank = hybrid_df[['RERANK_MAP','RERANK_NDCG','RERANK_MAP@3','RERANK_NDCG@3','RERANK_MAP@5','RERANK_NDCG@5','RERANK_MAP@10','RERANK_NDCG@10']].mean().round(6)\n",
    "\n",
    "print(f\"Averages (alpha={ALPHA:.2f}) - FUSED:\")\n",
    "for m in ['FUSED_MAP','FUSED_NDCG','FUSED_MAP@3','FUSED_NDCG@3','FUSED_MAP@5','FUSED_NDCG@5','FUSED_MAP@10','FUSED_NDCG@10']:\n",
    "    print(f\"{m}: {avg_fused[m]}\")\n",
    "\n",
    "print(f\"Averages (alpha={ALPHA:.2f}, beta={BETA:.2f}) - RERANK:\")\n",
    "for m in ['RERANK_MAP','RERANK_NDCG','RERANK_MAP@3','RERANK_NDCG@3','RERANK_MAP@5','RERANK_NDCG@5','RERANK_MAP@10','RERANK_NDCG@10']:\n",
    "    print(f\"{m}: {avg_rerank[m]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "halu_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
