{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "7nQeOfB7NpT9",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "hS1COAtyPxYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "from datasets import load_dataset\n",
        "import csv"
      ],
      "metadata": {
        "id": "WA3Nub3BJoVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_dataset(filename):\n",
        "  # Read CSV file with comma (,) delimiter\n",
        "  df = pd.read_csv(filename, on_bad_lines=\"skip\")\n",
        "\n",
        "  print(len(df))\n",
        "\n",
        "  df.rename(columns={'answer': 'groundtruth-answer', 'answer_qwen4b_contriever': 'generated-answer'}, inplace=True)\n",
        "\n",
        "  #print(df.head(1))\n",
        "  return df\n",
        "  # Display first few rows of the DataFrame\n",
        "  #print(df.head())"
      ],
      "metadata": {
        "id": "OHD7V48XO5Wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_hf_dataset(ds_name, subset_name, split_name):\n",
        "  # Load the 'distractor' split of HotpotQA validation set\n",
        "  dataset = load_dataset(ds_name, subset_name, split=split_name)\n",
        "\n",
        "  # Extract only the desired fields\n",
        "  filtered_dataset = dataset.map(lambda x: {\n",
        "      \"id\": x[\"id\"],\n",
        "      \"question\": x[\"question\"],\n",
        "      \"answer\": x[\"answer\"],\n",
        "      \"context\": x[\"context\"]\n",
        "  })\n",
        "  return filtered_dataset"
      ],
      "metadata": {
        "id": "qFLaGuclPk8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to flatten context into a single text string\n",
        "def flatten_context(context):\n",
        "    # Each context has 'title' and 'sentences', we join all sentences into one string\n",
        "    sentences = []\n",
        "    for sent_list in context['sentences']:\n",
        "        sentences.extend(sent_list)  # sent_list is already a list of strings\n",
        "    return \" \".join(sentences)"
      ],
      "metadata": {
        "id": "EhKD4_RBPmO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sanitize_context(text):\n",
        "    \"\"\"Normalize and escape complex literary text for TSV/JSON safety.\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    clean = (\n",
        "        text.replace(\"\\\\\", \"\\\\\\\\\")    # keep backslashes valid\n",
        "            .replace(\"\\t\", \"\\\\t\")     # escape tabs\n",
        "            .replace(\"\\n\", \"\\\\n\")     # escape in‑field newlines\n",
        "            .replace(\"\\\"\", \"\\\\\\\"\")    # escape quotes\n",
        "            .replace(\"“\", \"\\\"\").replace(\"”\", \"\\\"\")  # normalize fancy quotes\n",
        "            .replace(\"’\", \"'\").replace(\"‘\", \"'\")\n",
        "            .replace(\"–\", \"-\").replace(\"—\", \"-\")\n",
        "            .replace(\"\\u00a0\", \" \")   # non‑breaking → space\n",
        "    )\n",
        "    # collapse redundant whitespace\n",
        "    return re.sub(r\"\\s{2,}\", \" \", clean.strip())"
      ],
      "metadata": {
        "id": "el9KfJCCS28Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def escape_text(text):\n",
        "  if isinstance(text, str):\n",
        "    return (\n",
        "      text.replace(\"\\t\", \"\\\\t\")\n",
        "      .replace(\"\\n\", \"\\\\n\")\n",
        "      .replace('\"', '\\\\\"')\n",
        "    )\n",
        "  return text"
      ],
      "metadata": {
        "id": "w_Zos94-QwZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def safe_escape(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Uses json.dumps() to properly escape quotes, backslashes, and control characters.\n",
        "    Returns string without the double quotes json.dumps adds.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    escaped = json.dumps(text)         # creates encoded str with outer quotes\n",
        "    return escaped[1:-1]               # remove the surrounding quotes"
      ],
      "metadata": {
        "id": "WZHtm-1DUE0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_triples(filtered_dataset):\n",
        "  # Create the triple list\n",
        "  triple_list = []\n",
        "  for _, item in filtered_dataset.iterrows():\n",
        "    #context_text = sanitize_context(item['context'])\n",
        "    question = sanitize_context(item['question'])\n",
        "    groundtruth_answer = sanitize_context(item['groundtruth-answer'])\n",
        "    model_answer = sanitize_context(item['generated-answer'])\n",
        "\n",
        "    # Escape problematic characters\n",
        "    # Replace TAB, NEWLINE, and QUOTE with visible safe placeholders\n",
        "    #context_text = escape_text(context_text)\n",
        "    #question = escape_text(question)\n",
        "    #answer = escape_text(answer)\n",
        "\n",
        "    triple_list.append((question, model_answer, groundtruth_answer))\n",
        "\n",
        "  # Display first 3 triples\n",
        "  #for t in triple_list[:3]:\n",
        "    #print(t)\n",
        "\n",
        "  print(\"Triples created!\")\n",
        "\n",
        "  return triple_list"
      ],
      "metadata": {
        "id": "fuhDUyEzPnOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_prompting_model_old(triple_list):\n",
        "    \"\"\"\n",
        "    triple_list: list of (question, generated-answer, groundtruth-answer)\n",
        "    \"\"\"\n",
        "    batch_requests = []\n",
        "\n",
        "    for idx, (question, generated, ground_truth) in enumerate(triple_list):\n",
        "        system_block = {\n",
        "            \"type\": \"text\",\n",
        "            \"text\": (\n",
        "                \"You are serving as an impartial evaluator (LLM-as-a-Judge) for model-generated QA responses.\\n\"\n",
        "                \"You will be given a question, a model-generated answer, and a ground-truth reference answer.\\n\"\n",
        "                \"Your goal is to determine the correctness of the generated answer according to the following rules:\\n\\n\"\n",
        "                \"1. Correct: The generated answer semantically matches the ground-truth answer.\\n\"\n",
        "                \"2. Hallucinated: The generated answer clearly contradicts or diverges from the ground-truth answer.\\n\"\n",
        "                \"3. Insufficient Context: If the generated answer explicitly states that the context does not provide \"\n",
        "                \"sufficient information (e.g., contains phrases such as 'the context does not provide sufficient information', \"\n",
        "                \"'cannot be determined from context', or similar), label it as Insufficient Context.\\n\\n\"\n",
        "                \"Provide exactly one JSON object as output in the following format:\\n\"\n",
        "                \"{\\\"label\\\": one of [\\\"Correct\\\", \\\"Hallucinated\\\", \\\"Insufficient Context\\\"], \"\n",
        "                \"\\\"explanation\\\": one concise sentence explaining your reasoning.}\\n\\n\"\n",
        "                \"Example:\\n\"\n",
        "                \"{\\\"label\\\": \\\"Correct\\\", \\\"explanation\\\": \\\"The generated answer accurately matches the gold reference answer.\\\"}\"\n",
        "            ),\n",
        "        }\n",
        "\n",
        "        user_prompt = f\"\"\"\n",
        "        QUESTION: {question}\n",
        "\n",
        "        MODEL-GENERATED ANSWER: {generated}\n",
        "\n",
        "        GROUND-TRUTH ANSWER: {ground_truth}\n",
        "\n",
        "        Compare the generated answer against the ground truth.\n",
        "        Return exactly one JSON object containing:\n",
        "        - \"label\": one of [\"Correct\", \"Hallucinated\", \"Insufficient Context\"]\n",
        "        - \"explanation\": a single concise justification.\n",
        "        \"\"\"\n",
        "\n",
        "        request = {\n",
        "            \"custom_id\": f\"judge-{idx}\",\n",
        "            \"params\": {\n",
        "                \"model\": \"claude-sonnet-4-5\",\n",
        "                \"max_tokens\": 120,\n",
        "                \"system\": [system_block],\n",
        "                \"messages\": [\n",
        "                    {\"role\": \"user\", \"content\": user_prompt}\n",
        "                ],\n",
        "            },\n",
        "        }\n",
        "\n",
        "        batch_requests.append(request)\n",
        "\n",
        "    return batch_requests"
      ],
      "metadata": {
        "id": "jILce0HtdRav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_prompting_model(triple_list):\n",
        "    \"\"\"\n",
        "    triple_list: list of (question, generated-answer, groundtruth-answer)\n",
        "    \"\"\"\n",
        "    batch_requests = []\n",
        "\n",
        "    system_block = {\n",
        "        \"type\": \"text\",\n",
        "        \"text\": (\n",
        "            \"You are serving as an impartial evaluator (LLM-as-a-Judge) for model-generated QA responses.\\n\"\n",
        "            \"You will be given a question, a model-generated answer, and a ground-truth reference answer.\\n\"\n",
        "            \"Your goal is to determine the correctness of the generated answer according to the following rules:\\n\\n\"\n",
        "            \"1. Correct: The generated answer semantically matches the ground-truth answer.\\n\"\n",
        "            \"2. Hallucinated: The generated answer clearly contradicts or diverges from the ground-truth answer.\\n\"\n",
        "            \"3. Insufficient Context: If the generated answer explicitly states that the context does not provide \"\n",
        "            \"sufficient information (e.g., contains phrases such as 'the context does not provide sufficient information', \"\n",
        "            \"'cannot be determined from context', or similar), label it as Insufficient Context.\\n\\n\"\n",
        "            \"Provide exactly one JSON object as output in the following format:\\n\"\n",
        "            \"{\\\"label\\\": one of [\\\"Correct\\\", \\\"Hallucinated\\\", \\\"Insufficient Context\\\"], \"\n",
        "            \"\\\"explanation\\\": one concise sentence explaining your reasoning.}\\n\\n\"\n",
        "            \"Example:\\n\"\n",
        "            \"{\\\"label\\\": \\\"Correct\\\", \\\"explanation\\\": \\\"The generated answer accurately matches the gold reference answer.\\\"}\"\n",
        "        )\n",
        "    }\n",
        "\n",
        "    for idx, (question, generated, ground_truth) in enumerate(triple_list):\n",
        "        user_prompt = f\"\"\"\n",
        "        QUESTION: {question}\n",
        "\n",
        "        MODEL-GENERATED ANSWER: {generated}\n",
        "\n",
        "        GROUND-TRUTH ANSWER: {ground_truth}\n",
        "\n",
        "        Compare the generated answer against the ground truth.\n",
        "        Return exactly one JSON object containing:\n",
        "        - \"label\": one of [\"Correct\", \"Hallucinated\", \"Uncertain\"]\n",
        "        - \"explanation\": a single concise justification.\n",
        "        \"\"\"\n",
        "\n",
        "        request = {\n",
        "            \"custom_id\": f\"judge-{idx}\",\n",
        "            \"params\": {\n",
        "                \"model\": \"claude-sonnet-4-5\",\n",
        "                \"max_tokens\": 200,\n",
        "                \"system\": [system_block],\n",
        "                \"messages\": [\n",
        "                    {\"role\": \"user\", \"content\": user_prompt}\n",
        "                ],\n",
        "            },\n",
        "        }\n",
        "\n",
        "        batch_requests.append(request)\n",
        "\n",
        "    return batch_requests"
      ],
      "metadata": {
        "id": "nlUlfTt1SPkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def send_batch_request(API_KEY, API_URL, batch_requests):\n",
        "  # Prepare batch payload\n",
        "  payload = {\n",
        "    \"requests\": batch_requests\n",
        "  }\n",
        "\n",
        "  # Send request to Batch API\n",
        "  headers = {\n",
        "    \"x-api-key\": API_KEY,\n",
        "    \"anthropic-version\": \"2023-06-01\",\n",
        "    \"content-type\": \"application/json\"\n",
        "  }\n",
        "  response = requests.post(API_URL, json=payload, headers=headers)\n",
        "  try:\n",
        "        result = response.json()\n",
        "  except json.JSONDecodeError:\n",
        "        print(\"Error: Could not decode JSON.\")\n",
        "        print(\"Response text:\")\n",
        "        print(response.text)\n",
        "        raise\n",
        "\n",
        "  # Debugging output\n",
        "  print(\"HTTP Status:\", response.status_code)\n",
        "  print(\"Response Data:\", json.dumps(result, indent=2))\n",
        "\n",
        "  # Handle errors explicitly\n",
        "  if \"error\" in result:\n",
        "      print(\"API Error:\", result[\"error\"])\n",
        "      raise RuntimeError(f\"API Error: {result['error']}\")\n",
        "\n",
        "  # Ensure the expected keys exist\n",
        "  if \"id\" not in result or \"processing_status\" not in result:\n",
        "      print(\"Unexpected response keys:\", result.keys())\n",
        "      raise KeyError(\"Missing 'id' or 'processing_status' in response\")\n",
        "\n",
        "\n",
        "\n",
        "  #result = response.json()\n",
        "  print(\"Batch Created:\", result[\"id\"])\n",
        "  print(\"Processing Status:\", result[\"processing_status\"])\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "id": "bO2J1d9iS4bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_results(API_KEY, results_url, output_file = \"results.json\"):\n",
        "# Local file to save the results\n",
        "  try:\n",
        "    response = requests.get(\n",
        "        results_url,\n",
        "        headers={\n",
        "            \"x-api-key\": API_KEY,\n",
        "            \"anthropic-version\": \"2023-06-01\"\n",
        "        }\n",
        "    )\n",
        "    response.raise_for_status()  # Raise error for HTTP issues\n",
        "\n",
        "    # Save the results locally\n",
        "    with open(output_file, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "    print(f\"Download complete! Results saved as: {output_file}\")\n",
        "\n",
        "  except requests.exceptions.RequestException as e:\n",
        "    print(f\"Failed to download results: {e}\")"
      ],
      "metadata": {
        "id": "6PIcKzHzCeHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def poll_batchcompletion(result, headers):\n",
        "\n",
        "  batch_id = result[\"id\"]\n",
        "  poll_url = f\"https://api.anthropic.com/v1/messages/batches/{batch_id}\"\n",
        "\n",
        "  # Start the timer\n",
        "  start_time = time.perf_counter()\n",
        "\n",
        "  while True:\n",
        "      poll_response = requests.get(poll_url, headers=headers)\n",
        "      poll_data = poll_response.json()\n",
        "      status = poll_data[\"processing_status\"]\n",
        "      print(\"Current Status:\", status)\n",
        "      if status == \"ended\":\n",
        "          results_url = poll_data[\"results_url\"]\n",
        "          break\n",
        "      time.sleep(30)  # Wait 30 seconds and retry\n",
        "\n",
        "  # Stop the timer\n",
        "  end_time = time.perf_counter()\n",
        "  elapsed = end_time - start_time\n",
        "\n",
        "  print(f\"Batch complete! Total time: {elapsed / 60:.2f} minutes\")\n",
        "  print(\"Batch complete! Download results at:\", results_url)\n",
        "\n",
        "  download_results(API_KEY, results_url, output_file = \"results.json\")"
      ],
      "metadata": {
        "id": "DSjIi2iSTsHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def process_json(file_name=\"results.json\", triple_list=None):\n",
        "    with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    results = []\n",
        "    for idx, line in enumerate(lines):\n",
        "        if not line.strip():\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            data = json.loads(line)\n",
        "            custom_id = data.get(\"custom_id\", f\"judge-{idx}\")\n",
        "\n",
        "            stop_reason = data.get(\"result\", {}).get(\"message\", {}).get(\"stop_reason\", \"\")\n",
        "            if stop_reason != \"refusal\":\n",
        "              # Extract label and explanation JSON from the model output\n",
        "              content = data[\"result\"][\"message\"][\"content\"][0][\"text\"]\n",
        "              match = re.search(r\"\\{.*\\}\", content, re.DOTALL)\n",
        "              if match:\n",
        "                  inner_json = json.loads(match.group(0))\n",
        "                  label = inner_json.get(\"label\", \"\")\n",
        "                  explanation = inner_json.get(\"explanation\", \"\")\n",
        "              else:\n",
        "                  label, explanation = \"\", \"\"\n",
        "\n",
        "              # Retrieve question, model-generated answer, and ground-truth answer if provided\n",
        "              if triple_list and idx < len(triple_list):\n",
        "                  question, model_answer, groundtruth_answer = triple_list[idx]\n",
        "              else:\n",
        "                  question, model_answer, groundtruth_answer = \"\", \"\", \"\"\n",
        "            #refusal\n",
        "            else:\n",
        "                question, model_answer, groundtruth_answer = triple_list[idx]\n",
        "                label = \"Refusal\"\n",
        "                explanation = \"Model refused to generate output due to safety filters.\"\n",
        "\n",
        "            results.append({\n",
        "                \"question\": question,\n",
        "                \"model-generated answer\": model_answer,\n",
        "                \"groundtruth answer\": groundtruth_answer,\n",
        "                \"LLM-as-a-judge label\": label,\n",
        "                \"LLM-as-a-judge explanation\": explanation\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            results.append({\"id\": f\"error-{idx}\", \"error\": str(e)})\n",
        "\n",
        "    # Convert to DataFrame and save\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(\"processed_results.tsv\", sep=\"\\t\", index=True, index_label=\"row_id\", encoding=\"utf-8\", quoting=3)\n",
        "    print(\"Results saved → processed_results.tsv\")"
      ],
      "metadata": {
        "id": "Svofkg77JS5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY = \n",
        "API_URL = \"https://api.anthropic.com/v1/messages/batches\"\n",
        "\n",
        "# Send request to Batch API\n",
        "headers = {\n",
        "  \"x-api-key\": API_KEY,\n",
        "  \"anthropic-version\": \"2023-06-01\",\n",
        "  \"content-type\": \"application/json\"\n",
        "}\n",
        "\n",
        "folder_path = \"/content/drive/MyDrive/GenerationRAG/\"\n",
        "file_name = \"hotpotqa_answers_contriever_qwen3_4b_1200rows_final_cut.csv\"\n",
        "ds = read_dataset(folder_path + file_name)\n",
        "\n",
        "#filtered_dataset = load_hf_dataset(ds_name, subset_name, split_name)\n",
        "#filtered_dataset = filtered_dataset.select(range(1000))\n",
        "triple_list = create_triples(ds)\n",
        "batch_requests = batch_prompting_model(triple_list)\n",
        "\n",
        "result = send_batch_request(API_KEY, API_URL, batch_requests)\n",
        "poll_batchcompletion(result, headers)\n",
        "\n",
        "#processing the json file returned from LLM-as-a-judge process\n",
        "file_name = \"results.json\"\n",
        "process_json(file_name, triple_list)"
      ],
      "metadata": {
        "id": "1Se9GQU2OD4A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}